{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>TODO - [ ] copy files from promptier directory, create mkdocs.yaml and render, see how it looks. </p> <p>Compare https://github.com/Joe-Bulfer/studylinux with rendering ../studylinux locally without meterial theme. I like the default mkdocs better</p>"},{"location":"AWS-Azure/","title":"AWS Azure","text":"<p>AWS targets startups and companies without large Microsoft infrastructure. Azure focuses on products and services that appeals to traditional IT departments. </p> <p>\"AWS runs the internet and Azure runs the enterprise\"</p> <p>Many traditional IT jobs at companies with established connections to Microsoft will often use Azure because of interactions with AD and O365.</p> <p>So typically, Azure runs infrastructure of companies that are switching from Windows servers to the cloud while companies that develop their own software will use AWS which runs services for end users.  </p>"},{"location":"Contact%20Me/","title":"Contact Me","text":"<p>  jbulfer13@gmail.com </p> <p>  LinkedIn </p> <p>  Github </p> <p>  Linux Study Website </p> <p>  Join the Linux Discord </p>"},{"location":"Criticism%20of%20How%20Computer%20Science%20is%20Taught/","title":"Criticism of How Computer Science is Taught","text":"<p>Throughout my computer science undergrad, I am disappointed by other students lack of interest and curiosity. Like how most show up to work with only a paycheck in mind, most students only ask, \"Will this be on the test?\" and are only concerned with deliverables. Doing only the bare minimum to scrape by and get to the next step, \"only one more class until I graduate\". Then the information is brain dumped and forgotten about entirely. If one only sees the immediate transient objective in front of them at any given time, they will live and die without ever asking the question of why. Why study computer science or any field for that matter? There is lack of intrinsic motivation and enjoyment in the pursuit of learning.</p> <p>University has taken the role of trade schools in recent history, mainly serving to make young people employable. This conflicts with the original intent of producing research and expanding human knowledge. The chair of computer science at my university transitioned from teaching the C programming language to Python and Javascript as these are the two industry adopted languages despite C closer to the hardware, allowing students to learn the underlying memory and way code is executed. Python is a direct wrapper of C and hides many intricate details, from an academic perspective, this is harmful.</p>"},{"location":"Criticism%20of%20Rust/","title":"Criticism of Rust","text":"<p>\u26a0\ufe0f: Trigger Warning: I know Rust is a well loved language by those that use it and clearly it is useful in some areas considering its wide adoption. I haven't used Rust extensively and keep in mind these are just my opinions.</p> <p>There are two kinds of languages, those that solve a problem and those that prove a point, the latter goes to die. ~ Bjarne Stroustrup, I think</p> <p>Rust proposes a new system of memory management, one based on ownership and borrowing. Kitty rails set in place as if I don't know how to properly allocate my own bytes. Many would consider it a skill issue of not knowing C. </p> <p>If writing software for embedded systems, such as a microcontroller/IOT device, TinyGo has the same performance as Rust as found in this research paper. If you can write Go instead of Rust and get the same performance, the answer is clearly the simpler language.</p> <p>65% of Go Developers use the language for work, vs 20% of Rust developers. This means the majority of those learning and writing software in Rust are doing so as a hobby for personal/side projects. Go on the other hand is used by professionals in the industry. This is because Go solves several real world problems, one being code maintainability. Becuase the language is intentianally minimal with features and emphasizes readability, a code base could be untouched for 10 years after the original author left the company and still be maintained or refactored with minimal hassle.  </p> <p>todo: it's overly complex.  no video made yet</p> <p>TLDR: Just write C, if not then Go or TinyGo for Microcontrollers where low-power and efficiency is needed. </p>"},{"location":"Every%20Programming%20Language%20Explained/","title":"Every Programming Language Explained","text":"<p>\ud83d\udcdd: Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also find the video here.</p> <p>To understand the many programming languages you can choose to learn and create software with, it's important to understand, in addition to the technical details, the historical context and reason the language was created.  Often, they provide a solution to a series of problems encountered in previous languages, which we'll have several examples of, starting with the creation of Java, which arose because of problems with C and C++.</p> <p>See this timeline as a foreshadow for all languages to be discussed.</p> <p></p>"},{"location":"Every%20Programming%20Language%20Explained/#java","title":"Java","text":"<p>It was known since the early 90s, a time when almost all software was written in C and C++, that the majority of security vulnerabilities come from pointer bugs and buffer overflows. </p> <p>James Gosling, an engineer at Sun Microsystems, was frustrated by spending a significant amount of time hunting these pointer bugs down in large C and C++ codebases, which is a notoriosly difficult task.  This eventually lead to him to create his own language and runtime with a garbage collector, which provides automatic memory management. In addition, Java has a strict object design intended to develop software with interfaces as contracts or signatures between objects. In this interview, James Gosling discusses the benefit of having clear interfaces between different components in large scale projects and preventing the \"backdoor\" developers make to bypass the intended object-oriented programming principles or established interfaces to instead quickly hack something together.</p> <p>Another characteristic of Java, besides the language itself, is the architecture it run on, the JVM, or Java Virtual Machine is an abstraction layer agnostic to the underlying machine, which was a brilliant new idea during it's inception.  </p> <p>In the 90s all CPUs were very diverse and proprietary. Most applications ran on computers from different vendors, with different chips and different instruction sets such as MIPS or RISC. When writing C or C++, it had to be tailored to that specific architecture. Not only did software have to be written for specific ISAs, but it often had to be re-written for newer generations, such as transitioning from the Motorola 68010 to the  68020, which James Gosling himself spent a year on re-implementing at Sun Microsystems. He later developed the JVM as a solution where you could write it once and run anywhere. </p> <p>Though people nowadays criticize Java for it's large overhead, understanding the historical context of when it was created puts it design into perspective. </p> <p>Nowadays, Java powers the Android platform, web applications with Spring Boot, back-end server software, enterprise applications and much more. </p>"},{"location":"Every%20Programming%20Language%20Explained/#python","title":"Python","text":"<p>Moving on we have Python, created by Guido Van Rossum, working at a computing research institute in the 80s,  where nearly every program they wrote was either a shell script, or in C. Wishing there was a language somewhere in the middle, that felt like a genuine programming language, like C, but was also interpreted, easy to use and concisely written like shell scripts, but without their poor readability. After creating Python, it was quickly adopted by fellow researchers for scripting tasks.</p> <p>Python's advantage is it's shallow learning curve, since it's interpreted, you don't have to compile code into an separate executable file every time you want to run it, instead just save your file and run a single command. What else contributes to it's ease of use is it's syntax, which is famously known for being simple and easy, making it the preferred beginner programming language for many, often taught in schools and universities. I personally was introduced to programming in a computer class from high school, where we built a platoformer game using the Ktinker module.</p> <p>Besides it's shallow learning curve, another reason for Python's wide adoption is it's massive package ecosystem.  Importing code written by someone else has never been easier, and if you can imagine any use case there is surely a suitable module that exists in the half a million packages on PyPi, or Python Package Index.</p> <p>According to Jetbrain Developer survey, Python currently sees use primarily in data analysis and machine learning, with a long list of other use cases including web development, system administration, and software prototyping.</p>"},{"location":"Every%20Programming%20Language%20Explained/#javascript","title":"Javascript","text":"<p>Moving on to Javascript, we have to understand the early days of the internet and very first web browsers. As the demand of websites grew from static web pages to dynamic, interactive applications, the need for a scripting language specifically for the web became apparent. The idea of code injected directly into markup was new at the time, but became necessary.  Working at Netscape, which later became Firefox, Brendan Eich had a deadline of 10 days to finish the language because it had to be rolled out fast during a time critical period where the internet was booming in popularity and Netscape was in fierce competition against Microsoft's internet explorer for market share. Javascript today has evolved into what people now use as a general purpose programming language despite originally meant for small scripting tasks. The fact that Javascript is loosely typed, means it's difficult to understand what types of data are being passed around, leading to unexpected behavior, such as referencing variables that don't exist and broken code only discovered at runtime when the browser throws an error. Because of this, it was not ideal for large applications. Developers at Microsoft realized this in 2010 when they were first building their new IDE, VSCode. The need for type safety and static type checking ahead of time lead the developers to create their own superset on top of the language, which transpiles back to Javascript, this became known as Typescript.</p> <p>Nowadays both Javascript and Typescript are widely adopted and still growing in popularity. For small web scripting tasks, Javascript is best, for large applications, either Typescript or Javascript paired with JSDoc to comment types works best.</p>"},{"location":"Every%20Programming%20Language%20Explained/#go","title":"Go","text":"<p>Next we have Golang. What lead to it's creation was massive and complex C++ code bases at Google taking minutes, and sometimes hours to compile. In addition, the turn of the century saw languages like C++, Java, and Python as unable to handle problems introduced by multi-core processors, networked systems, and modern server programs comprising tens of millions of lines of code.</p> <p>Go was conceived in 2007 by engineers with many notable accomplishments, such as Unix, C, UTF-8, and the Chrome V8 Engine. </p> <p>The language is intended for big teams working on large codebases. The greatest \"feature\" of Go is it's lack of features, this allows for readability, simplicity, and faster development time. Though you can define methods on types/structs, Go is not an object oriented language, rather a procedural language heavily influenced by C. Go was designed with modern hardware with multi-core CPUs in mind. Concurrency allows fully utilizing multiple CPU cores using goroutines for execution and channels for communication.</p> <p>In this presentation by Rob Pike, he first discusses his experience attending various language conferences, which discussed newer versions and features of Java, C#, C++, PHP, and more. He realized these languages are competing by actively borrowing features from one another. This means in a sense, they are all converging into the same language. Growing in complexity while becoming more similar, meaning bloat without distinction.</p> <p>Go does not try to compete, since Go V1.0, the language has been fixed, with very few features added. It is intended as a simple to use, procedural language. Without OO features, you can focus on the task at hand rather than think about the heirarchy or type system that you have to shape into the problem you're solving. There is no context switching jumping around files finding where something is inherited from. Code generally reads top to bottom in a simple manner.</p> <p>Go has seen widespread adoption for cloud and network services, command line programs, and web development, with the majority of it's use being in the work place by real businesses, as opposed to a language like Rust, which sees use mostly by hobbyists and personal projects. Because of this, in addition to me just not knowing much about the language, means I will skip over it.</p>"},{"location":"Every%20Programming%20Language%20Explained/#c","title":"C","text":"<p>Next, we have the technology that powers big business and enterprise, dot net.</p> <p>The story of C# begins in the late 90s, when Java was basically taking over the world, Microsoft quickly developed their own implementation, called Visual J++, but realized it didn't make sense to build technology based on a license from their competitor, Sun, this lead Microsoft to build their own language and architecture entirely, leading to C# and dot net.</p> <p>See this quote from the creator of C#, Anders Heijlburg, in this interview. </p> <p>In his words, C# was basically created for the \"ease of use with visual basics rapid application devleopment with the power and expressivness of C++ \"</p> <p>Back in the early days, C# was used primarily for building Windows applications and was bound to to the Windows operating system running on dot net. However, nowadays cross platform is possible with dot net core. So although lot of people still associate .NET and C# with Windows only, that is no longer the case with development on Linux and MacOS being just as feasible.</p> <p>Besides just desktop apps, there is much more possibility such as web apps with Blazer, mobile apps with MAUI (previously xamarin), and even game development with the Godot and Unity engine.</p> <p>There is a common phenomenon of developers rejecting C# for political reasons, as in they don't like Microsoft, but the company is no longer the villain of open source like they once were. This could be a topic for a whole nother video, but to summarize, Microsoft historically had a terrible reputation with the open source community under CEOs Bill Gates and Steve Ballmer, but the company has seen a radical shift from close source and anti-competitive behavior to open source and collaborative with Satya Nadella. His first year as CEO he gave away .NET to the .NET foundation, separate from Microsoft. The rewrite of .NET to .NET Core was no longer closed source and proprietary, and nowadays, Microsoft is second in the Open Source Contributor Index behind Google. </p> <p>Because of this culture and history of smaller companies and startups being anti-microsoft, you'll rarely find C# and .NET in these types of jobs, instead finding employment in larger, stable companies like banks, insurance companies and healthcare. Where startups and tech companies are likely to use anything but C#.</p>"},{"location":"Every%20Programming%20Language%20Explained/#employment-and-usage-statistics","title":"Employment and Usage Statistics","text":"<p>Before finishing off the video, I'd like to go over employment and usage statistics.</p> <p>After searching for a particular language wrapped in quotes with the word \"software\" on Indeed, here are the results I found. You can see the Java and Python having the largest number of jobs, and Go with the lowest, although it may not be accurate because of the difficulty of searching a general word like \"Go\", giving false positives, this is the result of searching for Golang, but I suspect many jobs just referred to it as Go, which this search did not pick up. If you have some clever search to provide more accurate results, please leave it in the comments.</p> <p>Moving on the usage statistics, I have data gathered from the yearly stack overflow survey, where professional developers report which languages they have worked with extensively in the past year. If you'd like to explore this data, you can find the website in the description.</p> <p>In terms of employment, you will find C# in corporate jobs while anything else in startup/tech companies which commonly have anti-ms sentiment. Despite Dot Net Core, this culture will likely stick around. </p> <p>That sums up the video and thanks for watching. Like, comment, and subscribe to support more content like this.</p>"},{"location":"Every%20Programming%20Language%20Explained/#additional","title":"Additional","text":"<p>Reddit Post: No C# in Silicon Valley</p> <p>C# is most common in big, stable businesses, banks, and insurance companies. It is rare in startups and silicon valley.</p> <p>C# Jetbrains Developer Ecosystem </p>"},{"location":"First%20Principles%20and%20Holism/","title":"First Principles and Holism","text":"<p>There are many programming languages each with their own set of syntax rules. One could spend years learning the rules of each, but I say these are minutiae.  Although one must learn some basic syntax to write their programs, the knowledge of features and peculiarities of the language should come naturally over time as you write more and more programs, learning only what your program requires each time.  The most important thing aside from proficiency in a language is the knowledge of when and how to use it. To learn a language is to first ask why it was created in the first place.T his requires deeper understanding. Take Python for example. Guido Van Rossum created Python at the CWI research institute </p> <p>In music and piano, sheet music is required to play someone else song. There is a standard notation of 5 staff lines which notes are written on. However, I don't always care to play someone else song. I just sit and play music applying Music Theory principles. Understanding the number of half-steps in minor and major keys, scales of each note, how playing a single C note with my left hand in addition to a C major with my right gives a sweet sound. By learning principles which all music shares and picking up patterns, I am able to play and enjoy music regardless of notation.</p> <p>In a standard intro to calculus course, a student could spend an entire semester with a passing grade without an intuitive, visual understanding of what calculus really is. What most math classes focus on is symbolic manipulation such as algebraic factoring and exponent rules. One could convert an equation into it's first derivative with ease yet fail to understand what it represents, the rate of change of the original equation. </p> <p>Algebra is like sheet music. The important thing isn't can you read music, its can you hear it. Can you hear the music, Robert? ~ Niels Bohr</p>"},{"location":"Future%20of%20Programming/","title":"Future of Programming","text":"<p>\ud83d\udcdd TODO: This is data from 2023, trends are similar to 2024, but that survey has been released and I neeed to update this still</p> <p>According to the Stack Overflow Survey, the most authoritative source of developer technology trends, the languages with the most growth are the following languages which developers responded they plan to use next year.</p> Language Plan to Use Go 12% Rust 11% Kotlin 8% Python 7% Typescript 7% <p>We can see Go has the highest expected growth. It is also reported that a third of current developers have learned Go in the past year.</p> <p>Compare Go usage in the past 6 years with other languages here</p>"},{"location":"Game%20Written%20in%20Rust/","title":"Game Written in Rust","text":"<p>\ud83d\udcdd: Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also find the video here.</p> <p>I've been keeping an eye on Rust as an upcoming programming language and have seen it reach many areas I didn't expect, but what shocked me the most was game development. This is Veloren, the most impressive game I've seen written in Rust where developers created their own engine from scratch using the language. It is entirely community driven, open source and free, both as in price, and as in freedom with a GPL license. </p>"},{"location":"Game%20Written%20in%20Rust/#early-development","title":"Early Development","text":"<p>It is still in early, or pre-alpha development, with many new features yet to be added. Because of this, it is not yet available on Steam until the developers feel there is enough content. Currently, the world is super massive yet feels a bit empty at the same time. If it were released as it, many Steam users would be disappointed which would leave a lasting negative impression. As a wise man once said, \"Delay is temporary, Suck is forever\". </p>"},{"location":"Game%20Written%20in%20Rust/#how-it-started","title":"How it Started","text":"<p>The Game started as a fan-made open source clone of Cube world in 2018. This was a game that many were excited about years ago but was infamous for it's huge disappointment and lack of features, where people spent 20 dollars, and received a half completed, shell of a game.  Veloren's future on the hand seems promising, with constant new updates and a large community of volunteer developers. And even though it started as a clone of another game, its evolved on it's own and now having it's own vision as an action RPG set in a fantasy world. </p>"},{"location":"Game%20Written%20in%20Rust/#gameplay","title":"Gameplay","text":"<p>Although arbitrary building and destruction such as in Minecraft are not intented as a primary feature of the game, there is some limited in game building with admin commands and permissions. There is also some destruction in the form of mining ore chunks and certain kinds of rocks with a pickaxe. Free building like in some sandboxes is not intended as it would effect the RPG element of the game, though they have mentioned you may be able to build within your own house or a town with pre-made assets. </p> <p>Later on when selecting a sceptor as a starting weapon there is some destruction elements I didn't realize, you can see here I'm able to destoy and burn up parts of these trees. The glider you see is inspired by breath of the wild, which is by far my favorite part of the game.</p> <p>Besides flying around in your glider, climbing is a central part of the game, which depletes your stamina. You can roll by clicking the mouse wheel, you allows you to travel a bit faster, this also allows you to side step to dodge in battle. If you have a melee weapon, you can block with alt, and parry if the timing is just right. Enemies drop plenty of resources, as well as things you can collect by hand such as rocks and twigs. In a crafting station of a village, you can create better weapons and armor to progress.</p> <p>To learn more about different items and things you can craft, the wiki is very helpful</p>"},{"location":"Game%20Written%20in%20Rust/#rust-design-and-techinical-details","title":"Rust, Design, and Techinical Details","text":"<p>Concerning the technical side of things, I'm excited to see Rust, as a memory safe and high performance language, prove it's utility in the game development world. Common problems with C and C++ include uninitialized variables, which Rust does not allow, likewise, manually allocating memory with malloc and free run the risk of use-after-free bugs, this is not a problem with Rust's approach to memory management system based on ownership and borrowing. </p> <p>Like the Bevy game engine, instead of a traditional object-oriented design with a hierarchy of classes with inheritance, the architectural pattern used in Veloren is an ECS, or Entity Component System, this is basically a design pattern optimized for modern CPUs where data is stored closer to where the CPU cache can readily access it. This maximizes cache locality and overall performance. </p> <p>And I can tell you, the performance is impressive. What I like most is that you can zoom out, really far and there were still no choppy frames and dull performance. Here I take my glider and fly down this mountain, since I'm in singleplayer mode, I can use admin commands to change the day and night as well.</p> <p>In this footage I'm in spectator mode just flying around, and you can get a sense of how big the world really is. I've teleported using the goto command specifying a set of grid coordinates. You can see as I descend that the render distance is pretty good.</p> <p>If you liked this video, consider liking and subscribing to support the channel, and if you want to try out Veloren, you can download the official game launcher that works cross platform, with fairly low minimum hardware requirement.</p> <p>That's all and thanks for watching!</p>"},{"location":"Game%20Written%20in%20Rust/#additional","title":"Additional","text":"<p>Great talk by a core developer on the history and design of Veloren </p> <p>Other resources:</p> <p>https://book.veloren.net/players/building.html</p> <p>https://www.reddit.com/r/Veloren/comments/wnon6q/will_there_be_building_and_breaking/</p> <p>https://www.reddit.com/r/Veloren/comments/k85d33/farming_mining_house_building_and_item/</p>"},{"location":"Golang/","title":"Golang","text":""},{"location":"Golang/#history-and-overview","title":"History and Overview","text":"<p>In 1970 Dennis Ritchie created a high level language called C to build the Unix operating system, previously written in assembly. Unfortunately passing away before the creation of Go, his colleagues Ken Thompson and Rob Pike developed Go, a modern C of the 21st century, publically released as open source in 2012. Modern object oriented languages continue to add eachothers features in an attempt to keep market share. This means they are growing in complexity while simultaneously becoming more similar to one another. This is bloat without distinction.  </p> <p>Go is an unapologetic boring language with an intended lack of features. This means code is written procedurally in a simple fashion. A single programmer could write an entire codebase, leave for 5 years, and his coworkers will easily pick up where he left off. Writing in Go means large code bases are readable. Files are read top to bottom without hunting down where a method is imported from, or traversing several files to find the original implementation in a nest of inheritance.</p>"},{"location":"Golang/#problems-of-c-addressed-with-high-level-features","title":"Problems of C Addressed With High Level Features","text":"<p>Though required to manage if absolute performance is necessary, the following are the main problems with writing code in C:</p> <ol> <li>Uninitialized variables</li> <li>Out-of-bounds</li> <li>Use-after-free</li> <li>Read or write to unintended object (memory safety bug)</li> </ol> <p>Go solves 1 by zeroing out newly created variables, so instead of unexpected behavior in C, Go will return a value of zero. Problem 2 is solved with bounds checking of arrays/slices to ensure index is not out of range. Next, without needing to malloc and free memory by hand, Go's garbage collector takes care of number 3. Finally, number 4 is prevented without the use of pointer arithmetic, though pointers are allowed and encouraged in Go, we cannot manipulate them.</p>"},{"location":"Golang/#learning-resource","title":"Learning Resource","text":"<p>Go is intentionally a conservative and minimal language that doesn\u2019t change quickly, however, here are two things to be aware of when reading The Go Programming Language book.</p> <ul> <li>Go Modules Replaced GOPATH</li> <li>Generics are not included</li> </ul> <p>Besides this there may be occasional deprecated functions from standard packages such as rand or crypto. Overall it is a highly recommended book as Brian Kerningham is renowned computer scientist, OG Unix developer and also author of The C Programming Language. Alan Donovan, co-author, is an engineer in the Go Team at Google.    Book PDF</p>"},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/","title":"How to Learn Assembly Language (And Why)","text":"<p>\ud83d\udcf9 Related Video: Watch the related video to this article here.</p>"},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/#tldr","title":"TLDR","text":""},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/#how","title":"HOW","text":"<ul> <li>lc3tutor.org for web based simulator and code snippets<ul> <li>or LC3Tools for a desktop application simulator</li> </ul> </li> <li>Introduction to Computing Systems Book, written by LC-3 creator</li> </ul>"},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/#why","title":"WHY","text":"<ul> <li>Gain appreciation of how computers work and shoulders of giants we stand on.</li> <li>Optimization scenarios such as digging into x86 of high level language to prevent bounds checking.</li> <li>There are few jobs where people write in assembly relative to high level languages, but they are compilers/toolchain developers, malware analysts, embedded development, and computer science teachers.</li> </ul>"},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/#how-i-learned-low-level-stuff","title":"How I learned Low Level Stuff","text":"<p>Machine Organization is an upper level Computer Science course at my university. Dreaded by many, understanding the computer down to the transistor is necessary as well as designing logic gates and writing assembly code. Programming in high level languages (which is all most know how to do) does not help prepare. The course starts with character and number representation in binary, as well as arithmetic operations (add, subtract) and logical operations (AND, OR). After we understand base 2 and can perform basic operations, Circuit Verse was introduced to build logic gates. Our final culmination was an ALU (Arithmetic Logic Unit), which is the heart of the CPU. After CircuitVerse, LC-3 was introduced.</p>"},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/#lc-3","title":"LC-3","text":"<p>LC-3 is a simulated instruction set architecture (ISA) and assembly language designed for educational purposes. It is hypothetical does not run on actual hardware as most ISAs do. Instead there are several simulators available. One is LC3Tools, a desktop application which requires an install. On Linux I downloaded a single AppImage and it worked flawlessly. Alternatively, there is a web based simulator on lc3tutor.org, as well as many great example code snippets demonstrated in the video.</p> <p>If you would like to gain an understanding of an even lower level than assembly to start from a truly bottom up approach, you can learn binary and build logic gates on CircuitVerse. This will help in understanding the inner working of LC-3, but is not necessary.</p> <p>Though very dense at nearly 800 pages, the textbook written by the original LC-3 creator is available. This was the reading material in my Machine Organization course, though most students only opened it up when they needed to reference an opcode.</p>"},{"location":"Language%20Benchmarking%20Pitfalls/","title":"Language Benchmarking Pitfalls","text":"<ul> <li>Garbage Collection Pauses stop the world pauses vs concurrent or background GC <pre><code> Thelongest single GC-related pause suffered by NGINX was\n115 microseconds; the longest observed sum of GC delays\nto a complete NGINX client request was 600 microseconds.\n</code></pre> ~ Usenix MIT 5 Page Summary Paper</li> <li>JVM Warmup  lazy loading of classes. JMH</li> <li>Every algorithm is different. <pre><code>different languages may excel at different kinds of algorithms or tasks, leading to skewed or incomplete conclusions about their overall performance.\nchat gpt\n</code></pre></li> <li>some languages optimize frequently used bits of code for warm paths as opposed to less frequently used cold paths. </li> </ul> <p>vercel app some github hyperfine Anton PutraAnton P</p>"},{"location":"Language%20Benchmarking/","title":"Language Benchmarking","text":"<p>\ud83c\udfc3 TLDR: Based on these benchmarks and my code to aggregate the CPU seconds column, Go and Java perform the same, C# performs twice as fast as both of them, Rust outperforms C# by a small margin. Python is 10x slower than Go or Java.</p> <p>My favorite bench marking is the Computer Language Benchmarks Game created by Debian developers. They test physics and astronomy simulations (N-body), various matrix algorithms, binary trees, regex, and more. Though you can do your best to interpret their mean results. I have written some awk and sed to provide another simpler metric comparing one language to another. </p> <p>Take this this data comparing Go and Python which I've copied directly from the website. </p> <pre><code>fannkuch-redux\nsource  secs    mem     gz  cpu secs\nGo #3\n    8.25    10,936  969     32.92\nGo\n    11.83   11,128  900     47.26\nGo #2\n    11.89   12,708  896     47.47\nPython 3 #6\n    913.87  11,080  385     913.83\nPython 3 #4\n    285.20  14,264  950     1,123.47\nn-body\nsource  secs    mem     gz  cpu secs\nGo #3\n    6.36    11,244  1200    6.37\nGo\n    6.55    11,244  1310    6.56\nGo #2\n    6.94    11,244  1215    6.95\nPython 3\n    383.12  11,096  1196    383.11\nPython 3 #2\n    402.59  11,096  1242    402.57 \n</code></pre>"},{"location":"Language%20Benchmarking/#processing-the-data","title":"Processing the Data","text":"<p>I only care about the <code>secs</code> to measure CPU time. So that means I can remove the names of the algorithms as well as the headers that include <code>source</code>, <code>secs</code>, etc. A single mention of <code>source</code> will remove the each entire record. Though there are none above, there are some tests that fail, we'll remove those and the line above them that contain <code>Python</code> or <code>Go</code>.</p> <pre><code>/source\\|binary\\|reverse\\|nucleotide\\|fasta\\|regex\\|pidigits\\|mandelbrot\\|spectral\\|n-body\\|fannkuch/d\nN; /Timed\\|Bad\\|Failed/{d;}; P; D\n</code></pre> <p>Now before I run my awk script to average out the first numbered field, I want to make sure <code>Python</code> and <code>Go</code> are on the same record as the numbers. In vim, <code>J</code> will remove a newline, basically moving current selected line up one. Let's run that on every line. Also the test numbers <code>#3</code> I want to remove as well as the <code>3</code> after Python. That will make it easier for awk to read just the data we want.</p> <pre><code>:g/^/normal J\n:%s/#.//g\n</code></pre> <p>After all that our data should look much cleaner.</p> <pre><code>Go 33.37    247,824     525     60.85\nGo 34.62    236,728     482     61.41\nPython 104.73   271,980     338     104.72\nPython 35.33    274,884     660     125.79\n</code></pre> <p>Now using awk I simply take the second field and average them for Go and Python separately.</p> <pre><code>awk '/Python/{python_sum+=$2; python_count++} /Go/{go_sum+=$2; go_count++} END{print \"Python Average: \" (python_sum/python_count); print \"Go Average: \" (go_sum/go_count)}' go-python.txt\n</code></pre>"},{"location":"Language%20Benchmarking/#final-results","title":"Final Results","text":"<p>Here is the result. I've also included some other languages as well. First I compare Go to three other languages, then C# against Java and Go in the last two. Not surprising to see Go 10x faster than Python, though I was shocked to see C# performed 2-3x faster than Go and Java. </p> <pre><code>Python Average: 106.756\nGo Average: 8.98625\n\nJava Average: 9.0565\nGo Average: 8.98625\n\nRust Average: 3.06823\nGo Average: 8.98625\n\nC# Average: 3.74485\nJava Average: 9.0565\n\nC# Average: 3.74485\nGo Average: 8.98625\n</code></pre>"},{"location":"Microsoft-Rust-Linux/","title":"Microsoft Rust Linux","text":"<p>These notes I sort of scribbled down basically show the growth of Rust as a language and Linux used in the cloud. </p>"},{"location":"Microsoft-Rust-Linux/#rust-timeline","title":"Rust Timeline","text":"<p>In 2019, Windows started experimenting with Rust by migrating low level Windows Components.</p> <p>In 2021, The Rust Foundation was founded, with Microsoft being a supporter donating $1 million a year.</p> <p>Mark Russinovich, CTO of Microsoft Azure, tweeted that the industry should declare C/C++ as deprecated and instead use Rust for any project that requires a non-garbage collected language.</p> <p></p> <p>In November, 2023, Microsoft announced they will spend $10 million to make Rust a first class language in their engineering systems.</p>"},{"location":"Microsoft-Rust-Linux/#linux","title":"Linux","text":"<p>In this article, announcements by Microsoft's Azure CTO are compared year to year of the number of Azure instances (VMs) that were Linux Based. 2015 - 25%  2017 - 40%  2019 - 53%</p> <p>Linux has continued to dominate Azure in recent years, with Microsoft continuing to support open source ever since Asp.Net was open sourced.</p> <p>Linux dominates the cloud, even at Microsoft.</p>"},{"location":"My%20Story/","title":"My Story","text":"<p>Hi, My name is Joseph. When I was growing up, I didn't have that many friends, like many that grew up in either the suburbs, or the middle of nowhere, I didn't want to be shuttled around in a minivan for 30 minutes just to see my friends or go to the gym, or just about any other possible social occurrence. I blame this exact reason for my failure to socialize, lack of transportation, Or I was just straight up autistic. Well either way, more time alone meant more time on my hobbies, or whatever particular interest I had at the time. Like breeding extinct dinasuars for the best genetics, in ark, crafting tools and building my own fortress in the sky(minecaft?), in ARK, and overall just making lot's friends, oh that was Ark too. Eventually I thought it would be cool to make my own game, to do this I had to learn programming languages. I already was into foreign languages at the time, having taught myself spanish, so I was like, how hard can this be? So I bought this book called Automate the Boring Stuff with Python, got a hang of it, and eventually, built this absolute masterpiece (Orc Blaster Music Plays, dramatic edits, IGN Review with poorly slapped on photo). Ok, it actually sucked, but it was pretty fun, and it ended up impressing this guy at a career fair to land me an internship. Anyways, knowing some Python, and also realizing programming languages are NOTHING like natural languages, I realized there was actually a lot of money in this field, like some kid (photo of chinese kid) spends 8 hours a day solving leetcode problems and lands a job at Google for 200k. What the fu-? (Orc blaster music starts) Wait, I started doing this for fun, and now I'm worrying about whether I can invert a binary tree to impress some interview, and basing every technology decision based on what  language or javascript frameworks other people are using? Well, I know getting a job is important, but I also still do these things for fun, how about we find an overlap? Then use the technology I would anyways at home all while getting paid for it? Okay, Python is awesome, it has a cool history, easy to learn and very clean code. Also Linux, very fascinating history and very easy to tinker with. As a bonus, when members of the opposite sex see you with all that fancy computer code and with this keyboard, it will act as natural birth control (girls, party meme). Okay, so I like, programming in Python and playing around with Linux, let's see how these rank on the job market. (Cool music plays while screenshots and montage play). Ah it feels good to have job security. (AI clips play). Well that's not very encouraging, except this guy, sells an AI product who's purchase depends the buyer being convinced of this narrative he is selling, meaning he has financial incentive to convince you programming is dead, and that you should instead use his language model building platform, so I'm not convinced. Even if this was the future, it would mean with enough \"prompt engineering\" I could have ChatGPT build my own Facebook web application, or seriously get into game development and create the next Elder Scrolls barely touching C++, is this really such a bad thing? If anything, more companies and startups will spring up, creating even more jobs. Plus, that, game I made that won IGN game of the year, well I could make that game even better, bugs could be fixed with a single prompt and I would bang my head against the wall that much less. So no, I'm not scared of AI, I got into programming because I love building things, and if anything, it will make building things even easier....</p> <p>Anyways, I know this video wasn't supposed to be about AI and it kind of went all over the place. But, if you should take anything away from this, Python and Linux are super fun to learn and will definitely get you a job, and you should not worry about ChatGPT stealing your job at Google. </p> <p>Okay that's all, if you liked this video, please SMASH the like button and subscribe.</p>"},{"location":"Obsidian/","title":"Obsidian","text":""},{"location":"Obsidian/#overview","title":"Overview","text":"<p>If you haven't been living under a rock this past year, you've likely heard of a new note taking software, Obsidian. At first, you may be skeptical, does the world really need yet another note taking app? Although I can't speak from experience and compare it to something like Notion, I can argue against another alternative, which I've seen the majority of my classmates use in university, which is, if other than paper an pen, simply type their notes into a word or google document. This is not a very good idea, and it's much better to have a dedicated note taking app, preferably in a lightweight, universal file format like markdown. This is all obsidian is at it's core, simply a markdown text editor.</p> <p>Beyond simply creating files, editing them, and organizing them in folders, there is a plethora of features that can easily intimidate new users. My advice is to ignore all of them. Simply use obsidian as it is, as a markdown text editor to manage your notes. All this requires is to learn basic markdown syntax and jump into writing whatever you want. I've been using obsidian for nearly a year, and guess what? I don't use graph view, templates  or even the command pallet. I simply haven't had a need for any of these features. Obsidian doesn't have to be complex, I recommend to think carefully about what it is you want out of the app, if you want a simple text editor like I've described, you shouldn't have to watch any videos, or read any tutorial, it should be very easy and straightforward. However, if you want more features, such as an integrated calendar, or drawing software, then it's worth tinkering around. </p> <p>One of the strongest features of obsidian is it's massive collection of over 1,000 plugins,  which are not created by the developers at obsidian themselves, rather the community of open source developers. Some examples of plugins are simple UI enhancements like a search bar for the settings (which I highly recommend), and more complex plugins for integrating other software like ChatGPT inside of obsidian. More plugins like these will continue to be developed by the community and enhance obsidian for the foreseeable future, as obsidian has seen consistent growth and likely has several more years before it even peaks in popularity. </p>"},{"location":"Obsidian/#my-plugins","title":"My plugins","text":""},{"location":"Obsidian/#full-calendar","title":"Full Calendar","text":"<p>I have tried every popular calendar plugin for obsidian, and I've found obsidian's full calendar to be the only real functional calendar. The other Calendar plugin by Liam Cain is simply a wrapper around your daily notes, and is only centered around those daily notes. I have never used this feature, and never will, so I need a better option. Full calendar fits that need as it is simply an integration of the well known open source full calendar project. It works well enough for mobile, although with the limited space of a vertical mobile screen, it only allows a 3 day view.</p>"},{"location":"Obsidian/#settings-search","title":"Settings Search","text":"<p>This is a simple plugin I recommend to anyone. By default obsidian's settings do not have a search feature, this plugin fixes that.</p>"},{"location":"Obsidian/#excalidraw","title":"Excalidraw","text":"<p>Excalidraw is a free and open source whiteboarding/sketching software that supports live collaboration. Although I have found myself to prefer using their free web application, I do still appreciate their plugin for obsidian. </p>"},{"location":"Obsidian/#sync","title":"Sync","text":"<p>Sync is the first paid for service which I use. I have several laptops and a phone which, at any point I want to open up obsidian right where I left off. At 8 dollars a month billed annually, I found it to be a great price for the value I receive. I would be cautious on Windows however, as the forced Onedrive syncing seems to conflict with Obsidian. Luckily I use Linux, and therefore have control over my operating system, and the syncing feature works seamlessly.    </p>"},{"location":"Obsidian/#publish","title":"Publish","text":"<p>Publish is Obsidian's second paid for feature, at the same price as Sync: 10 dollars per month or 8 per month billed annually. This provides me with the website you are currently reading this on, which is just another vault of mine. Basically, I just open up another note and go about writing as I do, and then hit publish from my vault, it's really that simple. It currently does not support are drawings, canvases, and many other features. However, if you are experienced with web development, you can use a custom publish.js and custom.css file for more control and functionality.  </p>"},{"location":"Software%20Licenses/","title":"Software Licenses","text":""},{"location":"Software%20Licenses/#overview","title":"Overview","text":"<p>Software licenses are can be thought of on a spectrum of restrictive on one side and permissive on the other. Restrictive licenses support copyleft, meaning software must remain open source upon redistribution. Permissive licenses are more lenient and often allow businesses to use open source software for their own proprietary distributions.</p> <p>In general, all open source licenses roughly follow the four fundamental software freedoms, which is to run, study, modify and redistribute. Access to the source code is a precondition to both study and modify.</p> <p>GPL are the only licenses known as copyleft. While licenses such as Apache, BSD, and MIT are more permissive and business friendly. Apache covers patent issues, while others do not. BSD and MIT are the most similar with a minor difference of how they treat documentation.</p> <p></p>"},{"location":"Software%20Licenses/#gpl3","title":"GPL3","text":"<p>Linus Torvalds criticizes GPL3 for the naming used. He understands why the Free Software Foundation wanted a more restrictive license in the wake of TiVo, but he believes the GPL3 is fundamentally different than GPL2 and should be an entirely new license. To Torvalds, the GPL2 is as simple as giving someone source code and receiving changes/improvements in exchange. In contrast, the GPL3 introduces new rules such that using software on your own device forces you to follow more rules (stricter copyleft). At nearly double the length of the GPL2, GPL3 is much more comprehensive but fundamentally different license in Torvalds' view which overreaches in scope. </p>"},{"location":"Software%20Licenses/#agpl","title":"AGPL","text":"<p>The Aferro Gnu Public License is much simpler than the radical changes that GPL2 introduced. Instead of a completely rewritten work, the AGPL simply added one addition to the GPL2. This addition states that software ran over a network counts as distribution. This closed a loophole where SaaS providers were exempt from GPL conditions as it wasn't considered \"distribution\" in a traditional sense.</p> <p>Hopefully I will get around to documenting other licenses. Besides software licenses, the Creative Commons is a license that also supports open source values.</p>"},{"location":"Stop%20Leetcode/","title":"Stop Leetcode","text":"<p>Here is a collection of my notes for an article and video which is are both in development. It is essentially against the notion that algorithmic problem solving/competitive programming skills correlate with real world development skills. </p> <p>Many companies, notably FANG and large tech companies, must filter through thousands of applicants, so they must cut through the herd in one way or another. It is similar to filtering only those who have a computer science degree. Even though having a degree may or may not translate to being a more competent developer, it is still practical to apply some potentially arbitrary constraint to bring the number of applicants to a managable number.</p> <p>In this short 2 minute video by The Primeagen, a famous youtuber who also is a software developer and hiring manager at Netflix, discusses how he has seen his company move away from focusing on hard technical, Leetcode-like problems and towards basic data strucures (linked list, heaps,graphs etc.). Secondly he goes on to explain the importance of having a visible project on Github that actually solves a real problem.</p> <p>Article about Google conference mentioning why competitive programmers make worse developers as they are used to cranking solutions out fast instead of reflective thinking over problems.</p> <p>Topic starts at 1:11:00 in the video</p> <p>\"being a winner at programming contests was a negative factor for performing well on the job\"</p> <p>Overall, assuming competetive programming/leetcode skills correlate with real talent as a developer is the same as assuming spelling bee champions make the best writers and journalists.</p> <p>The entire fact that there are courses and services around passing interview problems is indicitive of the fact they are heavily flawed. If I spent all my time working on personal projects, or contributing to an open source project writing real software, I may still perform poorly on a question about inverting a binary tree or some other obscure problem. This is because these problems are very rare outside of whiteboarding style interviews or data structure classes in university. Real world development involves navigating around large codebases, writing understandable, well commented code and documentation, as well as communication. Above all, software development is about solving real world problems and helping people. Solving obscure puzzles to impress an interviewer accomplishes neither of these things.  </p>"},{"location":"The%20Notion%20of%20Abstraction/","title":"The Notion of Abstraction","text":"<p>Abstraction is hiding information to simplify a set of instructions. For example, you are not aware of the inner workings of the internal combusion engine or electrical system in your car. Your instructions are to turn on the ignition, switch the gear and press the pedal to go. No information or instructions are necessary pertaining to the inner workings of the car. Even further abstraction would be someone telling you to come to their house. Getting in your car, turning on the ignition and all the rest is implied with this single instruction. </p> <p>With computers, there are many layers of abstraction one must understand. From the CPython bytecode that is excecuted by C, to the assembly code that is further generated from the C compiler, layers, or steps (whichever you like to visualize), are present. To be most productive, one must hide this information as much as possible and only worry about theory when the machine stops doing what its supposed to. This is top-down design. However, for learning sake and gaining an intuitive understanding of software, bottom-up learning is essential. </p> <p>Legendary computer scientist Yale N. Pratt and creator of LC-3,discusses the notion of abstraction in his book, which is the reading material in many universities.</p>"},{"location":"Web%20Servers/","title":"Web Servers","text":"<p>\ud83d\udcdd\ufe0f Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also see the video Web Servers and Rise of Javascript and NodeJS.</p>"},{"location":"Web%20Servers/#overview","title":"Overview","text":"<p>To understand modern web servers and rise of dynamic applications with asynchronous Javascript, we have to understand the web before any such thing existed and to find a point in history to begin.   </p> <p>Since a comprehensive history of the web would require going back 30 plus years to the original invention of the world wide web, early browsers such as Mosaic, Netscape, and even more ancient web servers, we will begin in 1995 with the release of Apache HTTPd, which after nearly 3 decades, still powers over 23% of the top million busiest websites.</p>"},{"location":"Web%20Servers/#apache","title":"Apache","text":"<p>The original Apache developers, started as a group of webmasters, now an outdated term, at the National Center for Super computing Applications or NSCA, where much of the pioneering of the early internet began. The previous web server NCSA HTTPd, needed several patches and improvements, and eventually, Apache was built as a general overhaul and redesign. Still, 30 years later, the main function of serving static HTML files to clients hasn't changed. Over HTTP, or hyper-text-transfer-protocol, a client, aka web browser, issues a GET-request and a server responds with HTML files and other resources that make up the web page presented to the user.</p> <p>For this simple purpose, Apache did great, however, with the growing popularity of the internet, and more and more people watching porn, looking at cat pictures, and ordering dog food, the issue of scalability arose. A single server running can only handle so many requests per second as well as concurrent connections. This become known as the C10K problem, the major challenge for computing in the late 90s and early 2000s where handling over 10 thousand concurrent connections was impossible. Only one man was capable of solving this problem, a Russian system administrator by the name of Igor Syostev. </p>"},{"location":"Web%20Servers/#nginx","title":"Nginx","text":"<p>Responsible for running servers for the top websites in Russia at the time, as well as Rambler, a Russian search engine and web portal, Igor experienced the scalability issues of Apache first hand. In his spare time, he began writing drafts for NGINX in 2002, focusing on handling many simultaneous connections with an asynchronous event driven approach. After 2 years, it was publically released in 2004. Fast forward to 2019 and 66% of top 10k sites use NGINX according to W3 Techs. There are of course several other roles it plays besides serving static content such as HTML and other resources to clients. One, being a reverse proxy, which acts as a gateway between the mass of clients and web servers. This acts as an intermediary load balancer that also can enforce security by directing traffic through firewalls.</p> <p>With Apache and Nginx being used primarily as web servers at the time, Javascript and event driven programming was growing in popularity. Despite this, sequential programming and blocking I/O was still a problem with web servers. For example if a server is waiting for one function of code to finish, this may block user requests or database queries, slowing an application down. Inspired by Nginx, American software engineer Ryan Dahl had a solution: a vision of a purely event driven, non-blocking web server. </p>"},{"location":"Web%20Servers/#nodejs","title":"NodeJS","text":"<p>Ryan was inspired by Nginx and suggests scenarios where they could be used together, Apache on the other hand he criticized for it's limited design where an entire thread is created per connection.</p> <p>Ryan explains pitfalls of blocking, sequential programming and cultural bias where functions with callbacks are seen as too complicated and as nested \"spaghetti code\" in this excerpt.</p> <p>Though Node JS has proven to be highly performant and reliable, it may come at a cost of complexity and code readability, with many preferring threaded servers and sequential code where you can easily read a file top to bottom without jumping between Javascript's callback functions.</p> <p>After these early criticisms of Javascript, there were improvements such as Promises in 2015 with the release of ES6. Promises are a abstraction build on top of callbacks to better handle async code and prevent callback hell. Finally the latest evolution is async/await which is essentially syntactic sugar for promises, completely optional and fully compatible with promises.</p> <p>So clearly, the history of asynchronous JavaScript is a bit messy, but that's the story of any technology that's been around for 25 plus years in an ever changing landscape such as the web.</p> <p>Besides the async/await pattern found in almost every major language today, another model of concurrency that seems promising is in the Go programming language with it's goroutines and channels which has proven useful in modern web servers in recent years. But that's a topic another video.</p> <p>Thank you for watching and don't forget to like, comment and subscribe to support more content like this.</p>"},{"location":"Web%20Servers/#additional-unorganized-notes-or-not-included-in-video","title":"Additional Unorganized Notes Or Not Included In Video","text":"<p>So to wrap everything up, the earliest web server still in wide use today is Apache, which saw limitations in scalability, known as the C10K problem, NGNIX came as a solution with it's asynchronous, even driven approach. As the web evolved, the demand for... interactive web pages, non-blocking, </p> <p>Explain promises and then async/await a bit more. Then summarize everything, maybe somehow slip in Go's concurrency compared to async await. Perhaps hype up some a concurrency model different than async/await created from the greatest engineers in the world who created C and Unix, just mention it and then leave them on a cliffhanger and say that will be a future video. </p>"},{"location":"Web%20Servers/#additional-notes","title":"Additional Notes","text":"<p>2012: Apache 2.4 released with larger selection of MPMs, reverse proxy impovements, and additional load balancing mechanism according to Timeline</p> <p>Apache has Prefork and Worker MPM (Multi Processing Module)</p> <p>Though primarily a Javascript runtime, like the JVM is for Java, or CPython is to Python, NodeJS can be used to create a web server, and very often is. And in that sense, comparable to Apache and Nginx.</p> <p>For serving static content, Apache or NGINX do fine on their own, for dynamic server-side functionality like interacting with databases, or a real time chat application, Node shines best. </p> <p>Sometimes Node and Nginx can be used together, which node containing the code base of an application, while Nginx sits infront handling load balancing and caching.</p> <p>Ryan Dahl in 2010: \"Apache creates a thread per connection\" link</p> <p>What is the difference between nodejs and nginx? See Reddit post. NodeJS vs Nginx Reddit</p>"},{"location":"Welcome/","title":"Home","text":"<p>Welcome to my Website! This will serve as the place for my more formatted and organized writing.</p> <p>I am an undergraduate computer science student who has been programming and taking apart computers since I was was young. Linux has been a major interest as a hobby as well as in professional pursuit of certifications and employment. Various other interests are as follows:</p> <ul> <li> <p>Obsidian, a note taking app which I created my own plugin for, which measures aggregated file sizes (over 2k downloads).</p> </li> <li> <p>Godot, an open source game engine. I am specifically interested in creating 2d gravity simulations like this.</p> </li> <li> <p>Building websites with zero to minimal Javascript, such as simple staticly generated websites that you are likely reading this from.</p> </li> <li> <p>Ruby and Go. Both cool programming languages.</p> </li> </ul> <p>Many articles, notes, pages, etc. on here will have an accompanying youtube video, and vice versa. You can see my youtube channel here</p>"},{"location":"Why%20Learn%20Vim/","title":"Why Learn Vim","text":"<p>\ud83d\udcdd\ufe0f Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also see the video  Why and How to Learn Vim/Neovim.</p>"},{"location":"Why%20Learn%20Vim/#history","title":"History","text":"<p>Vim is a terminal-based text editor that dates back to it' predecessor, Vi, first released in 1976. At this time, during Unix and early days of MSDos, text based interfaces were predominant, So Vi's development was influenced by the environment of that era, where interacting with computers was done mostly with keyboards rather than mice. It wasn't until the 1980s with the rise of Graphical User Interfaces where Mice become popularized. Despite this, Vim, which stands for Vi improved, and's it's successor Neovim,  remain popular, with people choosing to ditch mice  for full control from their keyboard. </p> <p> Vi dates back nearly 50 years, with Neovim (2015) as it's most recent successor.</p>"},{"location":"Why%20Learn%20Vim/#why-vim-is-so-popular","title":"Why Vim is So Popular","text":"<p>It's popularity is likely due to two reasons. The first being a practical reason, that, when surpassing the inevitable learning curve, you eventually become more productive at navigating through files and directories, editing text files, creating macros to automate tasks, and so on. Besides the utility, the configuration of Vim and Neovim appeals to a particular audience, those who enjoy modifying and sharing their Vim and Bash RCs, or configuration files with others. There's a certain itch for people that's scratched when they have full granular control over applications, this is what some people call a PDE, or Personalized Development Environment, as opposed to a regular IDE, or Integrated Developer Environment, such as PyCharm and VScode. </p> <p>I happen to fall in into the former utility based group, so although I can't show you fancy ways to customize or install any additional plugins, what I can demonstrate is straightforward examples of Vim being more efficient than the typical clicking and copy/paste with your mouse.</p>"},{"location":"Why%20Learn%20Vim/#netrw","title":"Netrw","text":"<p>Firstly we have that weird screen that opens up when you accidentally run vim on a directory instead of a file. That is netrw, or network read and write. As the name suggests, it was originally intended for reading and writing files over a network, but most use it for local browsing to have have your own file explorer all within Vim. Lately I much prefer it over the traditional GNU CoreUtils, instead of moving or renaming a file with mv,  sorting with various ls flags, or typing a command longer than I'd like, in netrw, I'm one key away. Toggling with with i until we reach the tree view, allows me to toggle what's inside each directory. If I find a file I want to inspect, I'll hit v to open a visual split, when done, I could either ZZ, or :wq to save and quit or CTRL + w + q for window quit.</p> GNU CoreUtils Netrw Description mkdir d Make directory touch % Create empty file rm or rmdir D Remove file or directory mv [file or dir] R Move or rename file or directory ls -S, -t s Sort by size, time, etc. cd [dir] [Enter] Move into directory cd .. - move up a directory <p> Inside Netrw</p> <p>See full video for more Vim hotkeys and shortcuts.</p> <p>So although all of these are a lot to memorize, with actual practice, muscle memory quickly takes over, and quickly managing files and directories, and editing files, becomes effortless from the keyboard. Your hands will remain more stationary and your eyes fixed on the screen, moving them less often to find your mouse, in a brief, periodic, and distracting instant.</p>"},{"location":"Why%20Learn%20Vim/#vim-in-vscode","title":"Vim in VSCode","text":"<p>Unfortunately, when working collaboratively at your job, you will likely be the odd one out using Vim instead of VSCode. Many jobs even require you to use a certain IDE with their list of extensions. Luckily in my case, VScode has a Vim emulator that behaves just as you'd expect. The only downside is the occasional conflict of hotkeys. </p> <p> Vim Emulator Extension in VScode</p> <p>So overall, given Vim's long history as an effective editor, it will likely stick around for many more years in the future, and therefore worth investing your time in learning. For nearly 50 years, the same insert, normal, command mode and basic motions have stuck around, and are now portable across IDEs in the form of extensions and plugins. Even my note taking app, Obsidian, has a optional Vim mode built in. </p> <p>So even if you don't want to install and configure vim or neovim, chances are, an application you're using now, supports basic motions and keybinding, which is a great way to practice, </p> <p>If you liked this video, consider subscribing, liking or commenting to support the channel and more content like this. Thanks for watching.</p>"},{"location":"image-test/","title":"Image test","text":""},{"location":"todo/","title":"Todo","text":"<ul> <li>[x] finish every programming language video</li> <li>[ ] host current statsfordevs files on github pages for quick feedback</li> <li>[ ] post statsfordevs&gt;about on reddit for feedback </li> <li>[ ] webrtc video chat demo app</li> </ul>"},{"location":"todo/#svg-side-panel","title":"SVG Side Panel","text":"<p>swappable side panel, fhs with directories nested in svg in web/Linux-Roadmap/svg.html on ideapath Linux mint each clickable element in svg has 5 elements to be added with xml editor four htmx attributes hx get, trigger, swap, swap, target css class of \"clickable\" to use opacity: 0.7 on hover</p> <p>above should be simple and independent of of the sidepanel, that is the actual hard thing</p>"},{"location":"todo/#statsfordevs","title":"statsfordevs","text":"<p>get python and go html file with two javascript chart files from web/tech-wiki</p>"},{"location":"todo/#things-i-care-about","title":"Things I care about","text":"<ul> <li>Linux and open source</li> <li>Javascript Fatigue Solution: Vanilla JS and/or Hypmermedia Approach (HTMX)</li> </ul> <p>I don't have enough experience to give much commentary about Javascript Fatigue, so I should just build stuff to get experience.</p> <p>I enjoy history of software, such as NGINX and Node. I write about these things and can make content. I should write about them and host the files on my web server.</p> <p>I'm against overly complex web applications, I have personal experience testing SPA apps. </p> <p>Though Javascript powers the web and is useful. My previous take, in which I criticized Javascript for being a flimsy scripting language was naive.</p> <p>Despite this, the Javascript ecosystem is a bit of a mess. This talk by the creator of Django criticizes the large number of Javascript frameworks and complexity of learning web development. Though the talk is 6 years old, it is just as relevant. He suggests using vanilla Javascript. This funny [website] pokes fun at framework developers by suggesting \"Vanilla Javascript\" as the most lightweight and easy to use framework that browsers have been supporting for over two decades.</p> <p>Perhaps Javascript isn't so bad, it's just the culture around using pre-built frameworks and installing thousands of NPM packages. People want applications delivered fast, which leads to tech debt. Regardless of what technology is used, this is just human nature of wanting things sooner and lacking foresight.</p> <p>JS Fatigue - sense of being overwhelmed by growing javascript ecosystem</p> <p>Antidote to Javascript fatigue is using vanilla JS? A hypermedia approach with HTMX? Is there a similar type of fatigue in other areas of tech besides web development, like sysadmin, non-web software development, etc?</p> <p>Node is a JS runtime that can be used to build a web server, can also be thought of as an application server/framework.</p> <p>Like the creator of NGINX, Ryan Dahl, creator of Node.js, criticized Apache's limited ability of handling 10k + concurrent connections. Ryan also criticised the then dominant sequential programming paradigm. Nowadays non blocking i/o is the standard, this was not the case in 2008 and before.</p> <p>My goal with Node.js was to force developers to easily build optimal servers by only exposing async i/o (atleast for network i/o) Ryan Dahl</p>"},{"location":"Bash/Backticks/","title":"Backticks","text":"<p>Back ticks in bash are for command substitution, which is when you want to capture the output of a command, and store it in a variable.</p> <p>For example, running the <code>date</code> command in any Linux terminal will output the date in a long format. <pre><code>date\nWed Sep 13 06:41:48 AM CDT 2023\n</code></pre></p> <p>However, if we create a file <code>backticks.sh</code> and wanted to store that output in a special format inside our own variable, we can do that within back ticks. <pre><code>#!/bin/bash\nmy_date=`date +%m-%d-%Y`\necho \"You accessed this date on $my_date\"\n</code></pre> Output <pre><code>You accessed this date on 09-13-2023\n</code></pre></p> <p>Now let's get a little more advanced. First lets create three files, then write the text \"secret\" to the second file. We can search each file with <code>grep [options] [file]</code> and use the wildcard/star <code>*</code> to find all 3 files. We can see the output of <code>l</code> omits the regular output and just prints the file which contains the text, that's what we want. <pre><code>touch file1 file2 file3; echo secret &gt; file2\ngrep -l secret file*\n</code></pre></p> <p></p> <p>Let's write a script that uses the output of the second grep to automatically open our text editor with the file that contains the secret. Using any text editor,  wrap that command in the back tick.</p> <pre><code>#!/bin/bash\nvim `grep -l secret file*`\n</code></pre> <p>Now running the script with <code>bash backticks.sh</code> is equivalent to <code>vim file2</code> or opening whichever file has the secret. </p> <p>For a third example, I want to count how many markdown files are in my Obsidian vault, which contains all of my notes. Before we capture any new command in back ticks, let's first understand what we'll be using. <code>find</code> is a very useful tool in Linux an does just what it says, goes and finds files and directories. It has loads of options and it's man page is over 1,000 lines long, but we'll just stick with <code>-type f</code> to specify we are looking for files and <code>name \"*.md\"</code> to only return markdown files.</p> <p>If you would like to follow along, you can either find your own directory with all of the same file endings, or just <code>touch</code> a bunch of text files and replace the target directory and <code>name</code> with <code>\".txt\"</code>.</p> <p>After running the <code>find</code> command with the correct<code>type</code> and <code>file</code> options, I get very verbose output. </p> <p></p> <p>Next, since I know each instance of a markdown file is separated by a newline which I can use word count <code>wc</code> with the <code>-l</code> option to print the number of newline counts.</p> <p></p> <p>OK, we finally have what we want, now let's put this in a bash script. First, what we had before was recursive by default, meaning it continuously searched directories withing directories, but we can give the user the option by setting the <code>-maxdepth</code> to 1. This entire command is then wrapped in back ticks to be stored in the <code>count</code> variable.</p> <pre><code>read -p \"Recursive? y/n &gt; \" option \nread -p \"Enter directory path &gt; \" directory  \nif [ \"$option\" == \"y\" ]; then     \n    count=`find \"$directory\" -type f -name \"*.md\" | wc -l` \nelif [ \"$option\" == \"n\" ]; then     \n    count=`find \"$directory\" -maxdepth 1 -type f -name \"*.md\" | wc -l` \nelse     \n    echo \"Invalid option.\" \n    fi  \necho \"Count of .md files: $count\"``\n</code></pre>"},{"location":"Bash/More%20Bash%20Examples/","title":"More Bash Examples","text":""},{"location":"Bash/More%20Bash%20Examples/#add-text-to-all-files","title":"Add text to all files","text":"<p><pre><code>#option 1\nfor filename in *.md; do\n    echo \"for loop test\" &gt;&gt; \"$filename\";\n done\n\n# option 2\necho \"my text\" | tee -a *.md\n</code></pre> Named after the T-splitter in plumbing, <code>tee</code> \"splits\" to the standard output and the file to write to. The <code>-a</code> is for append, rather than overwrite. So the for loop and this command do the same thing, <code>tee</code> just prints out \"my text\" on top of appending the files. </p>"},{"location":"Bash/More%20Bash%20Examples/#for-loop","title":"For Loop","text":"<pre><code>for i in {1..5}\ndo\n\u00a0\u00a0\u00a0echo \"Welcome $i times\"\ndone\n</code></pre>"},{"location":"Bash/More%20Bash%20Examples/#make-three-files-and-search-all-with-grep-then-remove","title":"Make three files and search all with grep , then remove","text":"<pre><code>touch testfile1; echo I am file 1 &gt; testfile3\ntouch testfile2; echo I am file 2 &gt; testfile2\ntouch testfile3; echo I am file 3 &gt; testfile3\ntouch testfile4; echo I am file 4 &gt; testfile4\n# OR\nfor i in 1 2 3 4\ndo\n  touch \"testfile${i}\"\n  echo \"I am file ${i}\" &gt; \"testfile${i}\"\ndone\n\ngrep file testfile1 testfile2 testfile3\n\nrm -v testfile*\n#OR\nrm -v te?tf?l?*\n# ? is a single character wildcard\n</code></pre>"},{"location":"Bash/More%20Bash%20Examples/#cat-concatonate","title":"Cat (concatonate)","text":"<p><pre><code>#concatonate files\necho \"This is file1.\" &gt; file1.txt\necho \"This is file2.\" &gt; file2.txt\ncat file1.txt file2.txt &gt; combined.txt\n\n#print to stdout\ncat combined.txt\n</code></pre> <code>cat</code> can both be used to concatenate files or print to standard out (stdout), which is where you see it more commonly used.</p>"},{"location":"Bash/More%20Bash%20Examples/#while-loop","title":"While loop","text":"<p>Simple CPU monitor that prints out the load average every 3 seconds continuously.</p> <pre><code>while true; do\nclear\ncat /proc/loadavg\nsleep 3 \n</code></pre>"},{"location":"Bash/More%20Bash%20Examples/#vs","title":"&amp;&amp; vs ;","text":"<ul> <li>semicolon runs sequentially</li> <li>&amp;&amp; runs only if the previous on succeeds <pre><code>asdlkfjaaasldkfqwkler ; pwd\n# will run\n\nalasdfjanmsdqwuer &amp;&amp; pwd \n# will NOT run\n</code></pre></li> </ul>"},{"location":"Bash/More%20Bash%20Examples/#pretty-colors","title":"Pretty Colors","text":"<p>Green <pre><code>echo -e \"\\033[32;1mHello\"\n</code></pre> Red <pre><code>echo -e \"\\033[31;1mHello\"\n</code></pre></p>"},{"location":"Bash/More%20Bash%20Examples/#downloading","title":"Downloading","text":"<ul> <li><code>tar</code>: only supports .tar files<ul> <li>historically from tape archive, the name somehow stuck</li> </ul> </li> <li><code>wget</code>: World Wide Web get<ul> <li>primarily for downloading</li> </ul> </li> <li><code>curl</code>: Client URL<ul> <li>supports more protocols than wget (SCP, SFTP, SMB)</li> <li>often considered better for scripting due to its return codes and more output options.</li> <li>can download and upload</li> </ul> </li> <li><code>dpkg</code>: Debian packages<ul> <li>for .deb packages on Debian systems (Ubuntu, Linux Mint) </li> </ul> </li> <li><code>apt</code>: advanced package tool<ul> <li>will install all dependencies, not just single package</li> </ul> </li> </ul>"},{"location":"Bash/Python%20vs%20Bash/","title":"Python vs Bash","text":""},{"location":"Bash/Python%20vs%20Bash/#overview","title":"Overview","text":"<p>Python and Bash are both powerful languages and well worth learning. Before learning either, you should understand where they are and are not comparable. Unlike Bash, Python is used for data science, machine learning, and back end web development. Bash, being the default shell on Linux, can be ran directly from the terminal. If you've ever used Linux, the common \"Linux\" commands (historically UNIX programs)  such as <code>cd</code> or <code>ls</code> are really just writing bash directly in the terminal. This cannot be done with Python, which requires a <code>.py</code> file which must interpreted by a runtime. </p> <p>Basically, this means Bash scripts can often be more concise for simple tasks, but Python offers greater flexibility and is easier to integrate into larger projects. The two common libraries to do this are the os and pathlib library. Both serve to interact directly with you operating system, whether on Windows, Macos, or Linux. The pathlib is a more modern, object oriented library with more concise syntax. The os library is lower level with broader capabilities, such as process management and environment variable access. pathlib is exclusively designed for file and directory management, making it the better option.</p> <p><code>os.listdir</code> scans through the directory, and a list comprehension filters out files ending with <code>.md</code>. The <code>os.walk</code> function uses a recursive search, navigating through all sub directories. Notice how verbose and nested the function bodies of these functions are compared to pathlib.</p>"},{"location":"Bash/Python%20vs%20Bash/#python","title":"Python","text":""},{"location":"Bash/Python%20vs%20Bash/#os-library","title":"OS Library","text":"<pre><code>import os\n\ndef count_md_files(directory):\n    return len([f for f in os.listdir(directory) if f.endswith('.md')])\n\ndef count_md_files_rec(root_dir):\n    count = 0\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        for filename in filenames:\n            if filename.endswith('.md'):\n                count += 1\n    return count\n\noption = input(\"Recursive? y/n &gt;\")\ndirectory = input(\"Enter directory path &gt; \")\n\nif option == 'y':\n    print(\"Number of .md files:\", count_md_files_rec(directory))\nif option == 'n':\n    print(\"Number of .md files:\", count_md_files(directory))\n</code></pre> <p>pathlib uses <code>Path().glob</code> for a non-recursive search and <code>Path().rglob</code> for a recursive search. Notice from the <code>option</code> assignment to the end of both files they are exactly the same. The function body is all that is different. With the pathlib functions, it is much more concise, with one line of code each. </p>"},{"location":"Bash/Python%20vs%20Bash/#pathlib-library","title":"Pathlib Library","text":"<pre><code>from pathlib import Path\n\ndef count_md_files(directory):\n    return sum(1 for _ in Path(directory).glob('*.md'))\n\ndef count_md_files_rec(directory):\n    return sum(1 for _ in Path(directory).rglob('*.md'))\n\noption = input(\"Recursive? y/n &gt; \")\ndirectory = input(\"Enter directory path &gt; \")\n\nif option == 'y':\n    print(\"Number of .md files:\", count_md_files_rec(directory))\nelif option == 'n':\n    print(\"Number of .md files:\", count_md_files(directory))\n</code></pre>"},{"location":"Bash/Python%20vs%20Bash/#bash","title":"Bash","text":"<p>Now we have seen the python files, let's try the exact same task in Bash. As mentioned in the beginning, the fundamental difference is that Bash is not only a language, but also the shell, which is just the interface between the user and the kernel and executes programs called commands. This shell, bash being the most common, is accessed through the terminal. And through the terminal, any commands, even a series of commands at once (a script) , can be run directly in the terminal without needing to create a separate file. This means you have more direct and immediate access for troubleshooting, administration, or short tasks such as managing files, in this case, counting <code>.md</code> files.</p> <p>Each of these, one counting recursive by defualt, the other with a <code>maxdepth</code> of 1, can be ran directly in the terminal, getting immediate results. We specify the <code>-type f</code> for files, as opposed to directories, or anything else. We use <code>-name \"*.md\"</code> to acces only files with a name that ends in <code>.md</code>, the asterisks is just a wildcard, for any character or series of characters. We then pipe <code>|</code> that output to the word count<code>wc</code> program, which despite the name, is a more general program that can count many things, including in this case, lines with the <code>-l</code> flag. </p> <pre><code># the find command in recursive by default\nfind ~/documents/sync-main/todo/ -type f -name \"*.md\" | wc -l\n# set maxdepth 1 to only search the given directory.\nfind ~/documents/sync-main/todo/ -maxdepth 1 -type f -name \"*.md\" | wc -l\n</code></pre> <p>If we want a more interactive program that prompts a user, like the python scripts, we can <code>read</code> a variable called <code>option</code> and <code>directory</code> with a prompt specifies with the <code>-p</code> flag. Then run some basic if-else statements. If we really wanted to make it exactly like the python files, we could use functions, which Bash supports, but I found this to be more concise for this tutorial.</p> <pre><code>#!/bin/bash\n\nread -p \"Recursive? y/n &gt; \" option\nread -p \"Enter directory path &gt; \" directory\n\nif [ \"$option\" == \"y\" ]; then\n    find \"$directory\" -type f -name \"*.md\" | wc -l\nelif [ \"$option\" == \"n\" ]; then\n    find \"$directory\" -maxdepth 1 -type f -name \"*.md\" | wc -l\nelse\n    echo \"Invalid option.\"\nfi\n</code></pre>"},{"location":"Bash/Python%20vs%20Bash/#key-takeaways","title":"Key Takeaways","text":""},{"location":"Bash/Python%20vs%20Bash/#python_1","title":"Python","text":"<ul> <li>Python is used for Data Science, Machine Learning, and Backed web apps, and is only comparable to Bash for scripting purposes.</li> <li>Python offers two main libraries for file management: <code>os</code> and <code>pathlib</code>. <code>os</code> is older and has broader capabilities, while <code>pathlib</code> is modern and focuses solely on file and directory manipulation.</li> </ul>"},{"location":"Bash/Python%20vs%20Bash/#bash_1","title":"Bash","text":"<ul> <li>Bash serves two roles\u2014it's both a scripting language and the default shell for Linux. This gives it more direct access to system functionalities, allowing you to run scripts and commands directly from the terminal.</li> </ul>"},{"location":"Bash/Python%20vs%20Bash/#conclusion","title":"Conclusion","text":"<ul> <li>if you can solve the problem at hand using only calls to command-line tools, you might as well use Bash. But for times when your script is part of a larger Python project, you might as well use Python.</li> </ul>"},{"location":"Bash/Scripts%20From%20A%20Real%20Project/","title":"Scripts From A Real Project","text":""},{"location":"Bash/Scripts%20From%20A%20Real%20Project/#overview","title":"Overview","text":"<p>In this article you will find several bash scripts that I will walk through. These scripts were part of a real project and hopefully you will learn more with a real life example than a simple tutorial, but first we need context. </p> <p>Obsidian is a markdown based note taking app with a large ecosystem of over 1,000 3rd party extensions written by open source developers. Having my own idea of tracking disk usage by folders, filetypes and plugins, I decided to create my own. On Linux, I had Bash ready at my command to easily write these scripts, since Obsidian is all markdown based, all I would need to do is write all of this data to a <code>.md</code> file, such as the following:</p> <p></p> <p>This would then render a chart using Mermaid inside of Obsidian.</p> <p></p>"},{"location":"Bash/Scripts%20From%20A%20Real%20Project/#disk-usage-by-folder","title":"Disk Usage by Folder","text":"<p>To start the bash script, we must first learn about the <code>du</code> (disk usage) command. <code>du</code> followed by a directory will output the directory and size in bytes. I only want to display the first level of directories, so I will use the depth <code>-d</code> flag telling it to only go 1 level deep.</p> <pre><code># du -d 1 sync-main/\n\n552     sync-main/Academic\n968     sync-main/Notes\n5672    sync-main/.obsidian\n232     sync-main/Personal\n72      sync-main/Military\n12      sync-main/Daily\n188     sync-main/Professional\n364     sync-main/todo\n8072    sync-main/\n</code></pre> <p>Although you can use the human readable <code>-h</code> flag to read kilobytes or megabytes, we will need the raw bytes as a universal measure which will calculate the percents on the chart you see above.</p> <p>The problem is, the output will look a bit different. You see, this script sits in a plugin folder which is exactly 3 layers below where we need to measure <code>du</code> from, which is the root of the Obsidian vault.</p> <p><code>[ROOT-OF-VAULT]/.obsidian/plugins/disk-usage/script.sh</code></p> <p>This measures from the root of the vault. <pre><code># du -d 1 ../../../\n\n552     ../../../Academic\n968     ../../../Notes\n5672    ../../../.obsidian\n232     ../../../Personal\n72      ../../../Military\n12      ../../../Daily\n188     ../../../Professional\n364     ../../../todo\n8072    ../../../\n</code></pre></p> <p>Now let's pipe that output into <code>sed</code> (Stream EDitor) and clear up the slashes first.  <pre><code># du -d 1 ../../../ | sed 's|[/]||g'\n\n552     ......Academic\n968     ......Notes\n5672    .......obsidian\n232     ......Personal\n72      ......Military\n12      ......Daily\n188     ......Professional\n364     ......todo\n8072    ......\n</code></pre></p> <p>Next let's remove the dots. <pre><code># du -d 1 ../../../ | sed 's|[/]||g' | sed 's/\\.\\{2,\\}//g'\n\n552     Academic\n968     Notes\n5672    obsidian\n232     Personal\n72      Military\n12      Daily\n188     Professional\n364     todo\n8072    \n</code></pre></p> <p>I don't care to include the <code>.obsidian</code> hidden folder, so I will remove it with <code>grep -v</code>.  Also I don't need the total at the bottom so I can remove it wil <code>head -n -1</code>. These commands are also getting long so I will create a script for it. I'll give excecutable permission with <code>chmod</code> and excecute. <pre><code>#!/bin/bash\ndu -d 1 ../../../ | sed 's|[/]||g' | sed 's/\\.\\{2,\\}//g' | grep -v 'obsidian' | head -n -1\n</code></pre></p> <pre><code># chmod +x script.sh\n# bash script.sh\n\n552 Academic\n968 Notes\n232 Personal\n72  Military\n12  Daily\n188 Professional\n364 todo\n</code></pre> <p>Now we can <code>read</code> each line into an associative array. This is just key value pairs, similar to a dictionary in Python. The <code>while read</code> will read the input line by line storing each value in <code>numbers</code> and <code>name</code> respectivily, storing the name as key and numbers as values. The input source for the while loop is generated from <code>&lt; &lt;</code> followed by the commands from earlier.</p> <pre><code>declare -A myArray\nwhile read number name; do\n  myArray[\"$name\"]=$number\ndone &lt; &lt;(du -d 1 ../../../ | sed 's|[/]||g' | sed 's/\\.\\{2,\\}//g' | grep -v 'obsidian' | head -n -1)\n</code></pre> <p>Now remember the chart from before? Let's begin by writing to a file with <code>&gt;</code> and then appending with <code>&gt;&gt;</code>. We then loop through the keys and values of the associative array, appending the folders and their respective size. </p> <pre><code>echo '```mermaid' &gt; folders.md\necho 'pie title Disk Usage by Folder' &gt;&gt; folders.md\n\n# Add pie chart data to Markdown file\nfor key in \"${!myArray[@]}\"; do\n  echo \"    \\\"$key\\\" : ${myArray[$key]}\" &gt;&gt; folders.md\ndone\n\n# Close mermaid code block\necho '```' &gt;&gt; folders.md\n</code></pre> <p>Now we should have the same pie chart from before! </p>"},{"location":"Bash/Scripts%20From%20A%20Real%20Project/#disk-usage-by-file-type","title":"Disk Usage by File type","text":"<p>Now what if we wanted to find data by file types? For example, my vault could be slow while loading, and this could be due to limited space, so I then want to find the culprit. Could it be images, pdfs? A chart like this would help.</p> <p></p> <p>What we want to do is <code>find</code> all files of a specific type and then pipe that into <code>du</code> with the total <code>-c</code> flag. By default, the find command seperates the output with newlines, the du command will only work if they are null terminated (\\0) with <code>-print0</code>. The next odd looking flag, <code>--files0-from=-</code> basically says read a list of null-terminated file paths from standard input. <code>-</code> is stdin, <code>F</code> would reading from a file.</p> <pre><code># find ../../../ -name '*.md' -print0 | du -c --files0-from=-\n\n#[ommitted files for brevity]\n4   ../../../todo/Calendar/Drill/2023-09-09 Muta 6.md\n4   ../../../todo/Calendar/CSC155L/(Every R) CSC155L.md\n4   ../../../todo/Calendar/MUS100/2023-10-22 asdf.md\n4   ../../../todo/Calendar/MUS100/2023-10-03 Music Midterm.md\n4   ../../../todo/Calendar/MUS100/(Every R,T) MUS100.md\n4   ../../../todo/Calendar/CSC155/(Every R,T) CSC155.md\n4   ../../../todo/Calendar/THC Cutoff/2023-07-27 Go clean.md\n4   ../../../todo/st.md\n4   ../../../todo/scratch.md\n1800    total\n</code></pre> <p>We will get a very verbose output. The very last line is all we will need. <code>tail -1</code> will print just the last line and <code>awk '{print $1}'</code> will just print the first field of the line, which is 1800.</p> <pre><code>#find ../../../ -name \"*.md\" -print0 | du -c --files0-from=- | tail -1 | awk '{print $1}'\n\n1800\n</code></pre> <p>So now we have some commands strung together that can find the disk usage that any particular file type is taking up in a given directory. Let's now create a function with a local variable <code>file_type</code> which will be the first parameter. Then <code>disk_usage</code> will use command substitution to wrap the previous commands inside the variable. We can then call the function with the file types as arguments. </p> <p><pre><code>get_size() {\n  local file_type=\"$1\"\n  local disk_usage=$(find ../../../ -iname \"*.$file_type\" -print0 | du -c --   files0-from=- | tail -1 | awk '{print $1}')\n  echo \"    \\\"$file_type\\\" : $disk_usage\"\n  }\n\nget_size png\nget_size md\nget_size pdf\n</code></pre> Output <pre><code>\"png\" : 68\n\"md\" : 1800\n\"pdf\" : 0\n</code></pre></p> <p>Next, to generate the chart, we will create an array with every file extension we want, then loop through it while appending to the <code>.md</code> file which will contain the chart.</p> <pre><code>file_types=(\"webm\" \"gif\" \"pdf\" \"json\" \"md\" \"jpg\" \"png\" \"jpeg\" \"excalidraw\")\n\necho '```mermaid' &gt; filetypes.md\necho 'pie title Disk Usage by Filetype' &gt;&gt; filetypes.md\n\nfor file_type in ${file_types[@]}; do\n  get_size $file_type &gt;&gt; filetypes.md\ndone\n\necho '```' &gt;&gt; filetypes.md\n</code></pre> <p>The chart should look something like this which can be rendered inside Obsidian, and other markdown editors that support Mermaid.</p> <p></p>"},{"location":"Bash/Scripts%20From%20A%20Real%20Project/#full-bash-code-and-rewritten-in-javascript-for-obsidian","title":"Full Bash Code and Rewritten in Javascript for Obsidian","text":"<p>After I wrote all this I realized since it required sudo (elevated priviledge) and only would run on Linux, it would be a poor implementation. Instead writing it in Javascript would make it cross-platform as the code would excecute within the Electron cross-platform instance, whether on MacOS, or Android.</p> <p>Since over a year ago since publishing this plugin, it has 2,400 downloads as of writing this and can be seen on the Obsidian Website here. </p> <p>Here are links to the Bash implementation and the Disk Usage plugin repository (Javascript implementation).</p>"},{"location":"Bash/Shell%20Navigation/","title":"Shell Navigation","text":"<p>These are the default Bash terminal commands based on Emacs commands, however, you can set <code>set -o vi</code> to your .bashrc to set Vi mode</p> <p><code>alt + b</code> backward a word <code>alt + f</code> forward a word OR <code>ctrl +</code> left/right arrow</p> <p><code>ctrl + e</code> end of line <code>ctrl + a</code> beginning of line</p> <p><code>ctrl + w</code> kill/delete word until white space <code>alt + backspace</code> kill/delete word until special character - more useful for file trees, to stop at a slash</p> <p><code>ctrl + k</code> kill/cut ahead of cursor <code>ctrl + u</code> kill/cut before of cursor</p> <p><code>ctrl + y</code> paste </p> <p><code>ctrl + r</code> search old search <code>ctrl + R</code> search new commands</p> <p>More tricks on youtube</p>"},{"location":"History/Testing%20and%20Webscraping/","title":"Testing and Webscraping","text":""},{"location":"History/Testing%20and%20Webscraping/#selenium-2004","title":"Selenium 2004","text":""},{"location":"History/Testing%20and%20Webscraping/#cypress-2015","title":"Cypress 2015","text":"<p>history Cypress is the name of the company as well as the framework itself. It was created due to limitations of Selenium such as it's reliance of the Web Driver protocol. This protocol had developers write tests in a separate language such as Python or Java. Cypress instead integrates directly in the browser and is more compatible with modern SPA frameworks like Vue and React. Development first began in 2015, with the initial and stable release in 2017 and 2022 respectively.</p>"},{"location":"History/Testing%20and%20Webscraping/#puppeteer-2017","title":"Puppeteer 2017","text":"<p>Node.js library for Chrome/Chromium developed at Google.</p>"},{"location":"History/Testing%20and%20Webscraping/#playwright-2020","title":"Playwright 2020","text":"<p>Microsoft hired the same engineers who worked on Google's Puppeteer in order to create Playwright, which unlike Puppeteer, supports other browsers including Firefox and Safari. </p>"},{"location":"History/The%20Web/","title":"The Web","text":""},{"location":"History/The%20Web/#1969-arpanet","title":"1969 ARPANET","text":"<p>The US Department of Defense established the Advanced Research Project Agency Network (ARPANET) in 1969 as a way to share resources across computers. This would be the first spark for the information revolution.</p>"},{"location":"History/The%20Web/#1983-tcpip-protocol","title":"1983 TCP/IP Protocol","text":"<p>Two separate protocols that took many years of development in the 70s, this protocol suite that connects computers over a network was installed in the ARPANET and made standard for all military computers. Some consider this the \"birthday\" of the internet as computers had a common way of communicating with eachother.</p>"},{"location":"History/The%20Web/#1990-cern-and-the-web-era","title":"1990 CERN and The Web Era","text":"<p>British computer scientist, Tim Berners Lee, worked at the European Organization for Nuclear Research (CERN). While there, he created the first modern modern web server and graphical web browser. He called this connected network accessed through a web browser the \"world wide web\". Linking documents together called hypertext and hypermedia, this allowed all human knowledge to be intertwingled and easily accessible.</p>"},{"location":"History/The%20Web/#1994-the-browser-wars","title":"1994 The Browser Wars","text":"<p>The following era is analagous to the Unix Wars of the 80s, where AT&amp;T's SystemV competed with the Berkley Software Distribution (BSD) for setting the standard for Unix. This era was ended with POSIX (Portable Operating System Interface) in 1988 which brought compability across distributions.</p> <p>The next revolution was the web. Companies such as Amazon were created to jump on the e-commerce boom. Americans become curious of the web and started spending more time on the internet with AOL (America Online).</p>"},{"location":"History/The%20Web/#php-1993-95","title":"PHP 1993-95","text":"<p>First started in 1993 as a project for someone's personal home page (PHP), it was later released in 1995 and runs much of the web today.</p>"},{"location":"History/The%20Web/#javascript-1995","title":"Javascript 1995","text":"<p>Some guy at Netscape had to write a scripting language for the Netscape browser with a deadline of 10 days. They wanted this rolled out fast at a time critical period where the internet was booming and Netscape was in fierce competition against Microsoft's internet explorer for market share. Javascript today has evolved into what people now use as a general purpose programming language despite originally meant for small scripting tasks. </p>"},{"location":"History/The%20Web/#angular-2010","title":"Angular 2010","text":"<p>First major Single Page Application (SPA) framework developed at Google. This marked the beginning of the heavy client side Javascript approach to building web applications. Instead of sending and recieving HTML from server to client in a traditional hypermedia fasion, JSON is used.</p>"},{"location":"History/The%20Web/#react-2013","title":"React 2013","text":"<p>First developed at Facebook, React is what many consider the industry standard today for progressive web apps/SPAs.</p>"},{"location":"History/The%20Web/#vue-2014","title":"Vue 2014","text":"<p>Created by a former Google developer who used Angular, Vue was meant to keep the essential parts of Angular but create something very simple and lightweight instead. Today React and Next.JS are on one side and Vue and Nuxt.js on another side of the ecosystem. Both similar tools for building modern web apps but with different design philosophy.</p>"},{"location":"History/The%20Web/#svelte-2016","title":"Svelte 2016","text":"<p>Instead of monolithic Javascript frameworks (i.e SPAs), Svelte compiles HTML template to specialized code in advance which then manipulate the DOM directly. Despite a promising design and positive developer experience, Svelte has not seen the kind of market share or popularity of other frameworks like Vue, which despite being around the same age, are very different in terms of popularity. </p>"},{"location":"History/The%20Web/#htmx-2020","title":"HTMX 2020","text":"<p>Note: HTMX is really a rewritten Intercooler.js which dates back to 2013.</p> <p>HTML E(X)tended is a very simple Javascript library that allows your to swap different parts of the page intead of a dramatic page reload. Secondly any element can submit HTTP requests, not just link and form tags. Instead of JSON being sent back and forth from the client and server such as in SPAs, HTMX uses historically accurate Rest API's, i.e client sends HTTP request, server returns HTML.  </p>"},{"location":"History/Timeline/","title":"Timeline","text":"<p>In this timeline, I plan to analyze the history software and computers. Starting with the earliest mainframes, such as the ENIAC in 1945, designed to calculate artillery fire in WW2, and transitioning to UNIX development in the late 60s and early 70s. Next diving into GNU/Linux and the development of free and open source software in the 80s and 90s, all the way to newer technology, such as modern web development and javascript frameworks.</p> <p>Note: I will be placing all timeline related notes here for now not just what's mentioned above.</p>"},{"location":"History/Timeline/#alan-turing-1937","title":"Alan Turing 1937","text":"<p>Dealing with the philosophical problem of defining \"computing\" itself, Alan Turing came up with the theoretical machine that could solve any calculation, given enough time. In the 30's, computers were either analog/mechanical or built using tapes such as Turing's device below. This idea of a universal computational device is key to understanding Computer Science as a field of study and what distinguishes it from all others.</p> <p>A mechanical engineer does not study the engine, but rather physics. A chemical engineer does not study the factory, but chemistry itself. So then what does a computer scientist study? You might guess physics or electrical engineering, but they study the computer itself, that is because a computer, or rather, a universal computational device, is fundamentally different than an engine or a factory. A computer large enough could run the engine and the factory within a program a thousand times over, and any possible algorithm imaginable. Because we stand on the shoulders of giants, we understand computers as Turing complete which distinguishes Computer Science from all other fields.</p> <p></p>"},{"location":"History/Timeline/#the-von-neumann-model","title":"The Von Neumann Model","text":"<p>In progress</p> <p></p>"},{"location":"History/Timeline/#eniac-1946","title":"ENIAC 1946","text":"<p>The first digital computer was the [[eniac.webp|ENIAC]], which stood for Electronic Numerical Integrator and Computer. This project was originally intended for solving differential equations and other other academic pursuits by physicists and mathematicians, but the real funding and attention began when the US military saw it's potential for calculating artillery fire. </p> <p>Despite this, it was finished only after WW2 was ended. The ENIAC was completed in 1946, and never saw much fruition in wartime. Intended for death and destruction, it was instead used for weather prediction, cosmic-ray studies and other scientific pursuits. The ENIAC essentially became a foundational tool in the evolution of computer science.</p>"},{"location":"History/Timeline/#univac","title":"UNIVAC","text":"<p>The successor the ENIAC, the UNIVAC (Universal Automatic Computer) was the first business application computer.</p>"},{"location":"History/Timeline/#fortran-1954","title":"Fortran 1954","text":"<p>Arguably the first \"high level\" programming language, FORTRAN (FORmula TRANslator) was developed by IBM for science and engineering.  </p>"},{"location":"History/Timeline/#cobol-1959","title":"COBOL 1959","text":"<p>COBOL (Common Business-Oriented Language) was developed by American Computer Scientist Grace Murray Hopper and served as a solution to businesses to use a single language for facilitating transactions.</p>"},{"location":"History/Timeline/#tradic-1955","title":"TRADIC 1955","text":"<p>TRADIC stood for TRAnsistor DIgital Computer. It was the first of it's kind to be build using only transistors and diodes instead of vacuum tubes. It was built by Bell Labs for the US Military.</p>"},{"location":"History/Timeline/#multics-1964","title":"MULTICS 1964","text":"<p>Multics (Multiplexed Information and Computing Service) was a highly ambitious project funded by MIT, General Electric, and AT&amp;T's research group, Bell Labs. Ken Thompson and Dennis Ritchie were key figures and are later discussed for their major contributions to the world of software. </p> <p>In the early 60's, time sharing (multiple users sharing the same resources) and file systems were a novelty. However, the project was over-budget, overdue and over engineered.</p> <p>The developers were frustrated with it's complexity, Ken Thompson was unable to play his Space Travel game as it was costly and inefficient on the Multics. So he essentially started from scratch on a PDP-7, writing what would eventually become Unix.</p> <p>Ken Thompson (sitting) and Dennis Ritchie (standing) on a PDP-11</p> <p></p>"},{"location":"History/Timeline/#unix-1969","title":"UNIX 1969","text":"<p>Unix was born out of a failure of Multics. While Ken Thompson was rewriting everything from scratch on his PDP-7, it essentially became a lighter weight, stripped away version of Multics. When presenting it to his colleagues, they joked it was a \"Eunuch Multics\" as if it had been castrated, thus the name Unix was born.</p> <p>There is no doubt the frustration of complexity with Multics and the practice of simplifying and stripping away was key to the  Unix Philosophy, a design/programming style which emphasizes simplicity, modularity and to \"do one thing and do it well\".</p>"},{"location":"History/Timeline/#relational-database-1970","title":"Relational Database 1970","text":"<p>In the early 1960's, hierarchical and network databases were based off graphs and trees. E.F Codd, while working at IBM, proposed a new relational model made of tables (relations). This approach made it much easier to manipulate data, by modifying columns and rows.</p> <p>His work paved the way for SQL (Structured Query Language) and modern relational databases like MySQL, PostgreSQL, and SQL Server, which are descendants of these early efforts and to utilize SQL for data manipulation and querying.</p> <p>This article goes more in depth.</p> <p>One of the first companies to monetize relational databases was Relational Software Inc. (1979\u201382), later renamed to Oracle in 1982. In 1987, Oracle was named the largest database management company. In recent years, Oracle has partnered with Microsoft in the cloud space to challenge AWS. Here is a complete timeline.</p>"},{"location":"History/Timeline/#c-1972","title":"C 1972","text":"<p>Before 1972, Unix was written entirely in assembly. The use of assembly made Unix hard to port across different hardware. Dennis Ritchie, who worked with Ken Thompson at Bell Labs, developed C in the early 1970s. The development of C was revolutionary because it combined the efficiency of assembly language with the ease of a human readable, high level language.</p> <p>Once C was created, Ken Thompson and others rewrote Unix in C, making it one of the first operating systems to be written in a high-level language. This made Unix highly portable, a characteristic that contributed to its widespread adoption. The co-development of Unix and C at Bell Labs is a monumental moment in computer science, influencing operating systems and programming languages even today.</p>"},{"location":"History/Timeline/#gnu-1983","title":"GNU 1983","text":"<p>AT&amp;T was essentially a regulated monopoly in the 60s and 70s. They were restricted only to telecommunications at the time and were not allowed to sell computers. Their research institute, Bell Labs had just created Unix, so they sold licenses to schools and used it as an educational tool instead. The source code was accessible to students across universities with restrictions. One of these student's was Richard Stallman, who worked at MIT's Artificial Intelligence Laboratory.</p> <p>In the 1980's, almost all software was closed source and proprietary. Richard Stallman created the GNU (GNU Not Unix) Project in 1983 to develop a free and open source software replacement for Unix. This included replacing every program.</p> <p>Common GNU programs include <code>ls</code>, <code>cp</code>, <code>cat</code> and many others, If these look like \"Linux\" commands, they are not. What we know as Linux today is in fact mostly made of GNU programs. Linux is the kernel. Combined, they are GNU/Linux.</p>"},{"location":"History/Timeline/#linux-1991","title":"Linux 1991","text":"<p>The GNU operating system was nearly complete, but missing a key part, the kernel - the central component that manages resources and acts as an intermediary between software and hardware. GNU was working on a kernel called Hurd, but it was taking longer than expected to become functional. </p> <p>Around this time, a Finnish Computer Science student, Linus Torvalds, started working on a hobby operating system, outlined in his famous . After 3 years of work, Version 1.0 was released in 1994 and was clear it would serve as the missing piece to GNU.</p> <p></p>"},{"location":"History/Timeline/#debian-1993","title":"Debian 1993","text":"<p>American software engineer, Ian Murdock, created Debian, the first Linux distribution along with the Debian Project, which aimed to create a free and open source operating system based on GNU/Linux. The name \"Debian\" is a portmanteau of Ian and his wife's name Debora. </p> <p>Today, most Linux Distros such as Ubuntu and Linux Mint are Debian based, and the majority of web servers today are Debian or Debian based.  Common Debian utilities include <code>apt</code> (advanced packaging tool) and <code>dpkg</code> (debian package).  </p>"},{"location":"History/Timeline/#red-hat-1993","title":"Red Hat 1993","text":"<p>Red Hat was founded on two main pillars, develop a Linux distribution and provide related support services. Red Hat Enterprise Linux (REHL) served as it's flagship product along with subscription-based services that included technical support, software updates, and security patches.</p> <p>Targeted at businesses that needed a reliable OS for enterprise applications and infrastructure, they quickly became a success. in 2012, they became the first Open Source Company to exceed $1 Billion in revenue. After growing for a number of years, they were later acquired by IBM for $34 Billion in 2019.</p> <p>Although their business model still revolves around open-source software and subscriptions for support and services, their scope has expanded significantly, now providing cloud services and tools like Ansible for automating IT infrastructure. </p> <p>Despite claiming a commitment to open source, Red Hat is now under criticism for pay-walling the source code to RHEL. </p>"},{"location":"History/Timeline/#rest-web-standardization-2000","title":"REST &amp; Web Standardization 2000","text":"<p>In 2000, Roy Fielding introduced the concept of REST (Representational State Transfer) in his doctoral dissertation as a framework for web standardization. REST laid the groundwork for building scalable, stateless web services and has become a foundational in modern web development and API design.</p>"},{"location":"History/Timeline/#namespaces-2002","title":"Namespaces 2002","text":"<p>Linux kernel feature which allows a set of processes to only see a certain number of resources. This is a foundational technology which underlies container runtimes like Docker.</p>"},{"location":"History/Timeline/#aws-2002","title":"AWS 2002","text":""},{"location":"History/Timeline/#canonicol-ubuntu-2004","title":"Canonicol &amp; Ubuntu 2004","text":"<p>Canonical Ltd. was founded by Mark Shuttleworth, a multi-millionaire and vocal advocate of open source software. Ubuntu's first version, 4.10 (named after the year and month) was released in October of 2004 as a free and open source, Debian based distribution. The business model is similar to Red Hat: which aims to monetize open source software by providing services and support.</p>"},{"location":"History/Timeline/#amazon-s3-and-ec2-2006","title":"Amazon S3 and EC2 2006","text":"<p>Amazon's Elastic Cloud Compute (EC2) popularized the use of the term cloud by which offered customers a cluster of virtual machines communicating over the web via Rest API to relieve the burden of managing operating systems and hardware on-premise. </p>"},{"location":"History/Timeline/#cgroups-2007","title":"Cgroups 2007","text":"<p>The foundational (C)ontainer group technology is developed at Google. This limits CPU and other resource usage to running tasks which underlie all container technology today. </p>"},{"location":"History/Timeline/#openshift-2011","title":"Openshift 2011","text":"<p>Red Hat released Openshift as a cloud platform and the first container major container orchestrater. Though it is important to note key technology was developed at Google such as LXC and Cgroups in 2007-2008 which is used by nearly all container runtimes.</p>"},{"location":"History/Timeline/#docker-2013","title":"Docker 2013","text":"<p>Frustrated with managing different languages, libraries, and runtimes Solomon Hykes developed his own solution using cgroups to isolate resource usage. This eventually lead to what we know know as Docker. As the company Docker Inc. (previously Dotcloud) grew they wanted to differentiate their commercial product with an upstream open source project. They renamed Docker on Github to Moby in 2017.  </p>"},{"location":"History/Timeline/#kubernetes-2015","title":"Kubernetes 2015","text":"<p>With the emerging AWS as the giant in tech, Google realized the Cloud was inevitable. Inspired by Docker, and originally using Docker for a runtime (now CRI-O), the now most popular container orchestrater was built. Kubernetes emerged as the victor of the Container Orchestration War.</p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/","title":"Inkscape","text":"<p>\ud83d\udcdd\ufe0f Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also find the video here.</p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#inkscape","title":"Inkscape","text":"<p>I've used many image editing software and there's one in particular that stands out the most. Inkscape has grown on me more than anything else because it's easy to use, yet also has a large number of features where you're always learning something new and constantly improving your workflow. I have used Gimp in the past, but eventually gave up with how difficult it was to learn, not to mention how dated the interface felt. Inkscape seems to have more frequent updates and support and overall better outlook for the future. People often compare Inkscape to Adobe illustrator as they are both SVG based image editors. I have no comment on this as I am proud to say I have never paid for an Adobe product and do not support the company, so I unfortunately cannot give a good comparison.</p> <p>Here is my brain anatomy map in Inkscape I plan on making interactable in a webpage by clicking a region and a side bar appears with information. Also very useful for taking notes for Neurobiology.</p> <p></p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#photopea","title":"Photopea","text":"<p>The next image editor worth mentioning is Photopea, this is a free web based editor I have used in the past to make thumbnails. It is very easy to use and impressive as it was built entirely by one person. The difference between Photopea and Inkscape of course is one is a normal image editor and the other deals with SVG graphics. These are 2d vector graphics based on geometry rather than a fixed grid of pixels. What I like most about SVGs is that they are just a file with XML tags that I have more control over. When I draw a path or add a shape, a path or rect tag is added which I can inspect and have granular control in the XML editor.</p> <p>Though Photopea does not have the XML editing features, it is very handy for creating thumbnails or offensive memes.</p> <p></p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#svgs-for-web-design","title":"SVGs for Web Design","text":"<p>My favorite part of SVGs, and probably their biggest use case, is embedding in websites. Though there are other ways to make diagrams and cool visual graphics like the HTML canvas element, this requires more Javascript than I prefer. Instead, having the entire graphic be composed of elements in XML to be represented like a document makes more sense to me. Using Inkscape's XML editor, I've added these HTMX attributes to swap the side panel of the selected brain region with the associated HTML file. This is a pretty cool way of building interactive diagrams, maps, timelines, really anything you want. If you can draw it in Inkscape and write some basic Javascript and CSS to add functionality, you can build really anything you want.</p> <p></p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#history-of-inkscape","title":"History of Inkscape","text":"<p>When considering what kind of software to invest your time in learning, weather it be a framework for building websites, a programming language, or a graphics/image editor, one of the things to consider is age. Basically, the longer that something has been around, the longer it is likely to stick around in the future. Inkscape dates back to 2003 as a fork of Sodipodi (see below), a vector graphics tool intended for artists. This makes Inkscape over 20 years old, all while still maintaining popularity.</p> <p></p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#future-of-inkscape","title":"Future of Inkscape","text":"<p>Besides its historic track record, looking towards the future, Inkscape has a positive outlook. I say this because it's active group of developers just recently switched to GTK4 for their development version, or branch. GTK stands for Gnome Tool Kit, the fourth version being the most modern UI framework for Linux desktop apps. This is a major architectural improvement which enables graphics acceleration, better overall performance, and a more modern user interface. This will make Inkscape one of the first relatively high-profile desktop-agnostic apps to use GTK4 in the near future. Though we may not see this until after 1.4 as some issues were found on Windows and MacOS, as you can see in this \"toot\" on Mastodon from inkscape themselves. But, these occurrences are expected and sometimes more time is needed to ensure a stable release. Regardless, I'm impressed by the work so far optimistic for the future.</p> <p></p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#additional","title":"Additional","text":"<p>https://www.reddit.com/r/linux/comments/1bodw7i/inkscapes_development_version_switches_to_gtk4/</p> <p>https://inkscape.org/news/2023/04/17/inkscape-hiring-accelerating-gtk4-migration/</p> <p>https://www.phoronix.com/news/Inkscape-Switches-To-GTK4</p> <p>https://mastodon.art/@inkscape/112151266538190571</p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/SVG%20Timeline/","title":"SVG Timeline","text":"<p>I am very interested in history of programming languages, Linux distros, and computing in general. I want some way to visually graph all my notes seen in my timeline. At first I read through the SVG docs on Mozilla, and soon realized hand coding SVGs is very tedious and inefficient. I later found Inkscape, a click-and-drag open source SVG editor. Here is what I have so far and progress can be seen on this repo.  </p> <p>So far there is a main timeline of the major events, such as early computing, languages, operating systems, and recent container technology. We will have separate timelines such as modern web quality assurance (QA)/testing frameworks. There will be a internet/web history either starting with ARPANET in 1969 or Tim Berners Lee and the world wide web of the 90s.</p> <p></p> <p>The plan is to click on a single event and a side panel will open to show an article. Similar functionality to this website. There are multiple ways to do this, one, you could store each article as an HTML file on a server and every click of the SVG element would trigger an HTTP GET request and return that article, injecting/swapping whatever was in the side panel previously. Since by default only link and form tags can submit HTTP requests, you would use HTMX to give that ability to SVG rect elements. Also HTMX would gives hx-swap attribute to rects to swap the current content of the side panel with that which was just clicked. </p> <p>This hypermedia, client-to-server approach would be most scalable. However, until the project grows and it is necessary, I will be pushing a single SVG file to the limit by storing the articles as the rect's description attribute added with inkscape. A javascript event listener will be placed on every rect that will swap what is in the side panel all within the browser. As the single SVG grows, the loading speed may be effected and we can switch to a client-server architecture in the future.</p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/SVG%20Timeline/#svg-libraries","title":"SVG Libraries","text":"<p>Using Inkscape makes creating SVGs much easier. What might be even easier is using an SVG framework like Rapheal or Snap Repo. I am reading that Snap Repo is a newer version targeting modern browsers and was written by the same author as Rapheal. Here is a star comparison of the two repos. Also an example of what is possible with these frameworks. It is a map. We want to build a timeline, but it doesn't matter, they are clickable, intractable SVG elements with tooltips.  </p> <p>SVG libraries include Rapheal and Snap.svg. Snap was built by the same author of SVG but meant for modern web browsers. These are possibile frameworks though the current Inkscape approach works fine.</p> <p>Example of rapheal https://www.energy.gov/national-laboratories</p> <p>Snap repo https://github.com/adobe-webplatform/Snap.svg</p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/SVG%20Timeline/#personal-updates","title":"Personal Updates","text":"<p>UPDATE March 28th 2024: Ok looking back on this project, my Youtube channel, and website, I'm considering starting back up. I just got burned out cause I'm taking 18 credits in college plus an internship.</p> <p>UPDATE Oct 19th 2024: No progress since last update. I don't know if this will go anywhere unless I start making youtube videos about history of technology Asionometry style in which case I would make articles and videos with years associated with them, making a SVG timeline to embed into my website more useful and relavent.</p>"},{"location":"Linux/Encrypting%20Hard%20Drives/","title":"Encrypting Hard Drives","text":"<p><code>cryptsetup opep, lose, or</code></p> <p>unencrypted version of drive opened in <code>/dev/mapper</code></p> <p>Device mapper, the underlying kernel-space program that underlies <code>cryptsetup</code>, also is widely used by other software such as LVM, previously Docker.</p> <p>%% Docker calls device mapper a storage mapper? %%</p> <p>The Linux kernel may seem complex, but more than 60% of it's entire source code does one thing, provide software interface to hardware device, these are called device drivers. The userspace program responsible for managing these devices is <code>udev</code>. Have a look at it with <code>systemctl status systemd-udevd</code>.  This is useful for scripts to add additional security or convenience in our situation, such as automatically mounting and prompting for the passphrase when you plug in your encrypted drive, creating backups, or encrypting the drive after a certain period of inactivity while mounted. These scripts can called to run from <code>/etc/udev/rules.d/</code>.</p> <p>If the FBI, NSA, or your angry roommate have the means to unencrypt your drive, you can completely destroy all contents by overwriting with <code>/dev/zero</code> or <code>/dev/urandom</code>, preferably both. In addition, there are tools like <code>``</code>blkdiscard<code>and</code>shred`. Aside from these methods, the surest and simplest way to ensure contents of a drive are not recovered is physically destroying it, such as smashing with a sledgehammer or incinerating it.</p>"},{"location":"Linux/Encrypting%20Hard%20Drives/#tutorial","title":"Tutorial","text":"<p>Okay enough talk let's actually learn how to encrypt a drive. We will first practice on a loop device instead of a real drive until we are confortable. A loop device is basically a virtual file pretending to be a block device to the kernel, meaning we can create any file we want, and as long as we associate a loop device to it, we can partition it and create a filesystem as if it were a real physical hard drive or USB.</p> <p>To begin, create a file large enough to be detected. <pre><code>dd if=/dev/zero of=/tmp/test.img bs=1M count=5120\n</code></pre></p> <p>Now associate that file with a with a loop device. %% /dev/loop0 shoudl't have to exist, confirm tho %% <pre><code>sudo losetup /dev/loop0 /tmp/test.img \n</code></pre></p> <p>Now you can list it by running <code>losetup</code> with no arguments or specifying the file. It also appear when you list block devices with <code>lsblk</code>. <pre><code>\n</code></pre></p>"},{"location":"Linux/How%20Linux%20Boots/","title":"How Linux Boots","text":"<p>\ud83d\udcdd\ufe0f Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also find the video here.</p> <p>Understanding the Linux boot process is a complex task, but knowing a few major steps and concepts is possible for anyone. If you inspect your file system at the root level, you'll see you have a boot directory, upon inspecting your block devices, you'll find that it may be a separate partition altogether. Looking at the contents, you'll see each type of file have different versions according to each kernel release. Currently I'm on 6.8.9, my Fedora machine stores two previous versions in case I ever need to roll back a version due to a system breaking kernel update.</p> <p>After the firmware has check and initialized hardware components with a Power on Self Check, or POST test, it looks for a boot device on a hard drive or SSD where the bootloader, GRUB is stored, which the firmware hands over control to. The next step involves Vmlinuz, which stands for Virtual Memory LINUx gZip, meaning it's a gzip-compressed kernel image. </p> <p>If you run <code>journalctl</code> with the -boot flag, you can see uncompressing this image into memory is the first step. The next step is initramfs, or initial ram filesystem. This loads a temporary filesystem into memory to load just enough kernel modules to recognize the full kernel in order to mount the real filesystem, begin systemd as the init service, and continue into userspace.</p> <p>These steps can be viewed in a summarized way with <code>systemd-analyze</code> starting with exactly how long the firmware took to initialize the hardware, around 10 seconds, 2 and a half seconds for grub, the bootloader, 1 second for uncompressing the kernel, 5 for loading the temporary filesystem, and 12 seconds for systemd to start up every process necessary for userspace. </p> <p>An even better way to graphically visualize each step is the run <code>systemd-analyze plot</code> and direct the output towards a created SVG file. Here I open it up with Inkscape and have a comprehensive chart of every process. It looks complicated but as I mentioned, it just starts with firmware, the bootloader, uncompressing the kernel, our temporary filesystem which loads all these necessary modules, and next our very first process, or PID 1, SystemD, which spawns all other processes.</p> <p>This can be visualized with process tree, where you can see every ongoing process on your machine and it's parents, all eventually spawning from systemd. Also as seen in the pseudo or virtual filesystem proc, which displays each ongoing process as a directory in real time, here we can peak at what's inside.</p> <p>Keep in mind this video is not comprehensive and I'm leaving out many details of about early hardware stages like BIOS and EUFI, instead I hope to show some useful terminal commands and tips to explore yourself. Probably the most useful thing you can take from this video is the SVG chart created from <code>systemd-analyze</code>, which you should spent some time with exploring yourself.</p> <p>If you like these short videos where I explain a Linux subject leave a comment, like, or subscribe to support more content like it. Thanks for watching.</p>"},{"location":"Linux/Linux%2B%20vs%20LPIC/","title":"Linux+ vs LPIC","text":"<p>The first Linux certification to discuss is Linux+ offered by CompIA (Computing Technology Industry Associate), a more general IT certification association that offers everything from help desk support do data analyst certifications. The second certification worth comparing is the LPIC-1, offered by the LPI (Linux Professional Institute), an organization which focuses on Linux in and of itself. Although after passing the LPIC-1, there are additional certifications that go far more in depth, being the LPIC-2 and LPIC-3, the only certification that matches the depth of knowledge as Linux+ is LPIC-1.</p> <p>First off, let's compare what is similar. Both certifications are vender and distro nuetral, meaning they are not specifically about one Linux distribution offered by one company, such as Redhat offering certifications covering their REHL distibution (RHCSA or RHCE). Instead, both the Linux+ and LPIC-1 focus on concepts that apply across all distros, and include tools used by the most common distros, such as RPM (Red Hat Package Manager) as well as APT (Advanced Package Manager), used by Debian-based systems.</p> <p>While reading over the exam objectives for each certification, I noticed the LPIC-1 putting much more weight on basic file management, the shell environment and Bash scripting, while the Linux+, as seen below, puts less emphasis on Bash scripting. What the Linux+ has that the LPIC-1 doesn't however is Git version control, containerization (Docker/Podman) and orchestration (Kubernetes). These are more recent technology trends only found in the Linux+ and often used in an enterprise environment. So we could say the LPIC-1 is a more pure Linux certification and Linux+ focuses on Linux, in addition to the related tools and technology used in enterprise environments.  </p> <p>From what I have read, CompTIA is more widely recognized as a industry standard in the United States, while the LPI is more recognized in Europe. </p> <p>After passing the Linux+ or LPIC-1, the next certifications would either be the RHCSA (Red Hat Certified Systems Administrator) or the LPIC-2.</p> <p>For price, the two LPIC-1 exams each cost $200 in the United States as of writing this. The single Linux+ exam costs $358, although they have a tempting bundle for $462 which included a exam retake and official study guide.</p> <p>The old Linux+ XK0-004 exam is currently expired. The newer XK0-005 version has condensed 5 domains into only 4, and has included Docker/container management and orchestration.</p> XK0-005 System Management 32% Security 21% Scripting, containers, and automation 19% Troubleshooting 28% <p>Key Takeaways</p> <ul> <li>Both are vendor and distro neutral.</li> <li>CompTIA certifications will be far more recognized than LPI. </li> <li>LPI is a specialized Linux organization that has additional certs (LIPC-2 and 3), while CompTIA is a general IT certification company with only one Linux+ cert. </li> <li>Linux+ focuses on more enterprise technology like containers and additional tools like version control (Git) while LPIC-1 is purely Linux focused.</li> <li>Linux+ is a single exam while LPIC-1 is broken into two.</li> <li>CompTIA has a student discount for those in a four year university.</li> </ul> <p>Two Latest Exams Released</p> <p>2018 Oct LIPC-1 version 5</p> <p>2022 July Linux+ XK0-005</p> <p>Exam Objectives</p> <p>Linux+</p> <p>LPIC-1 Exam 101 &amp; 102</p>"},{"location":"Linux/Systemd/","title":"Systemd","text":"<p>Just some notes of mine so far.</p> <p>Here is a blog post titled Rethinking PID 1 in 2010 of experiments with a new init system.</p> <p>Unix System V is as old as 1983.</p> <p>Systemd is a relatively new system manager introduced in 2010 as a replacement for SysV init manager. Despite criticism of feature creep and bloat, Systemd has gained massive popularity in the last decade and is here to stay. Most distros such as Ubuntu and Red Hat ship it by default. </p> <p>Here are various changes introduced in Systemd to be aware of. Though these new features do not necessarily make the traditional tools obsolete, adoption may be the best long term solution.</p>"},{"location":"Linux/Systemd/#cron-jobs-systemd-timers","title":"Cron Jobs &gt; Systemd Timers","text":""},{"location":"Linux/Systemd/#etcfstab-unit-files","title":"/etc/fstab &gt; Unit Files","text":""},{"location":"Linux/Systemd/#run-levels-targets","title":"Run Levels &gt; Targets","text":"<p>Instead of traditional Sysv runlevels, such as switching to Single-User Mode with <code>telinit 1</code>, you would now use <code>systemctl isolate rescue.target</code>.</p> <p>Check out this Sysv to Systemd cheatsheet.</p> Run Level Description Systemd Target 0 Poweroff poweroff.target 1 Single user mode (recovery mode) rescue.target 2 Multi user mode with no networking multi-user.target 3 Multi user mode with networking multi-user.target 4 User Definable multi-user.target 5 \u00a0Multi user mode under GUI (standard in most systemds) graphical.target 6 Reboot reboot.target"},{"location":"Linux/Systemd/#community-feedback-and-opinion","title":"Community Feedback and Opinion","text":"<p>Notes of The Linux Experiment's video:</p> <ul> <li>All Linux based systems use an init system, the first process that starts after you boot your OS and continues to run essential programs and services.</li> <li>Systemd was spearheaded by Red Hat to replace existing Sysv (1983) and Ubuntu's now discontinued Upstart (2006-2015).</li> <li>Critics claim Systemd violates the Unix Philosophy of modularity, separate programs that communicate with each other. Instead, Systemd has become a monolithic, umbrella project that has more functions than traditional init systems were meant for.</li> <li>There is really not much competition to Systemd.</li> </ul>"},{"location":"Linux/Systemd/#unit-timers","title":"Unit Timers","text":"<p>Cron to Systemd Time Format Tutorial</p> <pre><code>\u276f cat foo.timer \n[Unit]\nDescription=Append to file in /tmp every minute\n\n[Timer]\nUnit=foo.service\nOnCalendar=*:*:0\n\n[Install]\nWantedBy=timers.target\n</code></pre> <pre><code>\u276f cat foo.service \n[Unit]\nDescription=Append text to /tmp file\n\n[Service]\nExecStart=/usr/local/bin/systemd-timer.sh\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <pre><code>\u276f cat /usr/local/bin/systemd-timer.sh \n#!/bin/bash\necho \"Systemd at `date`\" &gt;&gt; /tmp/from-systemd.txt\n</code></pre> <pre><code>\u276f cat /tmp/from-systemd.txt \nSystemd at Mon Dec 25 10:40:01 AM CST 2023\nSystemd at Mon Dec 25 10:41:01 AM CST 2023\nSystemd at Mon Dec 25 10:42:01 AM CST 2023\n...\n</code></pre> <p>For more information:<code>man systemd.unit</code>. WantedBy and Required by just create symlinks in .wants/ and .requires/ directories when <code>systemctl enable</code> is called. Wants and Requires are for actual dependencies.</p>"},{"location":"Linux/The%20Rise%20of%20Linux/","title":"The Rise of Linux","text":"<p>\ud83d\udcdd\ufe0f Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also find the video here.</p> <p>Covering the rise in market share of Linux may seem simple, but it's gets more complicated as you break it down. Since Android is based on the Linux kernel, and global operating system market share has Android at 43%, the number one place, wouldn't that mean Linux already won and we can end the discussion here? Well, when people say Linux, they usually mean common distributions like Ubuntu, Fedora, Arch and so on,  so we'll stick first to the desktop market share rather than mobile, then later dive into Server market share. </p> <p>But before discussing number of Linux users, it's worth mentioning something that could skew these statistics, that is the privacy measures many Linux users take to ensure their personal data is not collected, this very data that SEO and analytics companies use to provide these reports. Ad blockers such as U-block origin will block or filter out tracking scripts that websites use to gather data about users operating systems and other details. Other than custom scripts, this data can be found in your user agent, something that is included in the header of every HTTP Get request. Which, if you're curious, can be found by typing in <code>window.navigator.userAgent</code> in the dev tools, and that is your personal identifier which much of this data is collected from. </p>"},{"location":"Linux/The%20Rise%20of%20Linux/#liberal-estimate","title":"Liberal Estimate","text":"<p>Back to the market share, Stat Counter which is the most liberal estimate, shows Linux desktop worldwide reached just under 4% this year up from 3% in 2023. The increase is more pronounced in the United States at 3.7% up from 2% last year, nearly doubling. Those in India use Linux the most, at 14%, which brings the global average up significantly. </p>"},{"location":"Linux/The%20Rise%20of%20Linux/#india","title":"India","text":"<p>This is due to several reasons, one, since India is a developing country, not everyone can afford a computer with a i9 processor and 30 gigs of ram. Sometimes all they have is an old laptop where installing a Linux distro brings it back to life. Schools and governments adopt Linux to cut costs, as Windows can be expensive due to their licenses, not to mention increased risk of malware and cyber security threats. As a response, the Indian Defense Research and Development Organization  forked their own distribution of Ubuntu called Maya OS. So Linux in India is less expensive, more secure, and widely adopted. This also could be due to many in India choosing fields like engineering, including computer science.  </p>"},{"location":"Linux/The%20Rise%20of%20Linux/#use-friendly","title":"Use Friendly","text":"<p>Another reason for the adoption of Linux worldwide is it's user friendliness compared to 20 year ago. In the late 90s and early 2000s, installing Linux meant purchasing a floppy drive, partitioning hard drive space, and manually configuring and installing X Windows programs from the terminal. Nowadays its much more approachable with a streamlined installation process. In fact, during my last few installs of Fedora, I don't remember even needing to open the terminal once.</p>"},{"location":"Linux/The%20Rise%20of%20Linux/#microsoft","title":"Microsoft","text":"<p>Besides India driving the global market share and Linux being more approachable and user friendly nowadays, the third possible reason is Microsoft just not caring about Windows as much as the 90s and 2000s. Currently, Windows represents only 12% of total revenue, far surpassed by Azure cloud products and services as well as Office subscriptions. </p>"},{"location":"Linux/The%20Rise%20of%20Linux/#conservative-estimate","title":"Conservative Estimate","text":"<p>Anyways, aside from the 4% figure from Stat Counter, we have a more conservative estimate from Steam hovering just below 2%. Now of course these are Steam users where gaming isn't as supported on Linux, as opposed to Windows. Either way, this number doubled since 2020 where where market share was reported at just 0.9%. So whether you are looking at Stat Counter, or Steam's report, Linux desktop usage has been increasing considerably over the past several years.</p>"},{"location":"Linux/The%20Rise%20of%20Linux/#additional","title":"Additional","text":"<p>W3 Techs report shows Unix (which includes Linux) had an increase of operating system for websites from 65% to 85% from 2013 to 2024 while Windows server declined from 35% to 16%.</p> <p>5 Reasons Why Desktop Linux Is Finally Growing In Popularity: zdnet article</p> <p>India</p> <p>Good News! Indian State Aims to Save Over $400 Million by Choosing Linux Why is India\u2019s Defence Ministry ditching Microsoft Windows for Ubuntu-based Maya OS?</p> <p>User Friendliness of Linux in 2004</p> <p>very little has been done to increase its user-friendliness and extend its reach outside highly technical and knowledgeable individuals and those in academic computing environments. Without a reasonable level of user-friendliness, Linux cannot attract a critical mass of users required for its success.</p> <p>PG 19 of This Research Paper https://www.researchgate.net/publication/3248102_Economics_of_Linux_Adoption_in_Developing_Countries</p>"},{"location":"Linux/Top/","title":"Top","text":""},{"location":"Linux/Top/#overview","title":"Overview","text":"<p>Let's say your new to Linux, transitioning from the Windows Task Manager to Ubuntu's System Moniter. This is a relatively easy transition, however, further down your Linux journey you will delve further into the terminal and find alternatives to GUI applications. This alternative is a program called <code>top</code>. Written in 1984, it is tried and tested against time, and will continue to be the default resource monitor in the near future. Despite other programs, such as <code>htop</code>, <code>vtop</code>, <code>powertop</code>, etc., doing essentially the same thing, I would recommend not to over analyze and simply learn the default. <code>top</code> works perfectly fine as an alternative to the System Monitor for an everyday Linux user and can even help down the line with Linux Administration if that's what you are interested in.</p>"},{"location":"Linux/Top/#statistics-and-process-table","title":"Statistics and Process Table","text":"<p>After running the <code>top</code> command, you will get something like this. In two main parts, the statistics are at the top, and the process table at the bottom. At the very top, starting from the left, you can see the up time, which is how long the system has been running. Moving right, the number of users is displayed, then we have load average, which is average CPU usage in the past 1 minute, 5 minutes, and 15 minutes. It is  based on your number of cores. For example if you have 4 cores, then a 4.0 would mean 100% CPU usage, in this screenshot, I have 2 cores so in the past 1 minute I had 63% usage of my 2 cores.</p> <p></p> <p>Below the number of tasks running, you will see the percent of CPU utilization from user processes, the kernel, and processes with a positive niceness value (often manually configured). Next the <code>id</code> is the percent of time idle (if high, CPU may be overworked). Lastly <code>wa</code> is the percent of wait time (if high, CPU is waiting for I/O access).</p> <pre><code>%Cpu(s):  1.7 us,  0.6 sy,  0.1 ni\n</code></pre> <p><code>id</code>, <code>wa</code>, and <code>st</code> help identify whether the system is overworked. Here is a total  Now, let's go over each column in the process table.</p> <ul> <li>PID: Process ID.</li> <li>USER: The user running the process.</li> <li>PR: Priority of the task computed by the kernel on a scale of 20 to -20.</li> <li>NI: \"Niceness\" value, which involves the priority of user processes. 0 is the default and highest priority.</li> <li>VIRT: Virtual and physical memory, representing the total memory space allocated to a process, including RAM and swap. It's like a hypothetical maximum needed by the program. RES + Swap space = VIRT</li> <li>RES: Resident (Physical) memory used by the process. VIRT - Swap space = RES</li> <li>SHR: Shared memory.</li> <li>S: State of the process, where \"R\" means running, \"S\" means sleeping, and \"I\" is idle.</li> </ul>"},{"location":"Linux/Top/#htop","title":"htop","text":"<p>The most popular alternative, <code>htop</code>, provides more customization, scrolling and mouse support, color, and an overall cleaner interface. Unlike <code>top</code>, it does not come preinstalled but is worth checking out with a quick download.</p> <pre><code>sudo apt install htop\n</code></pre> <p>I recommend to learn <code>top</code> first and then try out <code>htop</code>, comparing the two. </p> <p>Upon running <code>htop</code>, you will first realize the displayed columns discussed above are exactly the same, as well as the tasks, load average, and up time at the top right. The main difference is the colorful, more readable TUI (text user interface) that supports mouse events and scrolling. An example of this is the CPU column, colored blue after clicking, ordering the processes by CPU consumption. </p> <p></p>"},{"location":"Linux/Top/#additional","title":"Additional","text":"<ul> <li>Mental Outlaw, a popular Linux youtuber, has reviewed <code>htop</code> and like me, recommends still learning <code>top</code> as it comes preinstalled on many Linux distros by default. At 1:08-1:55 of this video he compares the two.</li> <li>Great video by Learn Linux TV going more into load average.</li> </ul>"},{"location":"Linux/Distros/Distros%20Compared/","title":"Distros Compared","text":"<p>In preparation for this video, I had to decide which exact distros to compare, considering the sheer size in numbers of Linux distributions out there, I had to narrow it down. Going off popularity and user base seemed to be the best option. In order to get some accurate measurements I decided to compare the sizes of subreddits based on number of followers. I was surprised to see Arch in number one by a long shot, then we have Ubuntu, Fedora, and Linux Mint. Since I don't use Arch BTW, I can only give a valid comparison of the latter three.</p> <p>Starting with my very first distro, we'll cover Linux Mint.</p> <p>Linux Mint is based on Ubuntu, which in turn is based on Debian, basically, the great grandfather of many distributions. There is an exception which is Linux Mint Debian Edition, which is directly based on Debian and whose goal is to continue Linux Mint if Ubuntu were to ever disappear, basically they're cutting out the middle man and no longer relying on Canonical.</p> <p>Linux Mint is the only fully community driven distro of these three. What I mean by that is Ubuntu is funded by, ran and heavily contributed to by Canonical, and similar with Fedora and Red Hat. Though their is some nuance with their relationship, with the Fedora Project being their own organization separate from, but funded by Red Hat.</p> <p>Linux Mint on the other hand, basically has no affiliation with commercial interests and is entirely ran by volunteers and through donations. This can be either a positive or negative thing  depending on how you look at it. One day, development may slow down if people become less interested in volunteering to contribute and donate, Fedora on the other hand, being backed by a multi billion dollar company, could be seen as more stable and a better outlook for the future. On the flip side, there are positives to not having any company affiliation and being entirely community driven, it really depends the your perspective.</p> <p>For deciding what exactly to compare I decided to separate those who are more technical and use the terminal frequently, and other casual users who just want a nice desktop experience. Firstly, for the technical details, Ubuntu and Linux Mint, being based on Debian, use it's respective package managers and other suite of tools. APT, which stands for Advanced Package Manager, is a front-end to the lower level, more granular tool, DPKG, which you can use manually to install .deb packages. The equivalent in Fedora is DNF, a front end to it's lower level tool RPM tool, used to install .rpm packages.   </p> <p>As far as user experience, different features, and thoughts between distros like Ubuntu, Fedora, and Linux Mint, all I can say is am not strongly opinionated one way or the other. Each distro is mostly the same, they all their own file manager, Firefox, software manager, Gnome Terminal and a similar suite of applications. To show these similarities, I have some footage of each, starting with Linux Mint.</p> <p>show footage</p> <p>In order to compare Linux distributions, we have to ask for who and for what reason. There are distros like Fedora, Linux Mint, and Ubuntu for everyday desktop users, then there are commercial distros like Red Hat Enterprise Linux (RHEL), Rocky Linux, and Alma Linux.  </p> <ul> <li>User experience of desktop distros<ul> <li>LM</li> </ul> </li> <li>history, funding, up/downstream of what</li> </ul>"},{"location":"Linux/Distros/LMDE%20%28Debian%20Edition%29/","title":"LMDE (Debian Edition)","text":""},{"location":"Linux/Distros/LMDE%20%28Debian%20Edition%29/#overview","title":"Overview","text":"<p>Given that Mint Main, the Ubuntu based Linux Mint, is based on Ubuntu 22.04 LTS, the entire operating system is dependent on Canonical. So if Canonical goes bankrupt, or if they were to be purchased by IBM and went closed source, Linux Mint may no longer be supported. This is where LMDE (Linux Mint Debian Edition) comes in, serving to create independence from Canonical. </p> <p>LMDE is based directly off Debian, the 30 year old Linux distro with proven stability and longevity. This gives Linux Mint a backup if ties were cut with Ubuntu. Basically, the chain of dependence looks like this: Mint Main depends on Ubuntu, which in turn depends on Debian. While LMDE is built directly from the foundation, only relying on Debian .   </p>"},{"location":"Linux/Distros/LMDE%20%28Debian%20Edition%29/#my-experience","title":"My Experience","text":"<p>My personal belief is that any software will have a stronger foundation and longevity when build by fully open source developers. Those who work on chosen projects for passion and their own personal interest will build a better product than those at a 9-5. Debian, and the Linux Mint Team, are comprised of volunteers as non-profit organizations. Canonical, while supporting Open Source, is still a for-profit company. This is what interested me in LMDE, an operating system built solely by open source projects, not businesses.</p> <p>I was very excited learning about Debian, and how much of the world's servers, infrastructure and Linux Distros rely on this single operating system. The long history, going back to 1993 makes it one of the oldest Linux distros. Having my operating system closer to Debian and more \"pure\" felt right. So I made backup drives on my Ideapad 3 Gaming laptop, and freshly installed LMDE. </p> <p>Immediately, the wifi did not work, nor could I even enable it, so I went and found a post with a similar problem, where the central issue was the older kernel version. </p> <p>Given LMDE has a slower development, the Kernel is 5 minor releases behind Mint Main. Here is a comparison of LMDE5 (top) and Ubuntu based Linux Mint Cinnamon (bottom). The only solutions were to manually configure a network adapter driver, which was no longer supported by Realtek, or update the Kernel myself. </p> <p>Notice how the the LMDE5 kernel is 5.10 while Mint Main is at 5.15. This is the main difference and will cause issues if you do not consider the age of your hardware. If you have a newer laptop like my Ideapad3, I would advise against LMDE5 in favor of Mint Main.</p> <p>LMDE5</p> <p> Mint Main (Ubuntu Based)</p> <p></p> <p>Overall, the desktop environment of LMDE5 was smooth and responsive, and almost identical to the newer Cinnamon edition, being 6 minor releases behind. However, the lack of driver support, not only for wifi, but also for my Nvidia GPU, was my last straw. I ultimately chose to return to Mint Main.</p> <p>But first, I wanted to test LMDE5 on an older laptop. Surprisingly, the wifi worked, and even the bluetooth. I realized the reason was this X230 Thinkpad was 10 years old, with hardware from the kernel's time. The Ideapad 3's network adapter drivers did not work because it was a very recent laptop, with new hardware. So the lesson it, LMDE5 will work much better on older hardware. </p> <p>Through my own experience and some digging around online, I put together this list of positives and negatives.</p>"},{"location":"Linux/Distros/LMDE%20%28Debian%20Edition%29/#lmde-pros","title":"LMDE Pros","text":"<ul> <li>Generally faster and more responsive than Ubuntu-based editions, the iso file itself is only 1. 90GB, while the rest are over 2.8GB.</li> <li>Worth supporting if you believe Linux Mint should not rely on a for profit company, and  be fully in control by the community of open source developers.</li> <li>Possibly more stable with greater longevity. </li> </ul>"},{"location":"Linux/Distros/LMDE%20%28Debian%20Edition%29/#lmde-cons","title":"LMDE Cons","text":"<ul> <li>Less PPAs (personal package archives)  for storing and distributing software packages </li> <li>No driver manager, which means configuring drivers at the terminal with no graphical interface</li> <li>Older kernel version 5.10 means less hardware and driver support for newer computers. This results in incompatibility such as modern network adapters not working on old drivers.</li> <li>If you are relatively inexperienced, you will find less tutorials and support as fewer use LMDE.</li> </ul>"},{"location":"Linux/Distros/LMDE%20%28Debian%20Edition%29/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>If you are an experienced Linux user with knowledge of Debian package management (apt, dpkg), and make sure not to install on brand new hardware, LMDE5 will work great.</li> <li>For those that are inexperienced, or simply want things to work out of the box, Mint Main will be just fine, with Cinnamon having the most sleek features.</li> <li>For those using Mint Main, LMDE can serve as a contingency plan for a backup distro in case Ubuntu support is lost.   </li> </ul>"},{"location":"Linux/Distros/LMDE%20%28Debian%20Edition%29/#addtional-reading","title":"Addtional Reading","text":"<p>July 2023 News on LMDE 6 development, no ETA (estimated time of arrival) announced </p> <p>Linux Mint 21.2 \u201cVictoria\u201d Released Based on Ubuntu 22.04 LTS</p>"},{"location":"Linux/Docker/Tutorial/","title":"Tutorial","text":""},{"location":"Linux/Docker/Tutorial/#installation","title":"Installation","text":"<p>The fastest and easiest way to get docker up and running on Linux is the  Docker Install Script.This will install the latest stable version on supported Linux Distros. This includes the following:</p> <ul> <li>docker-ce - Community Edition Docker Engine</li> <li>docker-ce-cli - Docker Client/Command line Interface</li> <li>docker-buildx-plugin: Extends the Docker build with new features with a CLI plugin.</li> <li>containerd.io: A container runtime that manages the container\u2019s lifecycle.</li> <li>docker-compose-plugin - orchestrates and manages Docker containers with compose files</li> </ul> <p>Let's first fetch the script from the repo. </p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\n</code></pre> <p>Now let's verify the script by looking through the code. You can cat into less or open it with your editor of choice</p> <pre><code>cat install-docker.sh | less\n#OR\n[YOUR-EDITOR] install-docker.sh`\n</code></pre> <p>For additional verification, we can dry run to display steps before actually running them</p> <pre><code>sh install-docker.sh --dry-run`\n</code></pre> <p>Now if we are confident it it secure and understand everything it is installing, let's run the script for real.</p> <pre><code>sudo sh install-docker.sh`\n</code></pre> <p>Now add yourself to the docker group. This will allow you to run docker commands with without prefixing everything with <code>sudo</code>. Reboot your system for this to take effect.</p> <pre><code>sudo usermod -aG docker ${USER}\n</code></pre> <p>That's it! Now you should have Docker installed and ready to go.</p>"},{"location":"Linux/Docker/Tutorial/#pull-run-and-basic-usage","title":"Pull, Run and Basic Usage","text":"<p>There are two common ways to run containers. First, we can <code>pull</code> pre-built images from public registries like Docker Hub. Second, we can create our own dockerfile, <code>build</code> an image from it, and <code>run</code> it to create our own container.</p> <p>First let's go with the first method. Before pulling this image, you can check it out on  Docker Hub to get information about it. Optionally you can run  <code>docker search hello-world</code> from the command line.</p> <p>You can either <code>pull</code> and then <code>run</code>, or you can just run. Docker will first check for the image locally, and if it does not find it (in this case, if you didn't pull it), it will automatically pull it for you off of Dockerhub.</p> <pre><code>docker pull hello-world\n#OR\ndocker run hello-world\n</code></pre> <p>This hello-world image is really just to make sure your installation works correctly, and in this case, to familiarize yourself with the basic commands.</p> <p>To run a real container, you can pull the official Ubuntu image and run it, or again, just run it and have docker find and pull it for you.</p> <pre><code>docker run -it ubuntu bash\n</code></pre> <p>The <code>-i</code> flag is for interactive, which keeps the standard input open, allowing you to interact with the container like you would expect, and <code>-t</code> is for tty, which allocates a psuedo tty/terminal inside the container. This simulates a real terminal like when you SSH into a remote server. After we specify our image, in this case Ubuntu, we run a bash shell. Although for the official Ubuntu image, this is probably unnecessary and already configured as a command in the cmd attribute.</p> <p>After running the above command you should now be inside a real docker container. Play around with it, install whatever you want. If you have some balls, run <code>sudo rm -rf /</code> and see what happens. When you're done dinking around, you can <code>exit</code> the container. </p> <p>Changes to the container will not be saved until you <code>commit</code> them. Let's say you make a test directory with <code>mkdir test</code>, then <code>exit</code> the container.  When commiting, you enter the container_id followed by the name of the image. The container_id can be found after the user \"root\", I've copied and pasted it here. Every commit creates a new image as if it were a snapshot in time. You can see my test_image worked after listing the images with <code>docker images</code>.</p> <p></p> <p>Ok cool, so we can play around with a prebuilt image. My problem is, this Ubuntu image always logs me in as root. I don't like that. I want to be logged in as a regular user but have the option to act as root with <code>sudo</code>, like the normal Linux experience. This is ideal for a development environment, which is what I am interested in, and maybe you are too. Either way it will be useful for you to know how to create your own images from docker files.</p>"},{"location":"Linux/Docker/Tutorial/#making-containers-with-dockerfiles","title":"Making Containers with Dockerfiles","text":"<p>Making docker files is simple, just create one with your text editor (I use neovim btw). Just make sure to name it exactly \"Dockerfile\" with a capital D. This is what docker expects when you <code>build</code> the image from it.</p> <p><code>nvim Dockerfile</code></p> <p>Add this to your file. Replace USERNAME with whatever name you want and 1234 to your own password.</p> <pre><code>FROM ubuntu\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y sudo &amp;&amp; \\\n    useradd -m USERNAME &amp;&amp; \\\n    echo \"USERNAME:1234\" | chpasswd &amp;&amp; \\\n    adduser USERNAME sudo\n\nUSER USERNAME\n</code></pre> <p>Basically all we are doing, is creating an Ubuntu image with your own username and password. This way it will be similar to a regular Linux experience, with regular root privileges using <code>sudo</code> which prompts for your password.   </p> <p>Now build an image from the file. You do not have to specify the file, Docker fill find it. the <code>-t</code> stands for tag, this allows you to name your container with an optional tag name prefixed with a semicolon. Make sure you include the dot at the end for location.</p> <pre><code>docker build -t container_name:tag_name . \n</code></pre> <p>Now make sure it was created by listing images with <code>docker images</code>. </p> <p></p> <p>Ok, now you should be good to good to go. Spin up the c</p> <p>NOTES ARE STILL IN DEVELOPMENT</p>"},{"location":"Linux/Docker/Tutorial/#manage-docker-engine","title":"Manage Docker Engine","text":"<p>Let's learn how the docker engine works. It is managed as a <code>systemd</code> service and is thus managed with the  <code>systemctl</code> (system control) command. Let's verify it's active. <pre><code>sudo systemctl is-active docker\n</code></pre></p>"},{"location":"Linux/Docker/Tutorial/#removing-images-and-containers","title":"Removing Images and Containers","text":"<p>When trying to remove a image, you will often get an error because an active container is running off the image. To fix this, simply <code>stop</code> and <code>rm</code> that container referencing it's id. This will permanently delete the container and image, so make sure you know what you are doing.</p> <p></p>"},{"location":"Linux/Docker/Tutorial/#additional","title":"Additional","text":"<p>To check the command attribute of an image, enter the following. This is useful to know to check if you need to specify the shell when spinning up a container.</p> <p><code>docker image inspect [image] --format '{{.Config.Cmd}}'</code></p> <p>This should usually return <code>[/bin/bash]</code>. If it doesn't, you may have to add <code>bash</code> to this command:  <code>docker run -it [image] bash</code></p>"},{"location":"Linux/Linux%2B/SCRATCH/","title":"SCRATCH","text":""},{"location":"Linux/Linux%2B/SCRATCH/#read-all-files-and-include-their-file-name","title":"Read all files and include their file name","text":"<pre><code>more * | cat\ngrep ^ /dev/null *\nawk ' { print FILENAME $0 } ' *\n</code></pre>"},{"location":"Linux/Linux%2B/SCRATCH/#reverse-search","title":"Reverse Search","text":"<p>Reverse search with <code>ctrl+r</code> and search command, then hit <code>ctrl+r</code> again to go back further. <code>ctrl+s</code> to go forward. Through this may not work and conflict with another hotkey, if so, set the following manually or in <code>.bash_profile</code>. <code>stty -ixon</code></p> <p>For example, if I ran a long command with <code>rsync</code> yesterday, but can't remember exactly what I ran, or flags, I used. I couldn't get this working in this code block but in my terminal ==rsync== is highlighted in the long command as well. <pre><code>(reverse-i-search)`rsync': sudo rsync -av /home/promptier/downloads/ /media/promptier/c3ff508b-cabf-40f4-9f5e-cd5b2f102cfa/downloads/\n</code></pre> Cycle with <code>ctrl+s</code> to go forward. <pre><code>(i-search)`rsync': rsync -av --dry-run // /media/promptier/usb-linux-ideapad/\n</code></pre></p>"},{"location":"Linux/Linux%2B/SCRATCH/#systemd","title":"Systemd","text":"<p>More possible newer alternatives in systemd. (ChatGPT)</p> <p>Inittab &gt; systemd configuration SysV init scripts &gt; systemd service units Log rotation &gt; systemd journal /etc/network/interfaces &gt; systemd-networkd /etc/resolv.conf &gt; systemd-resolved initrd &gt; systemd-initrd rc.local &gt; systemd rc-local /etc/hostname &gt; systemd-hostnamed /etc/securetty &gt; systemd-securetty</p>"},{"location":"Linux/Linux%2B/SCRATCH/#mkdocs-notes","title":"Mkdocs notes","text":"<p>add <code>text</code> or <code>nohighlight</code> to a codeblock to prevent code highlighting post</p>"},{"location":"Linux/Linux%2B/SCRATCH/#firewalls","title":"Firewalls","text":""},{"location":"Linux/Linux%2B/SCRATCH/#logical-volume-manage-lvm-using-kvm-qemu-and-qcow2","title":"Logical Volume Manage (LVM) Using KVM-QEMU and Qcow2","text":"<p>I attempted to create qcow2 formatted disks in my Fedora Virtual Machine in KVM. This is an easy playground to learn LVM as opposed to say, using real physical hard disks. The process seems to look like this. </p> <p>First create virtual images inside your virtual machine. <code>sudo qemu-img create -f qcow2 disk.qcow2 100M</code></p> <p>Associate that image with a loopback device, so it can \"pretend\" to be a block device <code>sudo losetup -f disk3.qcow2</code></p> <p>the <code>-f</code> flag on its own simply finds the first unused block device. You can optionally choose to be explicit in selecting the loopback device to associate with a given virtual image. <pre><code>\u276f sudo losetup -f\n/dev/loop3\n</code></pre></p> <p>Now to use LVM on these devices, you need to see the loopback devices and their virtual disks show up using <code>lsblk</code>, this command will show your real block/storage devices such as <code>nvme0n1</code>, <code>sda</code>, <code>sdb</code> etc. as well as the \"psuedo\" block devices we are trying to use for this exercise. The problem is the <code>qemu-img</code> program seems to always create images with a size of 193K, no matter my size option of <code>100M</code>, <code>1G</code>, <code>12345</code>, etc. I got as far as setting up loop devices, except with attempting to create a physical volume using <code>pvcreate</code>, the size of 193K is too small. <pre><code>[joseph@fedora /]$ lsblk \nNAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS\nloop0    7:0    0  192K  0 loop \nloop1    7:1    0  192K  0 loop \nloop2    7:2    0  192K  0 loop \nsr0     11:0    1    2K  0 rom  \nzram0  251:0    0  4.7G  0 disk [SWAP]\nvda    252:0    0   15G  0 disk \n\u251c\u2500vda1 252:1    0    1M  0 part \n\u251c\u2500vda2 252:2    0    1G  0 part /boot\n\u2514\u2500vda3 252:3    0   14G  0 part /home\n                                /\n[joseph@fedora /]$ losetup -l\nNAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE    DIO LOG-SEC\n/dev/loop1         0      0         0  0 /disk2.qcow2   0     512\n/dev/loop2         0      0         0  0 /disk3.qcow2   0     512\n/dev/loop0         0      0         0  0 /disk1.qcow2   0     512\n[joseph@fedora /]$ sudo pvcreate /dev/loop0\n[sudo] password for joseph: \n  Cannot use /dev/loop0: device is too small (pv_min_size)\n</code></pre></p> <p>SOLVED</p> <p>tdlr: qcow2 makes no sense to use. Instead just use <code>dd</code> or better, <code>fallocate</code> to create a file, then attach to loop device, then proceed with LVM.</p> <p><code>fallocate</code> can do exactly what <code>dd</code> can in this case, except it just preallocates the size instead of writing zeros as in the case of <code>dd if=/dev/zero of=disk.img</code>.</p> <p><code>-L</code> for fixed size, then <code>-l</code> to fill the remaining space dynamically. <code>-r</code> is to (r)esize the filesystem in addition to the logical volume. </p> <p>To create a filesystem on top of a LV, make sure it's atleast 300M. <code>mkfs.ext4 /dev/my_vg/main_lv</code></p> <p><code>lvreduce -L -2.5G -r /dev/vg00/vol_projects</code> <code>lvextend -l +100%FREE -r /dev/vg00/vol_backups</code></p> <p><code>pvs, vgs, lvs</code> shows quick information, i.e <code>pvdisplay, vgdisplay, lvdisplay</code> shows long information</p>"},{"location":"Linux/Linux%2B/SCRATCH/#free-memory-vs-file-cache","title":"Free Memory vs File Cache","text":"<p>Linux caches memory a particular way that may confuse people. Visit linuxatemyram.comfor an explanation. Memory in the 'free' column does nothing and is empty, some say this number should be low because \"unused memory is wasted memory\", though you should worry if it approaches 0.   </p> <p>available = buff/cache + free</p> <p>You can see your buffer/cache with <code>free</code>.  <pre><code>\u276f free -m\n               total        used        free      shared  buff/cache   available\nMem:           27909        3956       22620          44        1332       23488\nSwap:           2047           0        2047\n</code></pre> echo 1 to free page cache, 2 for dentries and inodes, and 3 for page cache, dentries, and inodes You'll need sudo all the way to the end, so you may need to execute it this way. <pre><code>\u276f sudo bash -c \"sudo echo 1 &gt; /proc/sys/vm/drop_caches\"\n</code></pre> Use <code>swapon</code> and <code>swapoff</code> to manage devices and files for paging and swapping</p>"},{"location":"Linux/Linux%2B/SCRATCH/#network-troubleshooting","title":"Network Troubleshooting","text":"<p>Red Hat Network Troubleshooting </p> <p>OSI Model (7 layers) is older and more theoretical. TCP/IP (4 Layers) is more modern and practical. Top 3 OSI model layers are combined for Application Layer and bottom two are combined as Network Access Layer. </p> <p>Check if your physically connected with <code>ip link show</code> and and with <code>-s</code> for statistics (more output).</p> <p>Check your router/default gateway's MAC address with <code>ip neigh</code>. If you just got a new router, if not done automatically, you may have to manually manipulate the ARP table with <code>ip neigh add</code>, <code>change</code> <code>del</code>, etc.</p> <p>Give brief output with <code>-br</code> <pre><code>\u276f ip -br a show\nlo               UNKNOWN        127.0.0.1/8 ::1/128 \nenp2s0           DOWN           \nwlo1             UP             192.168.50.36/24 fe80::976c:abc6:7f42:d74c/64 \nvirbr0           DOWN           192.168.122.1/24 \nbr-150bc1eb3b78  DOWN           172.18.0.1/16 \ndocker0          DOWN           172.17.0.1/16 fe80::42:98ff:fe61:5cc/64 \n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/","title":"1.1 Linux Fundumentals","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#file-system-hierarchy-fhs","title":"File System Hierarchy (FHS)","text":"<p>The FHS is a standard describing the layout of Unix-like systems. Regardless of what distribution you are on, you will always encounter these same directories, so it is worth understanding. Throughout exploring these directories you will come to learn that in Linux,  everying is a file. Your entire memory is represented in a single file as <code>/dev/mem</code>.  Each ongoing process is represented as a directory with associated files in <code>/proc</code>, and so on. Learning these directories and their role in the FHS is fundumental to Linux and give you foundation for the rest of the sections. %%  understanding the wierd sbin, bin, and other splits http://lists.busybox.net/pipermail/busybox/2010-December/074114.html  %%</p> <p>Try listing (<code>ls</code>) each directory and explore on your own system. <pre><code>ls /\n    bin   cdrom      dev  home  lib32  libx32    media  opt   root  sbin  swapfile tmp  varboot   etc  lib   lib64  lost+found  mnt    proc  run   srv   sys       usr\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#boot","title":"/boot","text":"<p>Upon starting a Linux system, after the BIOS or EUFI firmware has selected a boot device, such as a hard disk or SSD, it searches for a bootloader within that device inside the /boot directory. Grub (Grand Unified Bootloader) then uncompresses the <code>vmlinuz</code> executable file into memory. <code>vmlinuz</code> is an image of the entire kernel. compressed into one file. </p> <p>The next step after the kernel has been loaded into memory, is Grub handing over control to the kernel to complete the boot process. A temporary root file system is created with <code>initrd</code> (initial ramdisk) that contains just enough loadable modules to give the kernel access to the rest of the hardware. </p> <p>In many systems, boot is a seperate partition, you can check this by listing block devices with <code>lsblk</code>. You can see on my NVME card, it is indeed a seperate partition. <pre><code>lsblk\nNAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nnvme0n1     259:0    0   1.8T  0 disk \n\u251c\u2500nvme0n1p1 259:1    0   260M  0 part /boot/efi\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#proc","title":"/proc","text":"<p>The process directory contains a subdirectory for every ongoing process, similar to what is displayed in Windows Task Manager or Linux <code>top</code> command. The process with PID (process ID) of 1, <code>/proc/1</code>, is a special process know as the init process which is responsible for starting essential system services, and which all other processes are children of. This is often managed by <code>systemd</code>, replacing <code>init</code>. </p> <p>proc is one of the three virtual/psuedo filesystems as it creates files dynamically, on the spot to represent the current processes. </p> <p>Besides ongoing processes, the <code>/proc</code> directory also contains some kernel and hardware information. At one point in history, this got messy and Linux developers decided to create another directory <code>/sys</code> to contain this.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#sys","title":"/sys","text":"<p>As the second virtual filesystem, sys generates files and directories to represent the current hardware layout. This provides an interface to the kernel.</p> <pre><code>ls sys\nblock  bus  class  dev  devices  firmware  fs  hypervisor  kernel  module  power\n</code></pre>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#var","title":"/var","text":"<p><code>/var</code> contains variable data that is expected to change and grow during operation. This includes log files, mail, cache and more. <code>var/spool</code> contains files waiting to be excecuted such as printer ques and cron jobs. <code>/var/lib</code> contains state information of applications or the system, this is data that programs modify as they run, this is often for preserving condition of an application between instances. <code>/var/tmp</code> is for temporary files that persist across reboot. Unlike <code>/tmp</code> which is removed after a reboot.</p> <pre><code>ls var\nbackups  cache  lib  local  lock  log  mail opt  run  spool  tmp\n</code></pre>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#usr","title":"/usr","text":"<p><code>/usr</code> contains user-related data, programs and resources. Files not necessary for single user mode, a minimal system state for troubleshooting and maintenance. Most command's binaries that are commonly ran in the terminal are within <code>/usr/bin</code>.  These are commands considered non-essential, as opposed to built in shell commands. </p> <pre><code>which cd\ncd is a shell builtin\n\nwhich ls\n/usr/bin/ls\n</code></pre> <p><code>/usr/local</code> contains locally installed software and files that are seperate from the core operating system and package management system.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#lib-bin-and-sbin","title":"/lib, /bin and /sbin","text":"<p><code>/lib</code> contains libraries for <code>/bin</code> and <code>/sbin</code>. sbin is essential binaries with superuser (root) priviledges. Bin is similar and contains trivial binaries such as <code>cat</code> and <code>ls</code> . Both are required for single user mode.  </p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#dev","title":"/dev","text":"<p><code>/dev</code> is the third virtual file system and contains a layout of devices. When you plugin a USB, it will appear as <code>/dev/sda</code>, if you plugin another, it will appear as <code>dev/sdb</code>. </p> <p>Most device files are either character or block type.  Using the long <code>-l</code> format of <code>ls</code>, we can see the type by the first letter. Character devices, which begin with a letter <code>c</code>, do not have buffering, and are accessed through a stream of sequential data, one byte after another. Block (<code>b</code>) devices are accessed through a cache, and get their name because they are often read/write to a block at a time.</p> <p>The first special character device, <code>dev/zero</code>,  provides an endless stream of null characters, which can be used to zero out a hard disk. <code>dev/null</code> is essentially a black hole that discards anything directed into it. <code>dev/urandom</code> is a unlimited cryptographic (psuedo) random number generator.</p> <pre><code>ls -l /dev\ncrw-rw-rw-   1 root root      1,     5 Oct 30 14:10 zero\ncrw-rw-rw-   1 root root      1,     3 Oct 30 14:10 null\ncrw-rw-rw-   1 root root      1,     9 Oct 30 14:10 urandom\n</code></pre> <p>Some use cases of <code>urandom</code> include generating passwords, and to generate entropy for TLS/SSL modules like <code>mod_ssl</code> on Apache web servers. <pre><code>&lt; /dev/urandom tr -dc _A-Z-a-z-0-9 | head -c12;echo\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#etc","title":"/etc","text":"<p>At some point in Unix history, this likely stood for \"etcetera\", though now some like to squeeze in the acronym \"editable text configuration\". Although most simply pronounce it \"etsy\" and know it as the place for configuration files. </p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#opt","title":"/opt","text":"<p>The FHS standard defines <code>/opt</code> as \"software and add-on packages that are not part of the default installation\". Often when a company is deploying an application, they have the option of either placing it all in one directory in <code>/opt/[application]</code> or share it's files across <code>/usr/local/bin</code> and  <code>/usr/local/lib</code>. </p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#boot-process","title":"Boot Process","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#step-by-step-boot-process","title":"Step by Step Boot Process`","text":"<ol> <li>Firmware (UEFI/BIOS)<ul> <li>When you turn on a computer, the firmware built into the hardware initiates the boot process.</li> <li>It performs a power-on-self-test (POST) to check hardware components and initialize them.</li> <li>The firmware looks for a boot device, usually the hard drive or SSD, where the bootloader is stored.</li> </ul> </li> <li>Bootloader (GRUB)<ul> <li>Once the firmware identifies the boot device, it hands over control to the bootloader.</li> <li>The bootloader loads the kernel into memory and passes control to it.</li> </ul> </li> <li>Kernel<ul> <li>The kernel is the core of the operating system. It initializes hardware, manages memory, and provides essential services to user programs.</li> <li>It mounts the root file system, initializes drivers for hardware components, and starts user space initialization.</li> </ul> </li> <li>User Space<ul> <li>After the kernel initializes, it starts the user space by launching systemd (previously init). This is the first process and can be seen with <code>pstree</code> or <code>ps -p 1</code>.</li> </ul> </li> </ol> <p>Depending on the firmware (BIOS or EUFI), a hard disk or SSD will have 1 of 2 partitioning schemes, MBR or GPT.</p> <p>Master Boot Record (MRB) is used in older systems that is the first 512 bytes of a storage device which contains how and where the operating system is located in order to be booted. Legacy BIOS systems using MBR can only support drives up to 2TB and up to 4 partitions.</p> <p>The bootstrap binary of a BIOS equipped machine is located in the MBR of the first storage device.</p> <p>GUID Partition Table (GPT) is the newer standard that works with UEFI that can have any number of ESP (EFI System Partitions) anywhere an a drive to load multiple operating  systems, unlike MBR, which is limited to one.  GPT also supports virtually unlimited size and up to 128 partitions.</p> <p>EUFI boot process searches for EFI applications which are usually bootloaders, such as GRUB, which can either act as a EFI application or a BIOS bootloader for older setups. ESP is where EFI applications are stored and are automatically created on the storage device during OS installation.</p> <p>Older BIOS systems often overwrited MBRs during installation of multiple OS's, GPT makes this easier becuase ESP can be located anywhere on the drive, they are not location dependent, like MBR, which must be at the first 512 bytes of the drive.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#boot-files-and-commands","title":"Boot Files and Commands","text":"<p>mkinitrd - creates an initial ram disk image <code>initrd.img</code> which is used by the kernel for preloading the block device modules (such as IDE, SCSI or RAID) which are needed to access the root filesystem.  dracut -  a more modern alternative to <code>mkinitrd</code>. Both create an initramfs image whose sole purpose is to mount the temporary root filesystem in order to transition to the real root filesystem.  grub-install - installs grub onto a device, usually <code>boot/grub</code> grub-mkconfig - either generates a new <code>grub.cfg</code> file or updates the existing one also located in <code>boot/grub</code> grub-update - stub for <code>grub-mkconfig\u00a0-o\u00a0/boot/grub/grub.cfg</code></p> <p>initrd.img -  initrd is mounted in memory to determine which kernel modules are are needed to support the current hardware. Explanation on forum vmlinuz - the entire compressed kernel which is uncompressed and begins the initial systemd/init process, which starts all other processes Both are used both during the initial OS install, and every boot therafter.</p> <p>Note: <code>vmlinuz</code> kernel image is loaded first by the bootloader, followed by the <code>initrd</code>. The <code>initrd</code> serves as an intermediary step to initialize hardware and load necessary drivers before transitioning to the actual root file system.</p> <p><code>initrd</code> and <code>initramfs</code> are just two different methods of loading a temporary root file system into memory to start early kernel modules. </p> <p>dracut similar to initrd in providing an initial ramdisk during boot process and is a more modern and flexible tool that dynamically generates ramdisk images based on the system's hardware. It has replaced initrd in some distributions.</p> <p>Here are two short descriptions of dracut from the man page and it's github repository repsectively: \"low-level tool for generating an initramfs/initrd image\" \"a new initramfs infrastructure\"</p> <p>Kernel Panic - Critical error condition that usually results in the entire system becoming unresponsive and requiring a reboot. Nothing is userspace runs during and after the system panics, it will instead immediately shutdown all other CPUs, and the current CPU processing the panic will never return to userspace</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#boot-sources","title":"Boot Sources","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#preboot-excecution-environments-pxe","title":"Preboot Excecution Environments (PXE)","text":"<p>PXE is meant for booting off of a network, as opposed to a local storage device. This is done through TFTP (Trivial File Transfer Protocol), a simpler FTP which does not require authentication,. Once connected, the server will tell the client where to locate the boot files. iPXE is the newer protocol which uses HTTP to pull files. PXE and iPXE are essentially just one approach to provisioning and automate the deployment of operating systems and configurations for servers.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#optical-disc-image-iso","title":"Optical Disc Image (ISO)","text":"<p>ISOs are copies of of entire optical disks such as DVDs or CDs. Although bootable USB drives have largely replaced optical media, ISO files can still be used to create bootable USB sticks. The most common tool to create bootable drives is Rufus, although on my Linux Mint system below, I have downloaded a Fedora ISO, which I can create into a bootable stick from the file manager itself.</p> <p>![[iso-file.png]]</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#basic-package-compilation-from-source","title":"Basic Package Compilation From Source","text":"<p>Although software is often installed with a package manager (APT, RPM, etc.), there are times when it is necessary to install from source code. This could be when you are writing the software yourself, or the software is not listed in your package manager's repositories. </p> <p>Linux distros rarely come with the compiling/build tools themselves. To install on Debian based distros, run <code>sudo apt install build-essential</code> or <code>sudo dnf groupinstall \"Development Tools\"</code> on Red Hat based distros. These both install a set of essential development tools and libraries necessary for building software from source code.</p> <p>The following three commands are commonly used together and in this order to build and install software from source code on Unix-like systems.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#configure","title":"./configure","text":"<p>Configures the software build process. It checks the system for dependencies, sets build options, and generates a Makefile from Makefile.in that contains instructions for compiling the software.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#make","title":"make","text":"<p>running <code>make</code> will use the Makefile to build the program. It compiles the source code and produces executable binaries. I recommend running <code>man make</code> and digging around, here is a snippet: <pre><code>The  make  utility will determine automatically which pieces of a large program need to be recompiled, and issue the commands to recompile them. ...  Our examples show C programs, since they are  very  common,  but  you can use make with any programming language whose compiler can be run with a shell command.  In fact, make is not limited to programs.  You can use it to describe  any task  where  some  files  must  be  updated  automatically from others whenever the others change.\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#make-install","title":"make install","text":"<p>This command will also use the Makefile but will use it to install the program once it has been compiled. This involves installing binaries, libraries, and other necessary files to make the software available to use.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#example","title":"Example","text":"<p>To practice these commands, you can go to gnu.org and download the hello-2.12.tar.gz at the very bottom.</p> <p>You can extract the file with <code>tar -zxvf [file]</code>. The <code>z</code> flag is for g-zipped files, hence the <code>.gz</code>. <code>x</code> for extract, optional <code>v</code> for verbose output, and <code>f</code> to specify the file. This should create a directory of the extracted files.</p> <p>Once in that directory, <code>ls</code> to make note of the file <code>configure</code> , then run <code>./configure</code> to execute it. Now with a <code>makefile</code>, run <code>make</code>. This should create an executable <code>hello</code> which can be ran with <code>./hello</code>. You should get a \"Hello World!\" output on your terminal. But to install the program globally, run <code>sudo make install</code>, this will install the package's files in <code>/usr/local/bin</code>, <code>/usr/local/lib</code>, <code>/usr/local/man</code>, etc. </p> <pre><code>\u276f sudo make install\n[sudo] password for promptier:     \n./mkinstalldirs /usr/local/bin /usr/local/info\nmkdir /usr/local/info\n/usr/bin/install -c hello /usr/local/bin/hello\n/usr/bin/install -c -m 644 ./hello.info /usr/local/info/hello.info\n\u276f hello \nHello, world!\n\u276f which hello\n/usr/local/bin/hello\n</code></pre>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#basic-storage-concepts","title":"Basic Storage Concepts","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#block-storage","title":"Block Storage","text":"<ul> <li>No Metadata, or very minimal (basic file attributes)</li> <li>Each block has a unique ID</li> <li>Filesystems are often built on top of blocks</li> <li>Accessed through iSCSI networking protocol, or traditional Fibre Channel which requires hardware (cables, adapters, switches, etc.). Read more here</li> <li>Used in SANs (Storage Area Network) or cloud based storage</li> <li>highly performant and scalable, though expensive</li> <li>Examples include relational databases, email servers and VMWare's Virtual Machine Filesystem (VMFS)</li> </ul>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#object-storage","title":"Object Storage","text":"<ul> <li>Relatively new concept, often used in the cloud, Amazon S3 is a common example.</li> <li>Often accessed through an API.</li> <li>stored in \"data lakes\" or \"data pools\" in a flat manner, with no file hierarchy.</li> <li>ease of searchability and cost efficient. Best for large unstructured data.</li> <li>each object has metadata (size, date modified, permissions, etc.) and an UUID.</li> <li>poor performance because of the heavy metadata overhead</li> </ul>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#file-storage","title":"File Storage","text":"<ul> <li>Oldest and most widely used storage system</li> <li>Used with NAS (Network Attached Storage)</li> <li>Tree-like, hierarchical structure with files within nested folders, each being a child or parent</li> <li>Not very scalable.</li> <li>Can be visualized with the <code>tree</code> command.  <pre><code>tree\n.\n\u251c\u2500\u2500 Desktop\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bash\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 dep.txt\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 folders.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tar\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 file1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 file2\n</code></pre></li> </ul>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#filesystem-in-userspace-fuse","title":"Filesystem in Userspace (FUSE)","text":"<p>Used to mount and unmount filesystems in userspace without requiring root level, kernel access. Commonly used by <code>sshfs</code>, a tool to mount a remote filesystem using SFTP. Another example that uses FUSE are [[1.6 Package Management and Sandboxed Apps#Appimages|Appimages]].</p> <p>From the man page of <code>fusermount</code> <pre><code>Simple interface for userspace programs to export a virtual filesystem to the Linux kernel. It also aims to provide a secure method for non privileged users to create and mount their own filesystem implementations.\n</code></pre> For further reading, see Wikipedia and source code. </p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#raid","title":"RAID","text":"<p>RAID stand for Redundant Array of Independent/Inexpensive Disks. RAID gives a group of hard drives or block storage devices redundancy and data protection against disc failure. Different RAID levels offer different degrees of fault tolerance. Animated Video Explaining RAID 0, 1 and 10</p> <p>Raid 0 - Isn't even RAID in a sense, as it provides zero fault tolerance - Raid 0, or \"disk striping\", splits data across drives evenly. - With 4 drives and a 4 megabyte file being saved, 1 megabyte per drive is spread across and thus increases speed. This also means each drives space is being taken advantage of. - Very fast but if even one drive fails, all data is lost, this means RAID 0 isn't just not fault tolerant, it increases the chances of data loss.</p> <p>Raid 1 - Known as \"mirroring\", data is stored on a drive in addition to each mirror drive. Atleast 2 drives are required for RAID 2 -  A drive can fail and the controller will just use either use the first drive or any of it's mirrors for data recovery and continuous operation. - A disadvantage is less effective storage capacity as all data gets written at least twice</p> <p>Raid 5 - RAID 5 stripes data across each drive and provides fault tolerance with each drive having a parity. - Each parity is equivalent to an entire drive, meaning if you have 4 disks totaling 4 terabytes, only 3 terabytes will be used for actual data storage.  - Can only handle 1 disk failure at a time</p> <p>Raid 6 - A more fault tolerant RAID 5 that can handle 2 disk failures at once as parity is spread twice to all disks. - Minimum of 4 disks required. - In a setup of 4 disks totaling 4 terabytes, only 2 terabytes of actual data storage is available, as 2 disks are used to store a double parity. - Read performance is the same as RAID 5, but write performance is slower as it must write double the parity blocks instead of 1. Animated Video of RAID 5 and 6</p> <p>Raid 10 - 2 drives are mirrored in a RAID 1 setup, with a minimum of 4 drives. - Combines the fault tolerance of RAID 1 and the speed of RAID 0 - The downside is you can only use 50% of space for actual storage, the other half is mirrored.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.1%20Linux%20Fundumentals/#listing-hardware-information","title":"Listing Hardware Information","text":"<p>Linux Kernel Modules related to hardware devices are also called drivers. <code>lspci</code> will list your PCI devices, each starting with a  unique address, which more info can be found with <code>lspci -s [address] -v OR -k</code>. This will show what kernel module is in use which can be found in <code>lsmod</code>.</p> <p>To find you network card, pipe the output of <code>lspci</code> into <code>grep -i</code> for an insensitive search. <pre><code>lspci | grep -i network\n03:00.0 Network controller: MEDIATEK Corp. MT7921 802.11ax PCI Express Wireless Network Adapter\n</code></pre></p> <p>Now let's see what kernel modules the network card is using with the verbose <code>-v</code> output. <pre><code>lspci -s 03:00.0 -v\n03:00.0 Network controller: MEDIATEK Corp. MT7921 802.11ax PCI Express Wireless Network Adapter\n    DeviceName: Realtek RTL8111E Ethernet LOM\n    Subsystem: Lenovo Device e0bc\n    Physical Slot: 0\n    Flags: bus master, fast devsel, latency 0, IRQ 73, IOMMU group 11\n    Memory at fc02000000 (64-bit, prefetchable) [size=1M]\n    Memory at fc02100000 (64-bit, prefetchable) [size=16K]\n    Memory at fc02104000 (64-bit, prefetchable) [size=4K]\n    Capabilities: &lt;access denied&gt;\n    Kernel driver in use: mt7921e\n    Kernel modules: mt7921e\n</code></pre> We could then use <code>lsmod</code> to find more about the kernel module, such as the size and other modules that are dependent on it. <pre><code>lsmod | grep mt7921e\nmt7921e                94208  0\n</code></pre></p> <p><code>lsusb</code> is similar to the previous command. With option -d or -s, command lsusb shows the current USB device mappings as a hierarchical tree. A specific USB device can be found with the <code>-d</code> flag followed by the id.</p> <pre><code>lsusb -vd 1d6b:0003\n\n[very verbose output]\n</code></pre> <p><code>dmidecode</code> is a tool for dumping a computer's\u00a0DMI table contents in a human readable from. This includes detailed hardware information as well as serial numbers and BIOS revision that does not require probing the actual hardware. Use the type flag <code>-t</code> to specify what hardware, such as <code>dmidecode -t memory</code> or <code>dmidecode -t processor</code>.</p> <p>Here are some output from <code>dmidecode</code> from my computer. <pre><code>BIOS Information\n        Vendor: LENOVO\n        Version: H3CN32WW(V2.02)\n        Release Date: 02/23/2022\n        BIOS Revision: 1.32\n        Firmware Revision: 1.32\n\nHandle 0x0001, DMI type 1, 27 bytes\nSystem Information\n        Manufacturer: LENOVO\n        Product Name: 82K2\n        Version: IdeaPad Gaming 3 15ACH6\n        Serial Number: MP2BK1DS\n        UUID: 1c45bdff-12cd-11ed-8c90-e4a8dfe66d9e\n\nHandle 0x0004, DMI type 4, 48 bytes\nProcessor Information\n        Socket Designation: FP6\n        Type: Central Processor\n        Family: Zen\n        Manufacturer: Advanced Micro Devices, Inc.\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/","title":"1.2 Manage Files and Directories","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#file-editing","title":"File Editing","text":"<p>Note throughout this writing I use the words column and field interchangeably as well as record and row. </p> <p>In the Unix Philosophy, programs are written to handle text streams, as they are the universal interface. The following programs are essential for manipulating these text streams for common tasks at the terminal as well as more complex bash scripting in Linux. </p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#cut","title":"Cut","text":"<p>The simplest command to manipulate fields is <code>cut</code>. The default delimiter is a tab but can be specified with <code>-d[character]</code> and only supports a single character. Include desired fields with <code>-f</code>.</p> <p>Let's take a look at the <code>/etc/passwd</code> file, which contains all human and non-human users on the system.  <pre><code>~$ cat /etc/passwd \nroot:x:0:0:root:/root:/bin/bash\ndaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin\nbin:x:2:2:bin:/bin:/usr/sbin/nologin\n</code></pre> Now let's just grab the username and shell. I've set the delimiter <code>-d:</code> and specified I want fields 1-7 <code>-f1,7</code>. <pre><code>~$ cut -d: -f1,7 /etc/passwd\nroot:/bin/bash\ndaemon:/usr/sbin/nologin\nbin:/usr/sbin/nologin\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#awk","title":"Awk","text":"<p>The <code>awk</code> command is a much larger utility and is an entire programming language in and of itself. It supports both columns and rows. Awk is often compared to Perl (Practical Extracting and Reporting Language) for complex text/data processing jobs, such as handling large csv files.</p> <p>To specify your field's with the <code>-F</code> flag, and make sure it's uppercase. Let's print out just the username and shell of <code>etc/passwd</code> as before with the cut command. <pre><code>awk -F: '{print $1, $7}' /etc/passwd\nroot /bin/bash\ndaemon /usr/sbin/nologin\nbin /usr/sbin/nologin\n</code></pre> Unlike cut, which just supports field separation, awk can select records (rows) with <code>NR</code> (number of record). <pre><code>~$ awk -F: 'NR==2 {print $1, $7}' /etc/passwd\ndaemon /usr/sbin/nologin\n</code></pre> If a record has 8 fields, <code>$8</code> will select the last field, as the number of fields (NF) is eight. In this case <code>$NF</code> would be equivalent to <code>$8</code>. Knowing this, we can always select the last field of any given record with <code>$NF</code>. <pre><code>~$ awk -F: '{print $NF}' /etc/passwd\n/bin/bash\n/usr/sbin/nologin\n/usr/sbin/nologin\n</code></pre> Awk is a very powerful tool and this is just the tip of the iceberg. But I hope was a good introduction.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#sed","title":"Sed","text":"<p>Say we have a file <code>todo.txt</code> and want to manipulate by records/rows.  <pre><code>~$ cat todo.txt \n1. water plants\n2. walk dog\n3. finish homework\n4. study linux\n</code></pre> By default, sed will print out the line specified, then the whole file again.  <pre><code>~$ sed '1p' todo.txt \n1. water plants\n1. water plants\n2. walk dog\n3. finish homework\n4. study linux\n</code></pre> To prevent this, use the <code>-n</code> flag for (n)o output. Think of it as (n)ot printing the whole file, rather only what you specify.  <pre><code>~$ sed -n '1p' todo.txt \n1. water plants\n</code></pre> Specify ranges with a comma. <pre><code>~$ sed -n '1,3p' todo.txt \n1. water plants\n2. walk dog\n3. finish homework\n</code></pre> Use a semicolon as a delimiter. <pre><code>~$ sed -n '1p;4p' todo.txt \n1. water plants\n4. study linux\n</code></pre> When deleting lines, omit the <code>-n</code> flag. <pre><code>~$ sed '1,2d' todo.txt \n3. finish homework\n4. study linux\n</code></pre></p> <p>Besides selecting records of a file to print, we can find and replace with regex-like syntax. Say I have a dockerfile and I've decided to change the username. Instead of manually editing the file by changing each instance, I can use sed to substitute each value. <pre><code>~$ sed 's/promptier/NEWUSERNAME/' Dockerfile \nFROM ubuntu\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y sudo &amp;&amp; \\\n    useradd -m NEWUSERNAME &amp;&amp; \\\n    echo \"NEWUSERNAME:1234\" | chpasswd &amp;&amp; \\\n    adduser NEWUSERNAME sudo\n\nUSER NEWUSERNAME\n</code></pre> As this example only prints to STDOUT, we can choose instead to edit the file in place with the <code>-i</code> flag or create a new file. <pre><code>~$ sed -i 's/promptier/NEWUSERNAME/' Dockerfile \nOR\n~$ sed 's/promptier/NEWUSERNAME/' Dockerfile &gt; Dockerfile2\n</code></pre> Keep in mind substituting will run a single time on each line. If there were two or more instances of \"promptier\" on single line, I would add the global flag <code>-g</code> at the end to replace all instances : <code>sed 's/promptier/NEWUSERNAME/g'</code></p> <p>Two useful characters to know are the caret <code>^</code> and dollar sign <code>$</code> which represent the beginning and end of a line. These are common regex syntax that are used in both sed and vim. As an example, the way you would delete all blank lines in vim would be to check if the beginning of the line is immediately proceeded by the end of the line. <pre><code>~$ cat todo.txt \n1. water plants\n\n2. walk dog\n\n3. finish homework\n\n4. study linux\n\n~$ sed '/^$/d' todo.txt \n1. water plants\n2. walk dog\n3. finish homework\n4. study linux\n</code></pre></p> <p>It is possible to insert lines before and after a match with <code>i</code> and <code>a</code>. Here I add a note after \"walk dog\". I could optionally insert before the line with <code>i</code>. Note the insertion begins with a backslash. <pre><code>\u276f sed '/dog/a\\  -remember a leash' todo.txt \n1. water plants\n2. walk dog\n  -remember a leash\n3. finish homework\n4. study linux\n</code></pre></p> <p>Swap characters with <code>y</code>. This is identical to the translate <code>tr</code> command. <pre><code>\u276f sed 'y/sed/XYZ/' todo.txt \n1. watYr plantX\n2. walk Zog\n3. finiXh homYwork\n4. XtuZy linux\n\n\u276f cat todo.txt | tr 'sed' 'XYZ' \n1. watYr plantX\n2. walk Zog\n3. finiXh homYwork\n4. XtuZy linux\n</code></pre></p> <p>Say I have a bash script with many comments and newlines. I could make a short and concise version by removing the comments and removing newlines. <pre><code>sed '/^#/d;/^$/d' verbose-script.sh &gt; short-script.sh\n</code></pre></p> <p>Say I have a very long file with authors and their relevant works and info, I could see on what lines \"Hubbard\" appears with <code>=</code>. This is very similar to <code>grep -n</code>. <pre><code>\u276f sed -n '/Hubbard/=' poems.csv \n2\n3\n\u276f grep -n 'Hubbard' 18.csv \n2:1771/1798/E. H. (Elihu Hubbard)/Smith/8/SONNET I./Sent to Miss  , with a Braid of Hair.\n3:1771/1798/E. H. (Elihu Hubbard)/Smith/8/SONNET II./Sent to Mrs.  , with a Song.\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#sort","title":"Sort","text":"<p><code>sort</code> command will sort alphabetically by the first column by default, and will separate columns by white space unless the separator is specified with  <code>-t</code>.</p> <p>Say I wanted to sort kernel modules by their size, I would numeric sort <code>-n</code> , then choose the second column/key with <code>-k 2</code>. <pre><code>lsmod | sort -n -k 2\nModule                  Size  Used by\nalgif_hash             16384  1\nalgif_skcipher         16384  1\ncmac                   16384  2\n</code></pre> Now say I want to sort the users in <code>/etc/passwd</code> based on their user ID in descending order, which is the third column seperated by colons. I can use <code>-t</code> for field seperator and <code>-k3</code> for the third column. For descending order I will use the reverse flag <code>-r</code> and make sure to numeric sort <code>-n</code>.  </p> <p><pre><code>sort -t: -rn -k3 /etc/passwd\nnobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin\njoseph:x:1000:1000:joseph,,,:/home/joseph:/bin/bash\nhplip:x:126:7:HPLIP system user,,,:/run/hplip:/bin/false\nfwupd-refresh:x:125:135:fwupd-refresh user,,,:/run/systemd:/usr/sbin/nologin\n</code></pre> Keep in mind this is a file we are reading so it can come after the command, the previous command <code>lsmod</code> was not a file, so I had to pipe the output into <code>sort</code>.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#file-metadata","title":"File Metadata","text":"<p>Inodes keep track of files and directories in Linux, they contain everything but the filename and contents. This includes permissions, timestamps, file size, and ownership. Find exact inodes with <code>ls -i</code>. <pre><code>ls -i\n18219914 file1  18221167 file2  18256138 file3\n</code></pre></p> <p>Two commands are used to check metadata, <code>file</code> and <code>stat</code>. File is much simpler and is often used to check the type of a file. Stat shows much more information, including file size, modification and access time, user and group ID, etc. This is read directly from the Inode. <pre><code>$ file obs.sh\nobs.sh: Bourne-Again shell script, ASCII text executable\n\n$ stat obs.sh\n  File: obs.sh\n  Size: 694         Blocks: 8          IO Block: 4096   regular file\nDevice: 802h/2050d  Inode: 18220124    Links: 2\nAccess: (0775/-rwxrwxr-x)  Uid: ( 1000/     joe)   Gid: ( 1000/     joe)\nAccess: 2023-12-09 12:58:05.139821191 -0600\nModify: 2023-10-15 11:13:26.272931896 -0500\nChange: 2023-12-09 12:58:02.799821244 -0600\n Birth: 2023-10-15 11:13:26.272931896 -0500\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#archiving-and-compressing","title":"Archiving and Compressing","text":"<p>Compressed files use less disk space and download faster over a network. On Windows, often files are downloaded in a zip format, which then are unzipped (decompressed). When installing software on Linux, file are often archived and compressed with a <code>tar.gz</code> ending. An archive is simply a collection of files and/or directories with <code>tar</code> (tape archive). The <code>gz</code> ending means its been compressed using <code>gzip</code>. </p> <p>As an example, I will extract, meaning take from an archive, and decompress the files in <code>nvim-linux64.tar.gz</code>. <code>x</code> is extract, <code>v</code> for verbose, <code>f</code> for file, and <code>C</code> to place it in a directory.  <pre><code>sudo tar -xvf nvim-linux64.tar.gz -C /usr/local/lib\n</code></pre> Note it is not necessary to mention the compression type, such as <code>z</code> for gzip, <code>J</code> for xz and so on. <code>tar</code> will automatically detect the compression type. These flags are only necessary when creating compressed archives.</p> <p>In all the compression tools, there is an direct relationship between CPU processing time (compression time) and compression ratio. What this means is there is a trade off. If you want the maximum amount of disk space saved, it will require more CPU processing power and time to compress. If you want to save on CPU processing or time, a lower compression ratio can be used, but will use more disk space.</p> <p>In general <code>xz</code> achieves the highest compression level, followed by <code>bzip2</code> and then <code>gzip</code>. In order to achieve better compression however <code>xz</code> usually takes the longest to complete, followed by <code>bzip2</code> and then <code>gzip</code>.</p> <p>Each program has similar syntax. With no arguments, they compress. <code>-d</code> will decompress. Each deletes the input file during compression or compression. Use <code>-k</code> to keep it, verbose <code>-v</code> to display compression ratio, and <code>-t</code> for integrity test on compressed files.   </p> <p>To demonstrate, I will create a 100 megabyte file, and compress using each. <code>dd</code> is a powerful but dangerous command, so be careful. <code>dev/zero</code> as the input file (<code>if</code>) is just an endless stream of null characters, which are ASCII code zero and signifies the end of a string (\\0). <pre><code>\u276f dd if=/dev/zero of=bigfile bs=1M  count=100 \n100+0 records in\n100+0 records out\n104857600 bytes (105 MB, 100 MiB) copied, 0.0834263 s, 1.3 GB/s\n\u276f xz -k bigfile; gzip -k bigfile; bzip2 -k bigfile\n\u276f ls -lh\ntotal 101M\n-rw-rw-r-- 1 promptier promptier 100M Dec 23 10:11 bigfile\n-rw-rw-r-- 1 promptier promptier  113 Dec 23 10:11 bigfile.bz2\n-rw-rw-r-- 1 promptier promptier 100K Dec 23 10:11 bigfile.gz\n-rw-rw-r-- 1 promptier promptier  16K Dec 23 10:11 bigfile.xz\n</code></pre> Comparing the sizes, gzip shrunk it down to 100K, while xz came in at 16K. bzip2 came at 113 bytes, which I am unsure how.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#backup-with-dd","title":"Backup with dd","text":"<p><code>dd</code> is a tool that will copy everything from a file, bit by bit, in the same order. It is known by many names, including \"disk dump\", \"disk destroyer\" and \"(d)on't (d)o it\". It can be dangerous if you don't know what you are doing. Specify your input file <code>if</code> and output file <code>of</code>. The following are some common uses.</p> <p>Create backup image file of a drive then restore to another drive. <pre><code>dd if=/dev/sda of=~/backup.img\ndd if=~/backup.img of=/dev/sdb\n</code></pre> You could also copy directly to another drive if desired.</p> <p>We can also use special character devices to zero out/wipe a disk or add a layer of encryption. <pre><code>dd if=/dev/zero of=/dev/sda\ndd if=/dev/urandom of=/dev/sdb\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#text-editors","title":"Text Editors","text":"<p>Though I will not provide an entire guide of either Nano or Vim, I will touch on them briefly and give my advice on which you should spend your time using. </p> <p>Nano is easy to use and will perform the job just as well as Vim. However, if you plan to spend a lot of time at the terminal, learning <code>vim</code> will initially set you back in term of speed productivity, but will be worth the ROI in one to two weeks of use. </p> <p>Vim movements are not just confined to the editor. Try opening up a man page and you will find the same navigation keys such as <code>k</code> and <code>j</code> for up and down, <code>gg</code> and <code>G</code> to move to the top and bottom of the file, and <code>/</code> to search. </p> <p><code>less</code> is a program that displays the contents of a file one page/screen at a time, as opposed to using <code>cat</code> and then viewing the contents through stdout. Similar to man pages, less also has vim like navigation.  </p> <p>Overall, learning vim will not just help you while using the editor, but provide universal navigation across many programs on Unix-like systems. Additionally, once you have the Vim movements to muscle memory, you can bring these keybindings to any text editor or note taking app. Both Vscode and Obsidian are two examples which support vim, either nativity or through third party extension, this makes Vim a valuable skill to acquire for Linux and beyond. </p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.2%20Manage%20Files%20and%20Directories/#rsync","title":"Rsync","text":"<p>Rsync (Remote Synchronization) was an algorithm presented in a PhD thesis for sorting and syncing for remote data update. In the first phase, a client and server is defined, the client initiates and connects to the server either locally, via remote shell or via network socket. After connections is established, the client and server assume the roles of sender and receiver respectively. Rsync Takes into account the file path, ownership, mode (read, write, etc.), permissions, size, modification time, and an optional checksum. Primarily, rsync determines which files differ between the sender and receiver by comparing the modification time and size of each file.</p> <p>The general use is to copy contents of <code>dir1</code> to <code>dir2</code>. Keep in mind if a file already exists in <code>dir2</code>, it will simply ignore it, unless there is a more recent modification time, in which case it will be updated, hence the nature of syncing. <pre><code>~$ rsync -r dir1/ dir2\n</code></pre></p> <p>Instead of recursive <code>-r</code>, many use archive <code>-a</code>, which preserves links, timestamps, permission, etc. as well as recursively copies. Run <code>-v</code> for verbose. </p> <p>If copying a large number of files to a remote server, it is recommended to dry run first to simulate the command without actually running it to catch errors before they cause problems. <pre><code>rsync -av --dry-run send-dir joe@8.8.8.212:/home/recieve-dir\n</code></pre> Here was me backing up my downloads folder to an external USB. <pre><code>sudo rsync -av /home/promptier/downloads/ /media/promptier/USB/downloads/\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.3%20Storage/","title":"1.3 Storage","text":"<p>Shawn Powers Links Archiving &amp; Compressing (tar, gzip, cpio) Copying Between Networks (scp, rsync, nc)</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.3%20Storage/#systemd-and-fstab-file-system-mounting","title":"Systemd and fstab File System Mounting","text":"<p>The traditional way for auto-mounting filesystems is <code>/etc/fstab</code>. However, a more modern method with more flexibility is to create systemd unit files in <code>etc/systemd/system</code>. This allows more granular control over each mount point and allows dependency management between units, so you can control the order of mounting. It is also the preferred method for more complex and network-mounted filesystems.</p> <p>It is worth noting that systemd automatically generates <code>.mount</code> and <code>.automount</code> files from  <code>/etc/fstab</code> anyways. This simplifies the process of managing mounts, as you don't need to manually create systemd unit files for each mount point. Instead, you can continue to use the familiar <code>/etc/fstab</code> syntax and systemd handles the unit generation behind the scenes.</p> <p>Overall, <code>/etc/fstab</code> is an older and traditional way of configuring file system mounts in Linux, which systemd is still compatible with. Both methods are popular with <code>etc/fstab</code> preferred for simple mounts and manual systemd unit files preferred for more complex configurations.  </p> <p>Both are similar in displaying, what to mount, where to mount it, the type of filesystem, and any additional options.</p> <p><code>/etc/fstab</code> <pre><code># &lt;file system&gt; &lt;mount point&gt;   &lt;type&gt;  &lt;options&gt;       &lt;dump&gt;  &lt;pass&gt;\nUUID=df155a8b-6b89 /               ext4    errors=remount-ro 0       1\nUUID=E89D-9509  /boot/efi       vfat    umask=0077      0       1\n/swapfile                                 none            swap\n</code></pre></p> <p><code>/etc/systemd/system/mnt-data.mount</code> <pre><code>[Unit]\nDescription=Data mount\n\n[Mount]\nWhat=/dev/disk/by-uuid/filesystem_UUID\nWhere=/mnt/data\nType=xfs\nOptions=defaults\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.3%20Storage/#luks-cryptsetup","title":"LUKS Cryptsetup","text":"<p>Device Mapper is what maps physical block devices to high level virtual block devices. It is what underlies LVM, Raid Software, and Cryptsetup.</p> <p>Cryptsetup is the standard for encrypting disks on Linux. To experiment, I have created a file and attached it to a loop device using <code>fallocate</code> and <code>losetup</code>. This will be our psuedo block device which we will encrypt. <pre><code>\u276f sudo cryptsetup luksFormat /dev/loop0\n\nWARNING!\n========\nThis will overwrite data on /dev/loop0 irrevocably.\n\nAre you sure? (Type 'yes' in capital letters): YES\nEnter passphrase for /home/promptier/disk.img: \nVerify passphrase: \n</code></pre></p> <p>Now open it up using the passphrase.  <pre><code>\u276f sudo cryptsetup luksOpen /dev/loop0 maptest\n</code></pre></p> <pre><code>\u276f ls -l /dev/mapper/maptest \nlrwxrwxrwx 1 root root 7 Dec 28 17:45 /dev/mapper/maptest -&gt; ../dm-0\n</code></pre> <p>Watch Luke Smith encrypt his USB drives.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.3%20Storage/#lvm","title":"LVM","text":"<p>3 Components 1. Physical Volumes (pvs) 2. Volume Groups (vgs) 3. Logical Volumes (lsv)</p> <p>2 have counterparts to traditional disk partitioning</p> Disk Partitioning System LVM Partitions Logical Volumes Disks Volume Groups <p>First you create physical volumes <pre><code>\u276f sudo pvcreate /dev/sda1\n  Physical volume \"/dev/sda1\" successfully created.\n\u276f sudo pvs\n  PV         VG Fmt  Attr PSize   PFree  \n  /dev/sda1     lvm2 ---  &lt;28.91g &lt;28.91g\n</code></pre></p> <p>Now volume groups are collections of physical volumes.</p> <p>To add or remove physical volumes from logical volumes, use extend and reduce. <pre><code>vgextend &lt;volume_group&gt; &lt;physical_volume1&gt; &lt;physical_volume2&gt; ....\n\nvgreduce &lt;volume_groupe&gt; &lt;physical_volume1&gt; &lt;physical_volume2&gt;\n</code></pre></p> <p>Finally, logical volumes are created from volume groups.</p> <pre><code>sudo lvcreate -L &lt;size&gt; -n &lt;lvname&gt; &lt;vgname&gt;\n</code></pre> <p>physical volumes &gt; volume groups &gt; logical volumes</p> <ol> <li>create physical volumes from multiple devices <pre><code>sudo pvcreate /dev/sdc /dev/sdd\n</code></pre></li> <li>next create a volume group using multiple physical volumes <pre><code>sudo vgcreate lvm_tutorial /dev/sdc /dev/sdd\n</code></pre></li> <li>finally create a logical volume from a volume group <pre><code>sudo lvcreate -L 5GB -n my_lvm lvm_tutorial\n</code></pre></li> </ol> <p>![[lvm.png]]</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.3%20Storage/#filesystems","title":"Filesystems","text":"<p>DJWare Benchmarks and Comparison video</p> <p>Problems that were solved at Facebook data-centers by switching to BTRFS in this article. BTRFS provided efficient container isolation, reducing IO and CPU expenses, and enabling advanced features like snapshotting and transparent file compression.</p> <p>Though the upstream Fedora Project uses BTRFS, REHL 9 uses XFS by default. This Reddit thread is very informative, which a Lead Engineer at Red Hat responds.</p> <p>Recent thread in Red Hat forum concerning filesystem usage in Red Hat and Suse.</p> <p>OpenSuse and SLES typically use Btrfs as the default file system for the root partition due to its advanced features like snapshots, rollbacks, and data deduplication. For other partitions, such as <code>/home</code> or data partitions, XFS is commonly recommended due to its stability and performance, especially when dealing with large files. This approach combines the advantages of both file systems: Btrfs for system file management and rollback capabilities, and XFS for high-performance data storage. So in general, various filesystem types can be used for different partitions depending on the requirements.</p> <p>Debian and Ubuntu use EXT4 as their default filesystem.</p> <p>EXT4 uses journaling. This means information about changes is recorded in a separate log (the journal) before the indexes to the files are updated. After a crash, recovery simply involves reading the journal from the file system and replaying changes from this journal until the file system is consistent again. You normally cannot access the journal directly though you can find information using <code>debugfs</code> and <code>dumpe2fs</code>.</p> <p>BTRFS has a copy-on-write system, where changes to files create new blocks of data and preserve the older version instead of overwriting in place. A snapshot is basically a copy (of the differences) of a subvolume. <code>sudo btrfs subvolume create demo</code>, add some files, then <code>sudo btrfs subvolume snapshot demo demo-1</code>. Keep in mind snapshots are not backup, rather old versions you can rollback to.</p> <p>From article</p> <p>When data is overwritten in an ext4 filesystem, the new data is written on top of the existing data on the storage device, destroying the old copy. Btrfs, instead, will move overwritten blocks elsewhere in the filesystem and write the new data there, leaving the older copy of the data in place.</p> <p>The COW mode of operation brings some significant advantages. Since old data is not overwritten, recovery from crashes and power failures should be more straightforward; if a transaction has not completed, the previous state of the data (and metadata) will be where it always was. So, among other things, a COW filesystem does not need to implement a separate journal to provide crash resistance.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.3%20Storage/#sans","title":"SANs","text":"<p>Multipath is what is used to direct and manage I/O channels in storage area networks (SAN). Having many blocks that can be read or written to at any moment, multipath handles load balancing, as well as path fail-over and recover, which redirects I/O channels when one or more paths are not available. Note that multipathing protects against the failure of path(s) and not the failure of a specific storage device. Read more here.</p> <p>CIFS (common Internet File System)is the older SMB version 1. SMB (Server Message Block) is a large umbrella term for Proprietary Windows file shares. Andrew Tridgell, as a PhD student who also co-authored the Rsync algorithm, essentially reverse engineered SMB using network packet sniffers to understand it's protocol. He then created SAMBA, an open source alternative to SMB which allows interoperability between Linux and Windows.  </p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/","title":"1.4 Processes and Services","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/#systemctl","title":"Systemctl","text":"<p>The <code>systemctl</code> program inspects and manages the<code>systemd</code> system and service manager. Systemd runs as your your very first process (PID 1) on boot, which spawns all other processes. </p> <p><code>systemd status</code> will display all ongoing systemd services which can be narrowed down with <code>{service-name}</code>, such as the following.</p> <pre><code>~$ systemctl status cron\n\u25cf cron.service - Regular background program processing daemon\n     Loaded: loaded (/lib/systemd/system/cron.service; enabled; vendor preset: enabled)\n     Active: active (running) since Mon 2023-11-13 07:13:26 CST; 2h 31min ago\n</code></pre> <p>Each of these services are contained within control groups (cgroups) to isolates resource usage of a collection of processes. Very similar to <code>systemd status</code>, <code>systemd-cgls</code> will list all control groups in a more concise format. <code>systemd-cgtop</code> will list control groups by resource usage, similar to <code>top</code>.</p> <p>To manage these services, use <code>systemctl enable</code> and  <code>disable</code> for automatic starting or preventing the service at boot. Use <code>start</code> and <code>stop</code> to do this manually for immediate action. A more drastic action is <code>mask</code>, which prevents the service from starting on boot and manually starting by creating a symlink to <code>/dev/null</code>, this effectively discards any attempt to start the unit. As an example, if you have a legacy service (<code>old-service.service</code>) that should never be started again and should be completely removed from the system. Masking ensures that even if someone tries to start it manually, systemd will ignore it.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/#process-management","title":"Process Management","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/#kill-signals","title":"Kill Signals","text":"<p>You can  list all possible kill signals with <code>kill -l</code>, here are four common ones with there name and associated code to be called with <code>kill</code>.</p> <p>SIGHUP - 1 Originating from the concept of a \"hangup\", or when a something like a modem disconnects from a console, this signal now mostly refers to a program losing connection to the controlling terminal (tty).  Any program started in a given terminal will receive a SIGHUP when the terminal is closed. Besides terminals, daemons (being detached from the terminal) use this signal by convention to reread config files.</p> <p>SIGINT - 2 Signal interrupt is what occurs when <code>ctrl + c</code> is pressed during a running process. It is essentially a interruption request sent by the user.</p> <p>SIGKILL - 9 Instructs the process to terminate immediately. It cannot be ignored or blocked. This is the most powerful signal and can have unintended consequences.   </p> <p>SIGTERM - 15 Signal terminate tries to kill the process, but can be blocked or handled in various ways. It is a more gentle way to kill a process.</p> <p>SIGCONT - 18 Continue a process after having been paused with SIGSTOP</p> <p>SIGSTOP - 19  This pauses a foreground process, which is one that controls the tty and is currently being displayed in the terminal. <code>ctrl + z</code> will trigger this signal and bring the process the the backround. The process can be brought back the the foreground with <code>fg</code>. </p> <p>Let's run a sleep command to start a running process in the foreground, pause it, find it's PID, then kill it. Keep in mind <code>kill -9</code> is equivelent to <code>kill -SIGKILL</code> and so forth.</p> <pre><code>\u276f sleep 200\n^Z\n[1]+  Stopped                 sleep 200\n\n~ took 3s \n\u2726 \u276f ps\n    PID TTY          TIME CMD\n   8305 pts/0    00:00:00 bash\n  11615 pts/0    00:00:00 sleep\n  11645 pts/0    00:00:00 ps\n\n~ \n\u2726 \u276f kill -9 11615\n</code></pre> <p>Alternatively, we could have induced <code>SIGINT</code> with <code>ctrl + c</code> to interupt the process, or brought back the backround process to our terminal with <code>fg</code>.</p> <p>Commands to find PIDs by name include <code>pgrep</code> and <code>pidof</code>. They are similar except pidof only returns the exact match of the name, while pgrep includes partial matches. </p> <p>In this example, pidof returns only the exact match called <code>systemd</code>, while pgrep returns programs including <code>systemd-journal</code>, <code>systemd-logind</code>, etc. <pre><code>\u276f pidof systemd\n1779\n\n\u276f pgrep systemd\n1\n460\n495\n892\n1779\n</code></pre></p> <p>The the <code>kill</code> program requires the exact PID, there are two programs which take names as arguments. Comparable to pgrep and pidof, <code>pkill</code> and <code>killall</code> work with partial names and exact names respectively. In this example, pkill will forcibly kill all programs with matching \"obsidian\" including partial matches. killall will only kill programs with the exact name of \"obsidian\". <pre><code>\u276f pkill -9 obsidian\n\u276f killall -9 obsidian\n</code></pre></p> <p>Here I pause (SIGSTOP) and resume (SIGCONT) my file manager, Nemo, on Linux Mint. After I pause it, the application's window is basically frozen, then I resume it.  <pre><code>\u276f pkill -19 nemo\n\n\u276f pkill -18 nemo\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/#process-states","title":"Process States","text":"Process State PS Representation Running R Idle I Stopped T Sleeping(Int) S Sleeping(UnInt) D Zombie Z <p>These process states can be seen with <code>ps</code> and <code>top</code>. Processes that are stopped can be though of as paused, this is what happens with <code>ctrl + c</code> sending the SIGSTOP signal, which can be resumed with SIGCONT. Most processes seen with <code>top</code> are either running (S), idle (I).</p> <p>Interruptible sleep (S) is sensitive and waiting for user space signals. Uninterruptible sleep (D) waits for a specific system call (usually disk I/O) and cannot be interrupted or killed until the call is complete.</p> <p>Zombies (Z) are dead processes whose parent has not destroyed them properly. They do not use any resources but can pollute output in <code>ps</code> and <code>top</code> and should be cleaned up.</p> <p><code>jobs</code> views and manages background jobs within the current shell session. <code>sleep</code> is an easy command to experiment with. After sleeping for a set amount of time, the process can be paused (SIGSTOP) with <code>ctrl + z</code> or immediately brought to the backround with <code>&amp;</code>. <pre><code>~$ sleep 10\n^Z\n[1]+  Stopped                 sleep 10\n\n~$ sleep 20 &amp;\n[2] 3967600\n</code></pre> View current jobs with <code>jobs</code> and bring one back to the foreground with <code>fg [id]</code>. By default <code>fg</code> will run the job marked with a <code>+</code>. <pre><code>~$ jobs\n[1]-  Stopped                 sleep 10\n[2]+  Stopped                 sleep 20\n\n~$ fg 1\nsleep 10\n</code></pre></p> <p><code>ps</code> is a more general process monitoring tool for the whole system. Instead of viewing jobs only in the terminal session, you can detach from the terminal with the <code>x</code> flag and specify options/columns with <code>-o</code>. Here I only care about the PID and state.</p> <pre><code>~$ ps -x -o pid,state\n    PID S\n   1259 S\n   1260 S\n   1267 S\n</code></pre>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/#nice-and-renice","title":"Nice and Renice","text":"<p>Nice is intended for batch or background jobs. Jobs are niced (given lower scheduling priority) so they don't use CPU when online users need it, and so foreground tasks have ample performance.</p> <p>Niceness value refers to CPU priority of userspace programs on a scale of -20 to 20, lower being higher priority. Use <code>nice</code> to launch a program with a specific niceness value, and <code>renice</code> to alter the priority of a currently running process. </p> <p>Here I launch my terminal with a relatively low priority of 10. Then I alter it's nice value to an even lower priority. </p> <pre><code>\u276f nice -n 10 gnome-terminal\n\u276f renice -n 15 9830\n</code></pre> <p>Keep in mind in order launch a program with high priority (below 0), you need root privilege. In the same manner, root privileged is also needed to increase a current running program's priority. </p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/#at","title":"At","text":"<p>The <code>at</code> command is for simple on time jobs. </p> <p><pre><code>at now + 1 minute \n&gt;echo \"HELLO\" &gt;&gt; /tmp/from-at\n</code></pre> <code>atq</code> and <code>atrm</code> to list and remove jobs.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/#top","title":"top","text":"<p>Top is the default resource moniter preinstalled on nearly all Linux systems. You can think of it as a lightweight, terminal based Task Manager from Windows. Before jumping into it, you can list the running processes manually as well as find uptime and load average with simpler commands.</p> <p>The <code>ps</code> command by default will list out procceses ran in the terminal. The <code>x</code> flag lifts the restriction of the terminal and displays all ongoing processes in the system. The <code>u</code> flag adds a user column.</p> <pre><code>\u276f ps aux\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot           1  0.0  0.0 166648 12088 ?        Ss   07:23   0:01 /sbin/init splash\nroot           2  0.0  0.0      0     0 ?        S    07:23   0:00 [kthreadd]\nroot           3  0.0  0.0      0     0 ?        I&lt;   07:23   0:00 [rcu_gp]\n</code></pre> <p>We can also find uptime and load average with <code>uptime</code> or <code>cat /proc/loadavg</code>. These are explained below. <pre><code>\u276f uptime\n 10:05:42 up  2:42,  1 user,  load average: 1.51, 1.13, 0.92\n\n~ \n\u276f cat /proc/loadavg \n1.46 1.12 0.92 1/1319 13289\n</code></pre></p> <p>For a more dynamic listing of this data, we can run <code>top</code>. You can think of this which as essentially running commands like <code>ps aux</code>, <code>uptime</code>, <code>free</code> for memory, and additional commands every three seconds, to actively moniter the system.</p> <p>![[Linux/Linux+/1. 0 System Management/top.png]]</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/#statistics","title":"Statistics","text":"<p>Starting with the statistics at the top, starting from the left, you can see the up time, which is how long the system has been running. Moving right, the number of users is displayed, then we have load average, which is average CPU usage in the past 1 minute, 5 minutes, and 15 minutes. It is  based on your number of cores. For example if you have 4 cores, then a 4.0 would mean 100% CPU usage, in this screenshot, I have 2 cores so in the past 1 minute I had 63% usage of my 2 cores.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/#process-table","title":"Process Table","text":"<p>Now, let's go over each column in the process table.</p> <ul> <li>PID: Process ID.</li> <li>USER: The user running the process.</li> <li>PR: Priority of the task computed by the kernel on a scale of 20 to -20.</li> <li>NI: \"Niceness\" value, which involves the priority of user processes. 0 is the default and highest priority.</li> <li>VIRT: Virtual and physical memory, representing the total memory space allocated to a process, including RAM and swap. It's like a hypothetical maximum needed by the program. RES + Swap space = VIRT</li> <li>RES: Resident (Physical) memory used by the process. VIRT - Swap space = RES</li> <li>SHR: Shared memory.</li> <li>S: State of the process, where \"R\" means running, \"S\" means sleeping, and \"I\" is idle.</li> </ul> <p>To only display columns that matter to you, you can press <code>f</code> while in top for field management. In this case I only care about process state, cpu, memory, how long the process has been running, and the command name.</p> <p>Navigate with the arrows and select fields with the spacebar. ![[field-management.png]] Now I have a much more minimal process table with only the fields I care about. ![[Linux/Linux+/1. 0 System Management/top.png]]</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.4%20Processes%20and%20Services/#htop","title":"htop","text":"<p>The most popular alternative, <code>htop</code>, provides more customization, scrolling and mouse support, color, and an overall cleaner interface. Unlike <code>top</code>, it does not come preinstalled but is worth checking out with a quick download. <pre><code>sudo apt install htop\n</code></pre> I recommend to learn <code>top</code> first and then try out <code>htop</code>, comparing the two. </p> <p>Upon running <code>htop</code>, you will first realize the displayed columns discussed above are exactly the same, as well as the tasks, load average, and up time at the top right. The main difference is the colorful, more readable TUI (text user interface) that supports mouse events and scrolling. An example of this is the CPU column, colored blue after clicking, ordering the processes by CPU consumption. </p> <p>![[htop.jpg]]</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.5%20Networking/","title":"1.5 Networking","text":"<p>Networking tools in Linux vary significantly across distributions.</p> <p>The newer iproute2 package includes <code>ss</code> and <code>ip</code> which largely replace commands such as <code>ifconfig</code>, <code>route</code>, and <code>netstat</code>.</p> <p>On the man page of the netstat command, you can read: <pre><code>This program is mostly obsolete.  Replacement for netstat is ss. Replacement for  netstat -r  is ip route.  Replacement for netstat  i is ip -s link.  Replacement for netstat -g is ip maddr.\n</code></pre></p> <p>Like the man page says, the <code>ss</code> command has replaced the <code>netstat</code> command. Other deprecated programs in the net-tools package include:</p> <p><code>route</code> and <code>netstat -r</code> that have been replaced with <code>ip route</code></p> <p><code>arp</code> which have been replaced by <code>ip neigh</code> or neighbor.</p> <p>Some useful terminology to learn across these programs:</p> <p>inet - internet protocol family (IPv4) inet6 - the modern protocol of IP addresses represented in hexidecimal lo - virtual loop back device/interface for troubleshooting <code>ifconfig lo</code> wlo1 - wireless network interface (NIC) RX - Receive TX - Transmit</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.5%20Networking/#network-monitoring","title":"Network Monitoring","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.5%20Networking/#tshark","title":"Tshark","text":"<p><code>-i</code> for interface<code>-f</code> for capture filter <pre><code>\u276f sudo tshark -i wlo1 -f \"src port 443\"\n</code></pre> You can also read <code>-r</code> from and write <code>-w</code> to a file.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.5%20Networking/#tcpdump","title":"tcpdump","text":"<p>Similar to tshark, <code>tcpdump</code> can also analyze packets on a network.</p> <p>List interfaces with <code>-D</code>. <pre><code>\u276f tcpdump -D\n1.wlo1 [Up, Running, Wireless, Associated]\n2.any (Pseudo-device that captures on all interfaces) [Up, Running]\n3.lo [Up, Running, Loopback]\n4.enp2s0 [Up, Disconnected]\n5.virbr0 [Up, Disconnected]\n6.docker0 [Up, Disconnected]\n</code></pre></p> <p>The syntax is very similar to tshark <pre><code>\u276f sudo tcpdump -i wlo1 port 443 -w tcpdump.pcap\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.5%20Networking/#my-traceroute-mtr","title":"My Traceroute mtr","text":"<p>Article on <code>mtr</code> command. Combines <code>ping</code> and <code>traceroute</code> functionality.</p> <p><code>-r</code> report mode sends 10 packets in the background and write to sdout. Use <code>-c</code> to cycle the number of packets.</p> <pre><code>\u276f mtr -r -c 15 google.com &gt; report.txt\n</code></pre> <p>mtr uses ICMP echos by default, but you can use UDP with <code>-u</code> and TCP SYN packets with <code>-T</code>.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.5%20Networking/#netcat-nc","title":"Netcat (nc)","text":"<p>nc is known as the swiss army knife of networking. It is very versatile. Check if a port is open with <code>-zv</code>. <code>-z</code> scans for listening daemons without sending data or establishing a connection, <code>-v</code> is verbose.</p> <p>I will now listen on one machine with with listen <code>-l</code> , verbose<code>-v</code> and keeping the connection open <code>-k</code>. MACHINE 1 <pre><code>nc -lvk 6969\nNcat: Version 7.93 ( https://nmap.org/ncat )\nNcat: Listening on :::6969\nNcat: Listening on 0.0.0.0:6969\n</code></pre> After already finding the ip address with <code>ip a</code>, I can use the <code>-v</code> flag to check if the port is open. MACHINE 2 <pre><code>nc -zv 192.168.122.46 6969\nConnection to 192.168.122.46 6969 port [tcp/*] succeeded!\n</code></pre> We can see the output in on the listening machine. MACHINE 1 <pre><code>Ncat: Connection from 192.168.122.1.\nNcat: Connection from 192.168.122.1:55438.\n</code></pre></p> <p>To send a file over a network, on the listening machine, run <code>nc -l [port] &gt; file.txt</code> and on the sender <code>nc [ip] [port] &lt; file.txt</code></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.5%20Networking/#checking-open-ports","title":"Checking Open Ports","text":"<p>You want set up a server for SSH, the first step is making sure port 22 is open. Here are the various ways to check. <pre><code>ss -tuln | grep ':22'\ntelnet localhost 22\nsudo lsof -iTCP #look for a name with ssh\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.5%20Networking/#firewalls","title":"Firewalls","text":"<p>Let's say telnet refuses connection and the other commands do not find port 22, this could be UFW (Uncomplicated Firewall) is blocking it. UFW basically a wrapper over <code>iptables</code> for Ubuntu. <code>sudo ufw status</code> will let you know if it is enabled or not, and what ports it allows or blocks.</p> <p>UFW will either use <code>iptables</code> or <code>nftables</code> by default. Here we can see nf_tables is used, if it wasn't, it would say \"legacy\". <pre><code>iptables -V\niptables v1.8.7 (nf_tables)\n</code></pre> iptables is old and nftables if preferred.</p> <p>To see if you are running the old kernel module behind iptables, run <code>modinfo br_netfilter</code>. \"br\" stands for bridge, because it acts at the layer two (OSI Model) link layer. To see if you are using nftables, grep for \"nf\" or \"nft\": <code>lsmod | grep -E \"(nf|nft)\"</code>. Check systemd if you are running firewalld: <code>systemctl status firewalld.service</code> </p> <p>iptables came out in 2001 and has some problems, such as: - firewall rules are static - changes require a complete firewall restart -  These firewall restarts break stateful firewalling  </p> <p>nftables is the successor to iptables that allows for more flexible, scalable and performance packet classification. It is developed by the Netfilter project. It has been available since 2014 but only had it's first stable release in 2023.</p> <p>Firewalld is newer and came out in 2011. It is developed by Red Hat</p> <p>firewalld New Features - Dynamically managed firewall - Supports network zones with trust levels - Applies changes without restarting the entire firewall - No need to reload firewall kernel modules</p> <p>firewalld Service - Monitors the firewall - Provides information via D-Bus - Accepts changes via D-Bus using policykit</p> <p>These are the predefined/default firewalld Zones that you can learn more about by running <code>firewall-cmd --list-all-zones</code>, <code>man firewalld.zones</code>, or reading the documentation - dmz - external - home - internal - public - trusted - work</p> <p>firewalld vs. iptables - firewall-cmd --zone=public --permanent --add-service=ssh - iptables -A INPUT -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT</p> <p><code>ufw</code> and <code>firewall-cmd</code> are just front ends to nftables (formerly iptables) the following which you can see  <code>sudo nft list ruleset</code> <code>sudo iptables -S</code></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.5%20Networking/#dig-nslookup-hostnamectl-resolvectl","title":"dig, nslookup, hostnamectl, resolvectl","text":"<p>nslookup (nameserver) queries DNS servers translating domain names to machine readable IP addresses, and vice versa.  dig (domain information groper) is just like nslookup but with additional features. </p> <p>If you ever see your nameserver as <code>127.0.0.53</code> in <code>/etc/resolve.conf</code> that just means you are using systemd-resolved.service as the local DNS resolver.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/","title":"1.6 Package Management and Sandboxed Apps","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/#package-management","title":"Package Management","text":"<p>In the old days of Linux in the 90s, installing packages was only possible with tools like <code>dpkg</code> (Debian Package) and <code>rpm</code> (Red Hat Package Manager), which each install a single binary at a time. This meant you had to hunt down each remaining <code>.deb</code> or <code>.rpm</code> package dependency yourself. These would now be considered low-level package managers, since the creation of tools like <code>apt</code> (Advanced Package Manger) on Debian and Ubuntu, as well as <code>yum</code> and <code>dnf</code> on Red Hat. As high-level package managers, they will automatically resolve dependencies, often installing many packages at once for a single program. Additionally, these tools provide automatic updates and generally simplify the software installation process.</p> <p>These tools work by reading from a public database of packages called repositories. We can see the number of packages in the Apt repository with the word count program to count each line/package. As you can see, we have over 80 thousand packages to choose from. <pre><code>\u276f apt list | wc -l\n81413\n</code></pre> This package management system is not just for Linux Administration, but for software development across programming languages as well. Javascript has the Node Package Manager (NPM) which is a cornerstone for web development, while  Python has PIP (Package Installer for Python), which grabs packages from the Python Package Index (PyPi).</p> <p>Returning to Debian and Red Hat package managers, both <code>dpkg</code> and <code>rpm</code> have the same <code>-i</code> flag for installing. <pre><code>\u276f dpkg -i package.deb\nOR\n\u276f rpm -i package.rpm\n</code></pre></p> <p><code>yum</code> is the older package manager for Red Hat, with <code>dnf</code> (Dandified Yum) being the modern tool which includes more features such as rollback and undo.</p> <p>Here is a great article about the history of package management.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/#sandboxed-apps","title":"Sandboxed Apps","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/#appimages","title":"Appimages","text":"<p>Appimages are an entire app and all dependencies contained within a single file. This means there is no intermediary between the author and the user, such as required runtimes for Flatpacks and Snaps They are simple, distribution agnostic and do not require root. This is because appimages use FUSE (File System in Userspace), a system which allows non-root users to mount filesystems.</p> <p>To run an appimage, simply download an app from Appimagehub, give executable permissions with <code>chmod +x</code> and run.</p> <p>In this example, going to download Gun Mayhem 2.</p> <p>![[gun-mayhem.png]] Once it's in my downloads folder, I'll give it executable permissions and play! <pre><code>\u276f chmod +x Gun-Mayhem-2-x86-64.AppImage \n\u276f ./Gun-Mayhem-2-x86-64.AppImage \n</code></pre> ![[gun-mayhem-gameplay.png]] Now while the game is running, let's look at the temporary filesystem created with FUSE that the game is running on. We can see it is in a read-only (ro) mode for execution, ensuring isolation and security during its runtime.  <pre><code>\u276f mount | grep Gun-Mayhem\n/home/promptier/Downloads/Gun-Mayhem-2-x86-64.AppImage on /tmp/.mount_Gun-MaPpsVle type fuse.Gun-Mayhem-2-x86-64.AppImage (ro,nosuid,nodev,relatime,user_id=1000,group_id=1000)\n</code></pre> You can also find where it's mounted using <code>findmnt</code>. To see the contents of the game while it's running, use <code>tree /tmp/.</code> All of these files necessary to run the program in there own temporary file system all were easily created from running a single executable, that is the convenience of appimages. <pre><code>\u276f tree /tmp/.mount_Gun-Ma6jK4U3/\n/tmp/.mount_Gun-Ma6jK4U3/\n\u251c\u2500\u2500 AppRun\n\u251c\u2500\u2500 Gun_Mayhem_2.desktop\n\u251c\u2500\u2500 Gun_Mayhem_2.png\n\u2514\u2500\u2500 usr\n    \u251c\u2500\u2500 bin\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 ruffle\n    \u251c\u2500\u2500 game\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 Gun_Mayhem_2.swf\n    \u251c\u2500\u2500 lib\n    \u2514\u2500\u2500 share\n        \u2514\u2500\u2500 pixmaps\n            \u2514\u2500\u2500 Gun_Mayhem_2.png\n</code></pre></p> <p>Once you close the AppImage application, this temporary filesystem will be unmounted automatically and you can see the event in <code>/var/log/syslog</code> or the systemd journal. <pre><code>\u276f grep mount_Gun /var/log/syslog\n#OR\n\u276f journalctl --since today | grep mount_Gun\nDec 28 08:14:55 ideapad3 systemd[1]: tmp-.mount_Gun\\x2dMa6jK4U3.mount: Deactivated successfully.\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/#flatpaks","title":"Flatpaks","text":"<p>Flatpaks isolate applications in a 'sandbox' by providing separate runtimes/platforms that are distribution agnostic. On my Linux Mint system, I run many applications in flatpaks for the sandboxed security and automatic updates they provide. List running flatpaks <code>ps</code>. Take note of my Obsidian note taking app and Brave web browser. </p> <p><pre><code>\u276f flatpak ps\nInstance   PID  Application           Runtime\n169358187  6282 com.brave.Browser     org.freedesktop.Platform\n2422727375 6242 com.brave.Browser     org.freedesktop.Platform\n2148086033 3309 md.obsidian.Obsidian  org.freedesktop.Platform\n3377083729 3268 md.obsidian.Obsidian  org.freedesktop.Platform\n739023738  2990 com.obsproject.Studio org.kde.Platform\n</code></pre>  To find more information about a particular application, use <code>info</code>.  ```  \u276f flatpak info com.brave.Browser </p> <p>Brave Browser - The web browser from Brave</p> <pre><code>      ID: com.brave.Browser\n     Ref: app/com.brave.Browser/x86_64/stable\n    Arch: x86_64\n  Branch: stable\n Version: 1.60.114\n License: MPL-2.0\n  Origin: flathub\n</code></pre> <p>Collection: org.flathub.Stable Installation: system    Installed: 368.6\u00a0MB      Runtime: org.freedesktop.Platform/x86_64/22.08          Sdk: org.freedesktop.Sdk/x86_64/22.08</p> <p><pre><code>Whether an app is currently running or not, I can list all installed flatpaks with `list`. \n</code></pre> \u276f flatpak list Name                    Application ID                  Version           Branch      Installation Brave Browser           com.brave.Browser               1.60.114          stable      system Discord                 com.discordapp.Discord          0.0.35            stable      system OBS Studio              com.obsproject.Studio           30.0.0            stable      system Visual Studio Code      com.visualstudio.code           1.84.1-1699275408 stable      system Obsidian                md.obsidian.Obsidian            1.4.16            stable      system ```</p> <p>Flathub is the official distribution service for Flatpaks and provide easy installation of your favorite apps. In recent years, Flatpak has become a standard method of distributing software on Linux desktops</p> <p>![[flathub.png]] Besides the Linux desktop, Flatpaks are are often used for distributing applications on servers as well.</p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.6%20Package%20Management%20and%20Sandboxed%20Apps/#snaps","title":"Snaps","text":"<p>Developed by Canonical as the standard software packaging and deployment method on Ubuntu, Snaps are comparable to Flatpaks as self contained, sandboxed apps. Like Flathub, the Snapstore is the official  distribution service for snap packages.  ![[snapstore.png]] Though very popular now on Ubuntu desktop, Snap packages are primarily to distribute software across servers running Ubuntu. </p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/","title":"1.7 Manage Software Configurations","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#configure-kernel-options","title":"Configure Kernel Options","text":""},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#kernel-modules-vs-drivers","title":"Kernel Modules vs Drivers","text":"<ul> <li>some consider a kernel module just a \"piece of software\" of any kind at the kernel level that provide additional/different functionality, while a driver is a KIND of module used for communicating with hardware </li> <li>drivers are more specific and fixed, they communicate directly to the hardware and often start at boot. Examples include sound drivers for audio devices, display drivers for graphics cards, and network drivers for network interfaces</li> <li>modules are loaded dynamically throughout runtime and provide. They are easy to install or remove without rebooting the system.  Sometimes for a PCI device, the driver and module are the same, such as on my ethernet interface <pre><code>$ lspci -vs 00:1f.6\n00:1f.6 Ethernet controller: Intel Corporation Ethernet Connection (2) I219-LM (rev 31)\n    DeviceName:  Onboard LAN\n    Subsystem: Lenovo Ethernet Connection (2) I219-LM\n    Flags: bus master, fast devsel, latency 0, IRQ 122\n    Memory at df000000 (32-bit, non-prefetchable) [size=128K]\n    Capabilities: &lt;access denied&gt;\n    Kernel driver in use: e1000e\n    Kernel modules: e1000e\n</code></pre></li> </ul>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#sysctl","title":"Sysctl","text":"<p>Modify kernel parameters at run time using either of the following. <pre><code>echo 1 &gt; /proc/sys/kernel/sysrq\n#OR\nsudo sysctl -w kernel.sysrq=1\n</code></pre> These will not be persistent, to have this add lines in <code>/etc/sysctl.conf</code>. See all current parameters with <code>sysctl -a</code></p> <p>You can fine tune performace for networking, vm's, I/O, etc. <pre><code>ls /proc/sys/\nabi  debug  dev  fs  kernel  net  user  vm\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#ntp","title":"NTP","text":"<p>For system components to operate, such as Cronjobs and systemd timers to run at the proper time, there must be accurate timekeeping.  Computers can utilize the Network Time Protocol (NTP) to synchronize their time to an upstream time server or a pool of servers to maintain accurate time. It is a UDP service that runs on port 123.</p> <p>NTP is how millions of computer's clocks stay synchonized over a network. This involved many time servers utilizing algorithms to mitigate the effect of network latency. NTP can typically maintain time to within 10 milliseconds over the public internet.</p> <p>The stratum model is key in understanding how the precision of time degrades over a network. Stratum 0 are devices directly connected to the reference clock, such as a GPS antenna. Stratum 1 are time servers that distribute time to clients i.e Stratum 2 devices. The higher the Stratum number, the more the timing accuracy and stability degrades because the greater distance from the reference clock.</p> <p>Use the following command to check your system clock and the sync status. Though <code>status</code> is actually the default if you just run <code>timedatectl</code> by itself. <pre><code>\u276f timedatectl status\n               Local time: Wed 2023-11-15 16:44:58 CST\n           Universal time: Wed 2023-11-15 22:44:58 UTC\n                 RTC time: Wed 2023-11-15 22:44:58\n                Time zone: America/Chicago (CST, -0600)\nSystem clock synchronized: yes\n              NTP service: active\n          RTC in local TZ: no\n</code></pre> If NTP service is inactive, set it to true. <pre><code>timedatectl set-ntp 1\n</code></pre> Here we can look at the NTP server, what stratum level we are, delay, etc. <pre><code>timedatectl timesync-status \n       Server: 2620:2d:4000:1::40 (ntp.ubuntu.com)\nPoll interval: 4min 16s (min: 32s; max 34min 8s)\n         Leap: normal\n      Version: 4\n      Stratum: 2\n    Reference: C944586A\n    Precision: 1us (-25)\nRoot distance: 1.715ms (max: 5s)\n       Offset: +7.926ms\n        Delay: 139.890ms\n       Jitter: 2.988ms\n Packet count: 4\n</code></pre></p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#chrony","title":"Chrony","text":"<p>Chrony is a flexible NTP implementation to sync your clock across various NTP servers. It's deamon, <code>chronyd</code> continuously adjusts the system clock to calculate offset and drift.</p> <p>Here is a guide from Ubuntu on installing and configuring Chrony. </p>"},{"location":"Linux/Linux%2B/1.%200%20System%20Management/1.7%20Manage%20Software%20Configurations/#localectl","title":"localectl","text":"<p>This command is for managing your keyboard configuration and language settings. <pre><code>\u276f localectl\n   System Locale: LANG=en_US.UTF-8\n                  LANGUAGE=en_US:en\n       VC Keymap: n/a\n      X11 Layout: us\n       X11 Model: pc105\n</code></pre> As an example, you could set the keymapping to a US layout with <code>localectl set-keymap us</code>.</p> <p>Here you can display locale names available on your system, which determines the language, character encoding, and cultural conventions used for displaying and formatting text. <pre><code>\u276f localectl list-locales \nC.UTF-8\nen_AG.UTF-8\nen_AU.UTF-8\nen_BW.UTF-8\nen_CA.UTF-8\nen_DK.UTF-8\nen_GB.UTF-8\n</code></pre></p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.1%20PKI-Certs-Auth-Linux%20Hardening/","title":"2.1 PKI Certs Auth Linux Hardening","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.1%20PKI-Certs-Auth-Linux%20Hardening/#authentication","title":"Authentication","text":"<p>Note: Do not confuse authentication with authorization. Authentication is proving you are who you say you are. Authorizing is what comes after, this grants you access to certain resources. </p> <p>The following protocols are still widely used and worth studying, however, there are modern authentication frameworks such as Oauth(2006) and OpenID (2005) that are recommended for building modern applications. The main problems are older tools are being written in c (not secure), not web or mobile friendly, and poor developer experience.</p> <p>See Ubuntu's great article on the history of open source authentication and what lead to Oauth and OpenIP. Part 1 and [part 2] (https://ubuntu.com/blog/history-of-open-source-identity-management-part-2)</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.1%20PKI-Certs-Auth-Linux%20Hardening/#sssd-and-pam","title":"SSSD and Pam","text":"<p>PAM itself dates back at least to 1997. A major criticism was it's inability to implement Kerberos on it's own, which led to the monolithic pam_krb5 module beginning in 2006. SSSD is more recent with the first release in 2009 which is a modern replacement.</p> <p>RHEL 7 and Fedora 32 dropped both pam_krb5 and pam_pkcs11 in favor of SSSD.</p> <p>SSSD is used to authenticate into FreeIPA and Active Directory.</p> <p>Packages  <code>pam_tally</code> and <code>pam_tally2</code> have been deprecated by <code>pam_faillock</code> being most recent. The purpose is to maintain a counter of failed login attempts in a given interval to ban the user out after a set number of attempts. Whether that user is banned forever or for a certain time is defined in <code>/etc/security/faillock.conf</code>. </p> <p><code>authconfig</code> and <code>faillock</code> are used in this package.</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.1%20PKI-Certs-Auth-Linux%20Hardening/#freeipa","title":"FreeIPA","text":"<p>FreeIPA (Identity, Protection, Auditing) is a suite of technology that is upstream of RHEL Identity Management (IdM) FreeIPA uses 389 DS as it's database which uses the LDAP protocol. FreeIPA also includes SSSD and Kerberos.</p> <p>Active Directory also uses LDAP protocol.</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.2%20Identity%20Management/","title":"2.2 Identity Management","text":"<p>bashrc - Hidden file within each users home directory executed for non-login shells. Often stores aliases and other customization. Here is an example of mine. <pre><code># Useful aliases\nalias home='cd ~'\nalias cd..='cd ..'\nalias ..='cd ..'\nalias ...='cd ../..'\n\n# Set vi mode instead of default emacs based bash commands\nset -o vi\n\n# Get weather from the terminal\nalias wtr=\"curl wttr.in\"\n\n# Starship faunt\neval \"$(starship init bash)\"\n</code></pre> bash_profile or profile - Another hidden file in a users home directory executed for login shells. Instead of the bashrc which will run every time you open a terminal, this will only run once when you login. </p> <p>Keep in mind there is also system wide profile and bashrc files located within <code>/etc</code> for all users.</p> <p>/etc/login.defs - where commands like useradd, usermod, groupadd and others take values, such as min and max value for user or group id. Other functions like password aging controls, login retry attempts, and permissions/mode of new home directories with umask. Some functions like login retry attempts can be overridden by PAM and others obsoleted. </p> <p>pam_tally2 - <code>pam_tally2</code> has been deprecated in PAM 1.5.0 (2020) in favor of <code>pam_faillock</code></p> <p>faillock is a module for locking out users after failed attempts. It is configured in <code>/etc/security/faillock.conf</code>. Here you can adjust number of attempts and length of lockout time. </p> <p>In this setup, 3 failed login attempts within 15 minutes of each other will lock that user out for 10 minutes. <pre><code>deny = 3\n\n# The default is 900 (15 minutes).\nfail_interval = 900\n\n# The default is 600 (10 minutes).\nunlock_time = 600\n</code></pre></p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/","title":"2.4 SSH and Excecuting Commands as Another User","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#ssh","title":"SSH","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#history-and-overview","title":"History and Overview","text":"<p>SSH is a protocol developed by a Finnish researcher in 1995 after his university was attacked by a password sniffer. At the time insecure protocols like telnet were used. SSH encrypts all data over a network, so hackers can't do any sniffing.</p> <p>Two things are needed to connect, username for which user you want to login as, and the server host, which can either be an IP address or domain name.  <pre><code>ssh username@serverhost\n</code></pre></p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#key-based-authentication","title":"Key Based Authentication","text":"<p>By default, a server will have password-based authentication. Key-based authentication is recommended with a public and private key.</p> <p><code>ssh-keygen</code> is primarily used on the client to generate key pairs for authentication when connecting to servers.</p> <p>ed25519 is the recommended algorithm for SSH key generation. As of OpenSSH 9.5 ed25519 is used by default for <code>ssh-keygen</code></p> <pre><code>ssh-keygen -t ed25519 -f ~/.ssh/[filename] C \"[Comment]\"\n</code></pre> <p>An optional passphrase will be presented.</p> <p>This will create both <code>filename.pub</code> with the public key and comment, as well as <code>filename</code> which will contain the private key.</p> <p>Now the public key needs to be sent to the server with <code>ssh-copyid</code>. <code>-i</code> is for identify file. <pre><code>ssh-copyid -i ~/.ssh/filename.pub joseph@x.x.x.x\n</code></pre> This will ask for the password, then add the client's public key to the servers <code>~/.ssh/authorized_keys</code> for easy login thereafter.</p> <p>The client's <code>~/.ssh/known_hosts</code> file contains fingerprints of servers public keys so that it can recognize them as trusted for future logins.</p> <p>If you have your keys generated and able to login with the machines you want, you should then disable password-based SSH authentication on the server. </p> <p>This is the server's system-wide sshd config file  <pre><code>sudo vi /etc/ssh/sshd_config\n</code></pre> Change or uncomment these <pre><code>PasswordAuthentication no\nPubkeyAuthentication yes\nPermitRootLogin no\n</code></pre> now reload the ssh deamon <pre><code>sudo systemctl reload sshd\n</code></pre></p> <p>Now if you try to SSH into the server on any machine, it will be denied unless keys are generated and copied over like we did before.</p> <p>Other security measures could be changing the SSH port away from the default 22.</p> <p><code>/etc/ssh/sshd_config</code> is the server config, and <code>/etc/ssh/ssh_config</code> is the client config</p> <p><code>ssh_config</code> often comes by default so any computer can act as a client easily. HOWEVER, most systems will not have an <code>sshd_config</code> by default. For a system to act as a server, it needs this file.</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#ssh-agent-vs-systemd-sshd-service","title":"ssh-agent vs systemd sshd service","text":"<p><code>ssh-agent</code> is a program for clients that stores private keys in memory and is used for public-key authentication. Keys are added using <code>ssh-add</code> or when it is automatically configured in <code>ssh-config</code>. SSH looks at the environment variables below and uses it to establish a connection.</p> <p>Here is a look at one of my laptops which act as clients to connect ot my Ubuntu webserver. <pre><code>\u276f ssh-agent\nSSH_AUTH_SOCK=/tmp/ssh-XXXXXXDHLZVi/agent.5315; export SSH_AUTH_SOCK;\nSSH_AGENT_PID=5316; export SSH_AGENT_PID;\necho Agent pid 5316;\n\n\u276f ps aux | grep ssh\njoe         4863  0.0  0.0   7972  1080 ?        Ss   11:08   0:00 ssh-agent\n</code></pre></p> <p>Now on the server side, here is my Ubuntu machine I use to run my Apache webserver. You can see the various Systemd services running to authenticate clients. <pre><code>\u276f systemctl status ssh\nsshd-keygen.service  ssh.service          \nsshd.service         ssh.socket      \n</code></pre></p> <p>So to summarize, <code>ssh-agent</code> is typically used on the client side to manage SSH keys, while SSH Systemd services like <code>sshd</code> are usually found on the server side to accept incoming SSH connections.</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#executing-commands-as-another-user","title":"Executing commands as another user","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.4%20SSH%20and%20Excecuting%20Commands%20as%20Another%20User/#policy-kit-and-pkexec","title":"Policy Kit and pkexec","text":"<p>Polkit is the \"sudo of systemd\". It defines the policy that allows unprivileged processes to speak to privileged processes, such as granting a user the right to perform some task in a certain context. Instead of granting root permission of the entire process, like <code>sudo</code>, polkit instead allows more fine grain control, such as privileges for certain situations or duration of time. </p> <p>As an alternative to running a command as root or superuser privilege, you can use <code>pkexec</code>. This is common for when you screw up your <code>/etc/sudoers</code> file.</p> <p>It will prompt you for your password if on a text-based environment and window if in a graphical session. <pre><code>\u276f pkexec ls\n==== AUTHENTICATING FOR org.freedesktop.policykit.exec ===\nAuthentication is needed to run `/usr/bin/ls' as the super user\nAuthenticating as: Jobo Baggins (joe)\nPassword: \n</code></pre> ![[pkexec.png]] If failed, the incident will be reported and you can see the logs in <code>/var/log/auth.log</code> <pre><code>Dec 13 18:32:47 statsfordevs pkexec[853597]: joe: Error executing command as another user: Not authorized [USER=root] [TTY=/dev/pts/0] [CWD=/home/joe] [COMMAND=/usr/bin/ls]\nDec 13 18:32:47 statsfordevs polkitd(authority=local): Unregistered Authentication Agent for unix-process:846897:34748679 (system bus name :1.156, object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale en_US.UTF-8)\n</code></pre></p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/","title":"2.5 Access Controls","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#acl","title":"ACL","text":"<p>By default, Linux permissions only allow a single user and single group of a given directory/file. We can see this text file is owned by Joe and belongs to the \"study\" group. <pre><code>ls -l test.txt \n-rw-r----- 1 joe study 8 Nov 30 19:33 test.txt\n</code></pre></p> <p>If we wanted more flexible permissions, such as a user that is not Joe nor part of the study group to be able to read and write to the file, we can use Access Control Lists (ACL). These are great for giving multiple groups access to a file or directory, or giving a specific user access even though they are not part of the file's group.</p> <p><code>getfacl</code> and <code>setfacl</code> are our two go-to commands (the f in the commands are\"file\"). Use the former to view the current permissions of a file. Here we have the default permission setup of a single user and group. <pre><code>getfacl test.txt \n# file: test.txt\n# owner: joe\n# group: study\nuser::rw-\ngroup::r--\nother::---\n</code></pre></p> <p>Now lets add the work group by modifying <code>-m</code> the file and check it afterwords. <pre><code>setfacl -m g:work:rw test.txt \ngetfacl test.txt \n# file: test.txt\n# owner: joe\n# group: study\nuser::rw-\ngroup::r--\ngroup:work:rw-\nmask::rw-\nother::---\n</code></pre> Now not only is the work group added for read and write permissions, but this created a mask. This is the maximum effective permissions for any entry in the ACL.</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#attributes","title":"Attributes","text":"<p>Common permission attributes include append-only and immutability. Append-only prevents modifying existing content in a file, but only appending. Immutable files/directories cannot be written, appended, or deleted, even by root. </p> <pre><code># change attribute to immutable\nchattr -i [file]\n# change attribute to append-only\nchattr -a [file]\n</code></pre> <p>To list attributes of files in current directory, or point to a file/directory, use <code>lsattr</code>.</p> <p>To check whether a file has either ACL permission set, attributes assigned, or both, look for the plus at the end of the permissions with <code>ls -l</code>.  <pre><code>#         v\n-rw-rw----+  1 joe study     8 Nov 30 19:33 test.txt\n</code></pre></p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#special-permissions","title":"Special Permissions","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#set-user-id-suid","title":"Set user ID (SUID)","text":"<p>When a program is ran with SUID, it executes with the permissions of whoever owns the file. In the case of <code>umount</code>, if a normal user were to execute this command, they would in fact run it as if they were root. This is generally for programs that require system access that you still want users to run.</p> <p><pre><code>#SUID bit set\n   v\n-rwsr-xr-x 1 root root       35192 Feb 20  2022 umount\n</code></pre> Note if the file owner does not have execute permissions, there would be an uppercase S .</p> <p>Another common example of SUID is the <code>passwd</code> command. Any user should be able to change their password and this is possible by executing the file as if they were root. <pre><code>ls -l /bin/passwd\n-rwsr-xr-x 1 root root 59976 Nov 24  2022 /bin/passwd\n</code></pre> Adding this permission to a file is as follows. <pre><code>chmod u+s &lt;filename&gt;\n</code></pre></p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#set-group-id-sgid","title":"Set group ID (SGID)","text":"<p>SGID does the same thing but for running a program with permissions of the group. Here we can see the group of executable file <code>ssh-agent</code> is <code>_ssg</code>. When a user is running this program, they will act as a member of the file's group. <pre><code>#SGID bit set\n      v\n-rwxr-sr-x 1 root _ssh      293304 Aug 24 08:40 ssh-agent\n ```\n SGID has a different function on directories. If set, any file created within that directory will inherit the group ownership of the parent directory group.\n\nFor example, this directory has an owner of bill and group of study. I can add SGID with the following. Note the before and after in the executable portion of the group permissions.\n</code></pre></p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#before","title":"Before","text":"<pre><code>  v\n</code></pre> <p>drwxrwxr-x 2 bill study 4096 Dec  3 12:24 dir</p> <p>chmod g+s sgid</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#after","title":"After","text":"<pre><code>  v\n</code></pre> <p>drwxrwsr-x 2 bill study 4096 Dec  3 12:25 dir <code>`file1` was created by joe before adding the SGID. After it was set `file2` was created inheriting the study group from it's parent directory.</code> ls -l dir total 0 -rw-rw-r-- 1 joe joe   0 Dec  3 12:25 file1 -rw-rw-r-- 1 joe study 0 Dec  3 12:26 file2 ```</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#sticky-bit","title":"Sticky bit","text":"<p>Folders with this activated cannot have files within them deleted or renamed by other users. This is commonly used on directories like <code>/tmp</code> to prevent users from deleting each other's files.  ```</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#tmp-folder-has-sticky-bit-set-on-by-default","title":"tmp folder has sticky bit set on by default","text":"<pre><code>     v\n</code></pre> <p>drwxrwxrwt  17 root root       4096 Dec  3 11:44 tmp <code>Use the following either in symbolic or numeric mode to add this restriction.</code> chmod +t dir</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#or","title":"OR","text":"<p>chmod 1777 dir ``` Though it is possible to add sticky bit to a file, it currently has no function in modern Linux systems.  So what to remember about each 3 of these permissions is that SUID is for executable files only, SGID works on both executable files and directories, and sticky bit only works on directories. </p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#apparmor","title":"Apparmor","text":"<p>Apparmor is a Linux kernel security module found in Debian based and Ubuntu systems. It provides Mandatory Access Control (MAC) to restrict programs, applications, and scripts capabilities with per-program profiles. It is considerably easier than SELinux setup and maintain. Unlike SELinux, which is based on applying labels to files based on their inodes, AppArmor works with file paths.  <code>aa-status</code> will display profiles of programs either in enforce mode, which will log and block access, and complain mode, which will only log. You can view these logs in <code>/var/log/syslog</code>.  Installing <code>apparmor-utils</code> includes essential commands such as those needed to move profiles to enforce or complain mode, and to generate profiles.  In the <code>/etc/apparmor.d</code> directory, you will find the profiles on your system as seen with <code>aa-status</code>. To create a profile with an easy template, use <code>aa-easyprof</code>, or <code>aa-genprof</code> to create your own.  To restrict the scope of access of the <code>ss</code> program, I can create a profile and place it in the apparmor directory as follows. Next I will enforce it. <code>\u276f sudo aa-easyprof /usr/bin/ss &gt; usr.bin.ss \u276f sudo mv usr.bin.ss /etc/apparmor.d/ \u276f sudo aa-enforce /etc/apparmor.d/usr.bin.ss</code> At first, the program will essentially be disabled until you explicitly define it's permissions.   When I try to run <code>ss</code>, it is denied permission. These logs can then be found in <code>var/log/syslog</code>.  <code>\u276f ss Cannot open netlink socket: Permission denied Cannot open netlink socket: Permission denied Cannot open netlink socket: Permission denied \u276f tail -1 /var/log/syslog Dec  9 15:42:56 x230-2 kernel: [15915.826222] audit: type=1400 audit(1702158176.819:103): apparmor=\"DENIED\" operation=\"create\" profile=\"/usr/bin/ss\" pid=16843 comm=\"ss\" family=\"netlink\" sock_type=\"raw\" protocol=4 requested_mask=\"create\" denied_mask=\"create\"</code> <code>aa-logprof</code> is an interactive tools that reads <code>var/log/syslog</code> for Apparmor events and will generate a list of suggested profile changes that the user can choose from to update security profiles. For example, if I recently ran <code>ss</code> when after I restricted it, it would prompt me to adjust permissions. ``` \u276f aa-logprof  Updating AppArmor profiles in /etc/apparmor.d. Reading log entries from /var/log/syslog. Complain-mode changes:</p> <p>Profile:        /usr/bin/ss Network Family: netlink Socket Type:    raw</p> <p>[1 - include ]   2 - network netlink raw,  (A)llow / [(D)eny] / (I)gnore / Audi(t) / Abo(r)t / (F)inish <p>...</p> <p>(S)ave Changes / Save Selec(t)ed Profile / [(V)iew Changes] / View Changes b/w (C)lean profiles / Abo(r)t Writing updated profile for /usr/bin/ss. <pre><code>After I have allowed access to multiple processes and saved changes to the profile. I can see the change in `usr.bin.ss`. The default `aa-easyprof` template it started with basically allows nothing, then afterwords we can see the what we allowed from `aa-logprof`.\n\nBefore `aa-logprof` (default `aa-easyprof`, nothing)\n</code></pre> \u276f cat /etc/apparmor.d/usr.bin.ss </p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#vimsyntaxapparmor","title":"vim:syntax=apparmor","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#apparmor-policy-for-ss","title":"AppArmor policy for ss","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#author","title":"###AUTHOR","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#copyright","title":"###COPYRIGHT","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#comment","title":"###COMMENT","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#include","title":"include","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#no-template-variables-specified","title":"No template variables specified <p>/usr/bin/ss {   #include  <p># No abstractions specified   # No policy groups specified   # No read paths specified   # No write paths specified } <pre><code>After allowing access to multiple processes using `aa-logprof`\n</code></pre> \u276f cat /etc/apparmor.d/usr.bin.ss </p>","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#last-modified-sat-dec-9-172136-2023","title":"Last Modified: Sat Dec  9 17:21:36 2023 <p>include","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#vimsyntaxapparmor_1","title":"vim:syntax=apparmor","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#apparmor-policy-for-ss_1","title":"AppArmor policy for ss","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#author_1","title":"###AUTHOR","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#copyright_1","title":"###COPYRIGHT","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#comment_1","title":"###COMMENT","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#no-template-variables-specified_1","title":"No template variables specified <p>/usr/bin/ss {   include    include  <p>/proc//net/raw r,   /proc//net/udp r,   /proc/*/net/unix r, } <pre><code>#### SELinux\nSecurity Enhanced Linux is a Mandatory Access Control (MAC) system built into the upstream Linux kernel itself. It was originally developed by the NSA and Red Hat as a series of security patches using Linux Security Modules (LSM).  \n\nTwo most important concepts in SELinux are Labeling and Type Enforcement.\n\nFiles, directories, ports, etc. are all labelled with a SELinux context. Files and directories' labels are stored as extended attributes on the filesystem. Processes and port's labels are managed by the kernel.\n\nLabels are in this format: \n- user:role:**type**:level(optional)\n\nUse the `-Z` flag to create files and directories with SELinux security context, or `ls` to print security context.\n</code></pre> cp -Z mkdir -Z ls -Z <pre><code>Use `chcon` and `restorecon` to change context of a file. Note changes with chcon do not survive a filesystem relabel, often triggered with `/.autorelabel` at boot time.\n\nFile's contexts are set when they are created based on their parent directories context (with some exceptions). File context and label's refer to the same thing.\n\nLet's say some file is not working and you suspect it is the context after inspecting it with `ls -Z`. If you have a known good file you can reference it's context and point it at the file to copy it's context.\n</code></pre> chcon --reference known-good.txt broken.txt  ```</p> <p>You can also restore a file or directory to the default context using <code>restorecon</code>. </p> <p>To change the default setting, i.e persistent changes use <code>semanage fcontext</code>. Then run resorecon to apply the changes or run an autorelabel. </p> <p>Run <code>chcon -t httpd_sys_content_t file-name</code> to change context type. This will not persist across reboot.</p> <p>To put Selinux is permissive mode temporarily, run <code>setenforce 0</code> or <code>1</code> for enforcing. Modify this in <code>/etc/selinux/config</code> to persist across reboots.</p>","text":""},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#dac-mac-discretionary-vs-mandatory-access-control","title":"DAC &amp; MAC - Discretionary vs Mandatory Access Control","text":"<p>Traditional Unix/Linux systems were discretionary. These have simple owner, group and other permissions. A user has the ability (discretion) to change permissions on their own files and <code>chmod +rwx</code> his own home directory and nothing will stop them, and then nothing will stop other users or processes from accessing that user's directory. On top of this Discretionary Access Control gives the root user omnipotent powers.</p> <p>On Mandatory Access Control systems, there is a policy which is administratively set and fixed. Even if you change the DAC settings on your home directory, if there is a policy in place which prevents another user or process from accessing it, you're generally safe.</p> <p>These policies can be very fine grained. Policies can be set  to determine access between:  - Users - Files - Directories - Memory - Sockets - tcp/udp ports - etc.</p> <p>Type enforcement is the part of the policy that says, for instance, \"a process running with the label httpd_t can have read access to a file labeled httpd_config_t\"</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#selinux-and-apparmor-comparison","title":"SELinux and Apparmor Comparison","text":"<ul> <li>Selinux operates at the system level, while Apparmor works at the applications level.</li> <li>Apparmor identifies filesystem objects by filepath as opposed to Selinux which identifies them by inode.</li> <li>Selinux is used by RHEL(Red Hat), SLES(SUSE), and Android, while Apparmor is used by Debian and Ubuntu.</li> </ul> <p>Application Level Apparmor - Deny Everything Selinux - Allow Everything System Level Apparmor - Allow Everything Selinux - Deny Everything</p>"},{"location":"Linux/Linux%2B/2.0%20Security/2.5%20Access%20Controls/#comparison-of-selinux-and-apparmor","title":"Comparison of SELinux and AppArmor","text":"Feature SELinux AppArmor Original Sponsor DoD/NSA (USA) DoD/DARPA (USA) Approach Deny everything system level Deny everything application level Integration in Linux LSM Module + Userland LSM Module + Userland Performance Impact &lt;5% 0-2% Restriction MAC, RBAC, MLS, Network Labeling MAC, RBAC, (MLS) Confinement Indirect, via \"Labels\" Direct, Path based Profiles/Policies \"Program\" needs to be compiled Simple text files, Unix style Auditing / Human SELinux policies are hard to read and to audit AppArmor profiles are easy to audit Availability in Distros CentOS, RHEL, SUSE Linux Enterprise Ubuntu, openSUSE, SUSE Linux Enterprise Primary Adoption US/Canada Worldwide Notes SUSE Linux Enterprise - Support for software stack, no policy included in SLES. Preconfigured part of SLE Micro 5. Support for software stack and AppArmor profiles. Will be used for CCC for SLE 15. Courtesy SUSECON #### Additional Learning SUSE Conference Overview of SELinux and AppArmor Presentation Red Hat Summit Security-Enhanced Linux for mere mortals Presentation Very in depth interview with Selinux/NSA Engineer Interview Red Hat Selinux Documentation"},{"location":"Linux/Linux%2B/2.0%20Security/Permissions-Octal%20Values/","title":"Permissions Octal Values","text":"<ul> <li>r (read): 4</li> <li>w (write): 2</li> <li>x (execute): 1 </li> </ul> <p><code>chmod 764 file</code> would mean rwx for owner, rw for group, and r for others.</p> <p>umask is the default set of permissions that subtracts from 666 for files, and 777 for directories <pre><code>umask\n0002\n\ntouch file\n\nstat -c \"%a\" file\n664\n</code></pre></p> <p><code>chmod</code> can use octal mode/numeric (like above) or symbolic.</p> <p>Here we change the file permissions for owner and group to rwx <pre><code>chmod og+rwx file\n</code></pre></p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/","title":"3.1 Bash scripting","text":""},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#brace-expansion","title":"Brace Expansion","text":"<pre><code>\u276f echo {1..5}\n1 2 3 4 5\n\u276f echo {0..20..2}\n0 2 4 6 8 10 12 14 16 18 20\n\u276f echo {a..f}\na b c d e f\n\n\u276f touch file{1..4}.txt\n\u276f ls\nfile1.txt  file2.txt  file3.txt  file4.txt \n\n\u276f cp -p file.config{,.bak}\n\u276f ls\nfile.config  file.config.bak\n\n\u276f echo {contents,paper,bilbiography}.md\ncontents.md  paper.md  bilbiography.md\n\n\u276f wget https://www.some-url.com/picture{1..8}.jpg\n</code></pre>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#command-substitution","title":"Command Substitution","text":"<p>Capture the output of a command, and store it in a variable.</p> <p><pre><code>var=$(command)\n</code></pre> or <pre><code>var=`command`\n</code></pre></p> <p><pre><code>my_date=`date +%m-%d-%Y`\n#OR\nmy_date=$(date +%m-%d-%Y)\necho \"You accessed this date on $my_date\"\n</code></pre> Output <pre><code>You accessed this date on 09-13-2023\n</code></pre></p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#globbing","title":"Globbing","text":"<p>Globbing in Bash refers to how the shell interprets special characters such as <code>*</code>, and <code>?</code> which are commonly used to match filenames. Bash itself cannot recognize regular expressions, instead we use tools like <code>sed</code> and <code>awk</code>. It is important to distinguish globbing and regex as they can be easily confused.</p> <p>The question mark matches any single character while the asterisks matches zero or more characters. <pre><code>\u276f ls file?.txt\nfile1.txt  file2.txt  file3.txt file4.txt file5.txt\n\n\u276f ls file[1-3].txt\nfile1.txt  file2.txt  file3.txt\n\n\u276f ls *.txt\ndep.txt  file1.txt  file2.txt  file3.txt  file4.txt file5.txt   marks.txt\n\n\u276f ls file?+(.png|.txt)\nfile1.png  file1.txt  file2.png  file2.txt  file3.png  file3.txt  file4.png  file5.png\n</code></pre></p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#here-documents","title":"Here Documents","text":"<p>Send multiple lines of text to a command or shell script. <pre><code>command &lt;&lt; [marker]\ninput\n[marker]\n</code></pre></p> <pre><code>\u276f sort &lt;&lt; END\n\u2219 3\n\u2219 2\n\u2219 4\n\u2219 1\n\u2219 END\n1\n2\n3\n4\n</code></pre>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#here-string","title":"Here String","text":"<p>Send one line of text to a command or shell script. <pre><code>\u276f wc -c &lt;&lt;&lt; \"String with many characters\"\n28\n\n\u276f foo=\"bar\"\n\u276f sed 's/a/A/' &lt;&lt;&lt; \"$foo\"\n bAr\n</code></pre></p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#shell-control-and-redirection","title":"Shell control and redirection","text":"<p><code>&gt;&amp;</code>, <code>&amp;&gt;</code>, <code>&gt;&gt;&amp;</code> and <code>&amp;&gt;&gt;</code> : (read above also) Redirect both standard error and standard output, replacing or appending, respectively.</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#additional","title":"Additional","text":"<p><code>source</code> reads and executes the contents of a file as sets of commands in the current shell.</p> <p>Here is a file called commands.txt <pre><code>echo \"Your current directory is `pwd`\"\necho \"The date is `date +%m-%d`\"\n</code></pre></p> <pre><code>\u276f source commands.txt\nYour current directory is /home/promptier/Desktop/bash\nThe date is 11-20\n</code></pre> <p>Note <code>source script</code> is equivalent to <code>. script</code>, not to be confused with <code>./script</code>, which runs the script as an executable file, launching a new shell to run it.</p> <p>The <code>type</code> command is useful to learn more about a command. If it is a shell built in, it will not have a man page, instead, read about it using <code>man bash</code>.</p> <pre><code>\u276f man source\nNo manual entry for source\n\n\u276f type source\nsource is a shell builtin\n</code></pre> <p>The <code>$PATH</code> variable contains a list of directories the system checks before running a command. Instead of running <code>/usr/bin/python3</code>, we can just run <code>python3</code> because <code>/usr/bin</code> is located in the path.</p> <p>If you install a program that is not located in the <code>$PATH</code> variable, you can add it with either of the following two ways:  <pre><code>export PATH=/the/file/path:$PATH $\n#adds to the beginnning and will be checked first\n#OR\nexport PATH=$PATH:/the/file/path\n#adds to the end and will be checked last\n</code></pre></p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.1%20Bash%20scripting/#script-utilities","title":"Script Utilities","text":"<p>By default, <code>tee</code> will overwrite files, use <code>-a</code> to append:  <pre><code>command | tee -a file\n</code></pre> Tee can also be useful when writing to a protected file. <pre><code>echo \"newline\" | sudo tee -a secret.txt\n</code></pre> It is called \"tee\" as it resembles the letter \"T\" as well as the T-splitter in plumming since it takes from STDIN and \"splits\" or writes to both STDOUT and files. </p> <p><code>egrep</code>, <code>fgrep</code> and <code>rgrep</code> are the  same  as  <code>grep -E</code>, <code>grep -F</code>, and <code>grep -r</code>.</p> <p><code>grep -F</code> is a fixed string, meaning you want the string to be passed verbatim, and not interpreted as special regex. Such as if the search includes a dot <code>user.txt</code> that you don't want to be interpreted as a regex wildcard.</p> <p><code>grep -E</code> is extended grep, which can be used for fancy expressions, like <code>()</code> for groups and <code>|</code> for OR. Here we search for any line that starts with \"fork\" or \"group\". <pre><code>\u276f grep -E '^no(fork|group)' /etc/group\nnogroup:x:65534:\n</code></pre> If you used regular grep without the -E, you'd have to escape out the special characters or else they'd be searched literally. <pre><code>grep '^no\\(fork\\|group\\)' /etc/group\n</code></pre> As another useful example, we search for PCI devices starting with either \"ethernet\" or \"network\" with an insensitive search. <pre><code>\u276f lspci | egrep -i 'ethernet|network'\n00:19.0 Ethernet controller: Intel Corporation 82579LM Gigabit Network Connection (Lewisville) (rev 04)\n03:00.0 Network controller: Intel Corporation Centrino Advanced-N 6205 [Taylor Peak] (rev 34)\n</code></pre></p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.2%20Containers/","title":"3.2 Containers","text":""},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.2%20Containers/#history-virtual-machines-and-containers","title":"History, Virtual Machines and Containers","text":""},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.2%20Containers/#history","title":"History","text":"<p>When you learn about container technology i.e Docker, or Virtual Machines, a story is presented: at first, there were dedicated servers each running their own application. This ensured isolation and dedicated resources but was not cost-efficient. Then came along virtual machines, which allowed a single physical machine to simulate/virtualize multiple operating systems. This was more efficient, but then came along Containers. Why run an entire operating system to run a single application, even if it's virtualized? Containers emerged as a lightweight alternative to VMs, running only what is strictly necessary to run the single application. This allows for more efficient resource use and faster startup times.</p> <p>Here is a timeline of server evolution.</p> <p>Dedicated Servers (Early 1990s - Early 2000s) Companies had on-premise, separate physical servers for their email, website, and database services.  Companies like IBM and Dell were key players in providing server hardware during this period. This era also saw the emergence of rack-mounted servers, which brought a new level of scalability and manageability.</p> <p>Virtual Machines (Early 2000s - Early 2010s): Virtualization technology gained traction in the early 2000s, with widespread adoption occurring through the 2010s. VMware became a significant figure in this space with their product offerings like VMware Workstation (introduced in 1999) and VMware ESX (introduced in 2001). Microsoft released Hyper-V in 2008, integrating virtualization capabilities into Windows Server.</p> <p>Containerization (Mid 2010s - Present) Developers use containers to package applications with all their dependencies. This allows for consistent deployment across different environments. Microservices architectures also became more feasible with containers. Docker, released in 2013, revolutionized container management and deployment, making it easier to use. Google developed Kubernetes, which was released in 2014, to automate the deployment, scaling, and management of containerized applications.</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.2%20Containers/#virtualization","title":"Virtualization","text":"<p>For homelabs or personal projects, most use Virtualbox or Hyper-V on Windows and KVM-Qemu on Linux. KVM (Kernel Virtual Machine) is an example of a type-1 hypervisor as it essentially converts the kernel itself into a hypervisor. Virtualbox is a type-2 as you install it on top of your existing Host operating system.</p> <p></p> <p>When you have multiple different hypervisor stacks, you may have different sets of tools to manage them. Libvirt is designed to end this. It is a universal tool which can manage QEMU, KVM, LXC, VMware, Hyper-V and many more. Features include storage pools, virtual network management, snapshots, resource allocation, etc.</p> <p>cloud-init is a bootstrapping technology for deploying virtual machines that can automatically configure SSH keys, users, networking rules, packages, etc.</p> <p>LXC is sort of a middle ground between Docker containers and hypervisor based VMs in terms of isolation. LXD is an extension of LXC as a management tool.</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.5%20Cloud%20%26%20Orchestration%20Concepts/","title":"3.5 Cloud & Orchestration Concepts","text":""},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.5%20Cloud%20%26%20Orchestration%20Concepts/#kubernetes-vs-docker-compose","title":"Kubernetes vs Docker Compose","text":"<p>Docker compose is mainly for managing multi container applications on a single-host deployment, and lack the scalability features of kubernetes</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.5%20Cloud%20%26%20Orchestration%20Concepts/#cloud-init","title":"Cloud Init","text":"<p>Set of scripts and utilities used for configuring and customizing virtual machines when they first boot in a cloud environment, such as AWS and Azure. When the VMs boot, you can automatically configure the host name, install packages, add users and groups, configure SSH keys, really anything you want. This makes it easy to spin up VMs that are already configure how you want them. Cloud-init usually only runs on the first boot of a VM unless manually configured otherwise.</p> <p>Everything can be found in <code>/etc/cloud</code>.  <pre><code>\u276f ls /etc/cloud\nclean.d  cloud.cfg  cloud.cfg.d  cloud-init.disabled  ds-identify.cfg  templates\n</code></pre> Five stages of Cloud-Init 1. Generator (determines if ) 2. Local 3. Network 4. Config 5. Final</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/3.5%20Cloud%20%26%20Orchestration%20Concepts/#service-mesh","title":"Service Mesh","text":"<p>Service Mesh Interface (SMI) is a set of standards with a focus on those that run on Kubernetes. Examples include Istio and Linkerd.</p> <p>Most modern applications no longer follow a monolithic design with one massive code base, rather a containerized and modular design. Using Kubernetes for an E-commerce platform, each of these services would be contained within their own pod.</p> <ul> <li>Authentication Service to manage user login, registration, authentication and handling use sessions</li> <li>Product Catalog Service: to manages the inventory of products and provide product information to customers.</li> <li>Payment Gateway Service to integrates with external payment systems process payment and transactions securely.</li> </ul> <p>Service meshes are one approach to managing these distributed/micro service architecture. A dedicated infrustructure layer will monitor, provide traffic management, and security without adding to each pod's own code. </p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Docker/","title":"Docker","text":"<p><code>docker run</code> = <code>docker create</code> + <code>docker start</code></p> <p>A stopped container can be restarted with <code>start</code>, unlike an exited container. <code>ps</code> will show only running containers, add the <code>-a</code> flag to include stopped ones. <code>docker ps -l</code> will who only the most recently created container, though in recent versions this may be <code>-n 1</code>, where you can manipulate the ouput, such as listing the last 5 created containers with <code>docker ps -n 5</code>. Use <code>-q</code> for \"quiet\" mode that will only list ids. <pre><code>\u276f docker ps -n 3 -q\nf77c2b719993\n5b0b857c73eb\n807f5b547c1a\n</code></pre> The main difference between the paused and stopped (exited) states is that the memory portion of the state is cleared when a container is stopped, whereas, in the paused state, its memory portion stays intact.</p> <p>![[docker-lifecycle.png]]</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Git/","title":"Git","text":"<p>Recently, Github has changed the primary branch name from the traditional \"master\" to \"main\". This is becoming the new standard but they still refer to the same thing. </p> <p><code>git branch [name]</code> - create new branch  <code>git checkout [name]</code> - move to branch Pull Request - Proposal to merge a set of changes from one branch to another, must be reviewed then approved. Often provided by hosting services like Github or Gitlab rather than a feature of Git itself. (Gitlab refers to them as Merge Requests) fireship HEAD - most recent commit</p> <p>Say you've  been working on a new feature in a branch called <code>feaure</code>, once completed, you can checkout to the main branch and merge the changes. <pre><code>git checkout main\ngit merge feature\n</code></pre> ![[git-merge.png]]</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Git/#git-tags","title":"Git Tags","text":"<p>By default, a tag will be lightweight, which includes no metadata. Make it annotated with the <code>-a</code> flag to include who tagged it and when. These annotated tags are considered best practice as it is nice to know when a version was released and who released it.</p> <p>Include a message with <code>-m</code> or it will launch an editor for you to do it anyways. <pre><code>git tag -a v1.0 -m \"first release\"\n</code></pre> Then you can see the release info with <code>git show v1.0</code></p> <p>From <code>git tag --help</code> - \"Annotated tags are meant for release while lightweight tags are meant for private or temporary object labels.\"  Lightweight tags are basically just pointers to commits.</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Git/#rebase","title":"Rebase","text":"<p>Git rebase is an alternative to merge as a tool for integrating changes from one branch to another. Unlike merge, rebase has powerful history rewriting features. This is to maintain a linear and project history or in other words, a clean commit history.</p> <p>Note how the changes of the feature branch were moved on to the top of main. This flattens the history, removing unwanted entries.</p> <p>![[git-rebase.png]]</p> <p>The golden rule of git rebase is to never use it on public branches.</p> <p>To delete a specific commit, rebase in interactive mode just before the commit you want to delete, then delete the line containing that commit. <pre><code>git rebase -i &lt;commit&gt;~1\n#or possibly\ngit reset --soft &lt;commit&gt;~1\n</code></pre></p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Git/#tips-and-tricks","title":"Tips and Tricks","text":""},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Git/#add-and-commit-on-one-line","title":"Add and Commit on One Line","text":"<pre><code>git commit -am \"Git Add and Commit Shortcut\"\n#or\ngit commit -all -m \"Git Add and Commit Shortcut\"\n</code></pre> <p>This is equivalent to <pre><code>git add .\ngit commit -m \"Your commit message\"\n</code></pre></p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Git/#fix-previous-commit","title":"Fix Previous Commit","text":"<p>If you made a commit but realized you made a mistake, you can ammed the commit with the following. <pre><code>git commit --ammend -m \"New message\"\n</code></pre></p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/","title":"Infrastructure as Code","text":""},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#utilities","title":"Utilities","text":""},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#ansible","title":"Ansible","text":"<p>Uses playbooks written in YAML to automate Linux servers. One machine is a control node which sends the other managed nodes an Ansible module over SSH. Examples include updating network settings, provisioning databases, or any job you do more than once.</p> <p>The machine acting as the control node runs the CLI tools such as <code>ansible-playbook [file].yml</code> which runs on all managed nodes, sometimes referred to as \"hosts\", which are your target devices (severs, network appliances, VMs, etc.). Ansible does not need to be installed on the managed nodes, only the control node. Hosts are defined in an inventory file.</p> <p>![[ansible.png]] The industry standard Ansible certification is the RHCE (Red Hat Certified Engineer). A prerequisite to this is the RHCSA (Red Hat Certified Systems Administrator), which is more comparable to the Linux+, except more geared towards RHEL distributions.</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#terraform","title":"Terraform","text":"<p>Blueprints that automate everything you do in the cloud. Scripts are<code>.tf</code> files written in HashiCorp Configuration Language (HCL).</p> <p>People commonly use both Ansible and Terraform together, with Terraform for provisioning, and Ansible for configuring. Though many tasks can also be completed on boot with cloud-init (such as SSH and key setup).</p> <p>![[terraform.png]]</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#saltstack","title":"SaltStack","text":"<p>Salt is a configuration management and orchestration tool written in Python currently currently supported by VMware after it's acquisition. It came from the need for communication and task execution to many systems in complex data centers. It gives administrators the ability to manage massive infrastructure at scale using the Salt remote execution engine, which stores configuration (state) data accessed through YAML.</p> <p>Salt is known for it's steep learning curve. </p> <p>![[saltstack.png]] Terraform was released in 2014 by Hashicorp under the permissive Mozilla Public License v2.0 (MPL 2.0) but recently changed the licensing to a Business Source License (BSL), which allows copying, modifying and redistribution, but restricts commercial use. This upset the community as the Open Source Initiative (OSI) definition of open source software does not discriminate against commercial venture. Rather than open source, it is more \"source-available\". As a response, the Linux Foundation created a fork of Terraform known as OpenTofu as an open source alternative.</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#puppet-and-chef","title":"Puppet and Chef","text":"<p>Two less common automation tools written in Ruby.</p> <p>As a rule of thumb, whichever project has the cooler logo and has more Github star deserves more attention. In this case, Ansible is the clear winner.</p> <p>![[IAC-logos.png]]</p>"},{"location":"Linux/Linux%2B/3.0%20Scripting%2C%20Containers%2C%20Automation/Infrastructure%20as%20Code/#popularity-and-employment-compared","title":"Popularity and Employment Compared","text":"<p>Here are all the Infrastructure-as-Code tools popularity based on Github Stars. Ansible (pink) is by far the most popular followed by Terraform (green).</p> <p>![[IAC-tools.png]] Here are the number of jobs listed from Glassdoor in the United States in the month of November, 2023 mentioning these tools. ![[IOC-jobs.png]]</p>"},{"location":"Linux/Linux%2B/4.0%20Troubleshooting/4.1%20Storage/","title":"4.1 Storage","text":""},{"location":"Linux/Linux%2B/4.0%20Troubleshooting/4.1%20Storage/#io-schedulers","title":"I/O Schedulers","text":"<p>I/O Scheduling aka disk scheduling is how your computer handles read/write requests and in what order. Some emphasize particular goals, such as prioritizing certain process's I/O requests over others, to give a fair share of disk bandwidth to each process, or to ensure requests are complete by a particular deadline.</p> <p>Ubuntu Docs \"multiqueue is enabled by default providing the bfq, kyber, mq-deadline and none I/O schedulers\"</p> <p>mq-deadline (Multiqueue) This is an adaption of the deadline I/O scheduler but designed for Multiqueue devices. A good all-rounder with fairly low CPU overhead.</p> <p>To check your scheduler  <pre><code>\u276f grep -i 'io scheduler' /var/log/dmesg\n[    0.688085] kernel: io scheduler mq-deadline registered\n# OR\n\u276f cat /sys/block/[Storage Device]/queue/scheduler \n[none] mq-deadline \n</code></pre> To change your scheduler to either three. This will not persist across reboots <pre><code>\u276f sudo echo [noop|cfq|deadline] &gt; /sys/block/hda/queue/scheduler`\n</code></pre></p> <p>Three main ones are:</p> <ul> <li>CFQ (cfq): the default scheduler for many Linux distributions; it places synchronous requests, submitted by processes, into a number of per-process queues and then allocates timeslices for each of the queues to access the disk.</li> <li>Noop scheduler (noop): the simplest I/O scheduler for the Linux kernel based on the First In First Out (FIFO) queue concept. This scheduler is best suited for SSDs.</li> <li>Deadline scheduler (deadline): attempts to guarantee a start service time for a request.     -- TechRepublic</li> </ul> <p>The noop scheduler is often recommended for storage devices or storage controllers that already perform their own request sorting and scheduling. This is common with some SSDs and certain storage systems. If the underlying storage device has its own efficient I/O scheduling mechanism, using noop at the kernel level can be beneficial.</p> <p>The deadline scheduler is often considered a good choice for SSDs. Unlike traditional hard drives, SSDs don't have physical heads that need to seek to specific locations on the disk, making the seek time less relevant.</p> <p>The only way to confidently tell what scheduler is best on your system is to run and compare benchmarks with a tool like iozone.</p>"},{"location":"Linux/Linux%2B/4.0%20Troubleshooting/4.2%20Networks/","title":"4.2 Networks","text":""},{"location":"Linux/Linux%2B/4.0%20Troubleshooting/4.2%20Networks/#openssl-s_client","title":"OpenSSL s_client","text":"<p>Diagnostic tool for SSL/TLS Servers</p> <p><pre><code>\u276f openssl s_client -connect linkedin.com:443 -showcerts\nCONNECTED(00000003)\ndepth=2 C = US, O = DigiCert Inc, OU = www.digicert.com, CN = DigiCert Global Root CA\n...\n</code></pre> grab cert in .pem file <pre><code>\u276f echo | openssl s_client -connect linkedin.com:443 2&gt;&amp;1 | sed --quiet '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt; linkedin.com.pem\n</code></pre></p> <p>Read more here</p>"},{"location":"Linux/Linux%2B/4.0%20Troubleshooting/4.2%20Networks/#nmap","title":"Nmap","text":"<p>Nmap (Network Mapper) is a tools designed to scan large networks. You can easily see which ports are open, closed, or filtered, what devices and firewalls are in use, etc. Though common for security auditing, it can be used for much more, such as network inventory and managing service upgrade schedules.</p> <p>Concerning legality, it in not illegal to port scan in the US. The worst that might happen is the target network complains to your ISP, or your ISP may block your machine outright if it is against their policy. Keep in mind port scanning is not the same as vulnerability scanning, which can be illegal.</p> <p>One way to scan ports would be a tool like netcat. This is useful to check a particular connection or port, but not sufficient for scanning an entire network. <pre><code>\u276f nc -zv scanme.nmap.org 22 80 \nConnection to scanme.nmap.org (45.33.32.156) 22 port [tcp/ssh] succeeded!\nConnection to scanme.nmap.org (45.33.32.156) 80 port [tcp/http] succeeded!\n</code></pre></p> <p>Nmap is more efficient by automatically scanning the most commonly open 1,000 ports as per the nmap-services. </p> <p>The <code>-A</code> is for OS and version detection. As you can tell, Nmap provides additional information for each port, such as SSH key and what web server they are running (Apache). <pre><code>\u276f nmap -A -T4 scanme.nmap.org\nPORT      STATE    SERVICE     VERSION\n22/tcp    open     ssh         OpenSSH 6.6.1p1 Ubuntu 2ubuntu2.13 (Ubuntu Linux; protocol 2.0)\n| ssh-hostkey: \n|   1024 ac:00:a0:1a:82:ff:cc:55:99:dc:67:2b:34:97:6b:75 (DSA)\n|   2048 20:3d:2d:44:62:2a:b0:5a:9d:b5:b3:05:14:c2:a6:b2 (RSA)\n|   256 96:02:bb:5e:57:54:1c:4e:45:2f:56:4c:4a:24:b2:57 (ECDSA)\n|_  256 33:fa:91:0f:e0:e1:7b:1f:6d:05:a2:b0:f1:54:41:56 (ED25519)\n80/tcp    open     http        Apache httpd 2.4.7 ((Ubuntu))\n|_http-server-header: Apache/2.4.7 (Ubuntu)\n|_http-title: Go ahead and ScanMe!\n\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n</code></pre></p>"},{"location":"Linux/Linux%2B/4.0%20Troubleshooting/4.3%20CPU%20%26%20Memory/","title":"4.3 CPU & Memory","text":""},{"location":"Linux/Linux%2B/4.0%20Troubleshooting/4.3%20CPU%20%26%20Memory/#swapfile","title":"Swapfile","text":""},{"location":"Linux/Linux%2B/4.0%20Troubleshooting/4.3%20CPU%20%26%20Memory/#create-swapfile","title":"Create Swapfile","text":"<p>Swap is the reserve space on disk for when run out of memory. It can either be a dedicated file or partition. Here we will create a swapfile, and learn to increase it's size.</p> <p>Use <code>dd</code> or <code>fallocate</code> to create a file atleast 1 GB. Here <code>-l</code> is the length/size we are preallocating. <pre><code>sudo fallocate -l 1G /swapfile\n</code></pre> Set permission so only root can read and write to it. Then <code>mkswap</code>, and <code>swapon</code>. <pre><code>sudo chmod 600 /swapfile\nsudo mkswap /swapfile\nsudo swapon /swapfile\n</code></pre> Append the following to <code>/etc/fstab</code> <pre><code>/swapfile swap swap defaults 0 0\n</code></pre> Confirm with either of the following <pre><code>sudo free -h\n#OR\nswapon --show\n</code></pre></p>"},{"location":"Linux/Linux%2B/4.0%20Troubleshooting/4.3%20CPU%20%26%20Memory/#increase-size-of-swap-file","title":"Increase Size of Swap File","text":"<pre><code>sudo swapoff /swapfile\nsudo dd if=/dev/zero of=/swapfile bs=1M count=1024 oflag=append conv=notrunc\nsudo mkswap /swapfile\nsudo swapon /swapfile\n</code></pre>"},{"location":"Linux/Linux%2B/4.0%20Troubleshooting/4.5%20Systemd/","title":"4.5 Systemd","text":"<p><code>multi-user.target</code> is the typical default-target for servers without a gui. Systemd targets were designed to emulate legacy run levels, such as run level 5 requiring previous run levels in order. Likewise, <code>graphical.target</code> runs only after <code>multi-user.target</code>, which is equivilent to run level 5 on legacy SysV systems. </p> <pre><code>\u276f systemctl cat graphical.target \n...\nAfter=multi-user.target rescue.service rescue.target display-manager.service\n...\n</code></pre> <p>Many unit files have the following <code>WantedBy=multi-user.target</code> which just means this service should be started as part of a regular system startup, whether the gui is active or not.</p> <p>Unit files without <code>WantedBy=multi-user.target</code> with no other enabled service including a <code>Requires=your.service</code> or <code>Wants=your.service</code> in it's unit file, your service will not be started automatically. </p> <p>Stack Exchange post explaining multi-user.target.</p>"},{"location":"My%20Setup/Linux%20%26%20Vim/","title":"Linux & Vim","text":""},{"location":"My%20Setup/Linux%20%26%20Vim/#linux","title":"Linux","text":"<p>I currently use Linux Mint with the Cinnamon Desktop Environment and will for the foreseeable future, as long as it's dependencies are stable (Ubuntu&gt;Debian). Since Linux Mint relies on Ubuntu, which in turn relies on Debian, some would recommend Linux Mint LMDE since it does not rely on Ubuntu, but rather Debian directly, which is now 30 years old and has proven it's stability with the test of time.</p> <p>I would personally recommend Linux Mint over Ubuntu as it is easier to transition from Windows (Linux Mint has a windows like interface). Ubuntu is ran by Canonical, a company with open source in mind, but a for-profit company nonetheless. Linux Mint on the other hand is build and maintained fully by the community.  If you still use windows (or god forbid Apple), consider buying an old thinkpad, installing the Linux Mint ISO, making a bootable USB with something like Rufus, and installing it just to give it a try. I personally bought an \"broken\" thinkpad for $30, and simply replaced the hard drive to get it working. Things are getting better on Linux (especially gaming) and I have found myself requiring the terminal less often for installations, and things just working out of the box.</p>"},{"location":"My%20Setup/Linux%20%26%20Vim/#vim-vscode","title":"Vim &amp; VsCode","text":"<p>When doing web development, I will use VSCode because of it's convenient source control panel, extensions library/ecosystem, and just because things work out of the box, like Emmet and the Vim extension which I love.</p> <p>Although all of these things I'm sure are possible on vim/neovim with enough configuring, and likely, would be worth it in the long run.</p> <p>Besides VSCode, for Bash, or Python scripts, or really anything other than web development, I find myself using the terminal.</p> <p>Concerning Vim, it is seriously worth learning even just for the keybindings alone, as it will make you much more efficient in VSCode with the extension.</p>"},{"location":"My%20Setup/Keyboards/Kinesis%20vs%20Ergodox/","title":"Kinesis vs Ergodox","text":""},{"location":"My%20Setup/Keyboards/Kinesis%20vs%20Ergodox/#ergodox-ez","title":"Ergodox EZ","text":""},{"location":"My%20Setup/Keyboards/Kinesis%20vs%20Ergodox/#pros","title":"Pros","text":"<ul> <li>Smaller and easier to carry around</li> <li>I found Orynx to be easier and more intuitive than Kinesis SmartApp. In terms of their graphical user interface (GUI).</li> </ul>"},{"location":"My%20Setup/Keyboards/Kinesis%20vs%20Ergodox/#cons","title":"Cons","text":"<ul> <li>Although the web based software is nice is convenient in some ways, you have to reset and then flash your keyboard with Wally every time, which is tedious</li> </ul>"},{"location":"My%20Setup/Keyboards/Kinesis%20vs%20Ergodox/#kinesis-advantage-2","title":"Kinesis Advantage 2","text":""},{"location":"My%20Setup/Keyboards/Kinesis%20vs%20Ergodox/#pros_1","title":"Pros","text":"<ul> <li>Although it is less convenient to travel with, it's size and being just one big piece makes it very comfortable on my lap, which is how I always use it. </li> <li>I love that it is concaved (curved keys) because I have closer access to the keys with each fingers.</li> <li>Because of the concaved design, my thumbs have greater access to the thumb clusters, 5/6 keys in a resting postion, as opposed to the Ergodox, which if I remember I only had access to around 2-3/6 keys in a resting postion.</li> <li>I like that I could quickly remap keys and make macros all from the keyboard, without having to reset and flash everytime from the cloud.</li> </ul>"},{"location":"My%20Setup/Keyboards/Kinesis%20vs%20Ergodox/#cons_1","title":"Cons","text":"<ul> <li>It being large makes it harder to travel with, but again this is just a tradeoff of it being concaved, which means more access to keys.</li> </ul>"},{"location":"My%20Setup/Keyboards/Kinesis%20vs%20Ergodox/#linux-compatibility","title":"Linux Compatibility","text":"<ul> <li>For the Advantage 2, some things do not work on Linux like the Smartset App, however, it works fine for me because I can just adjust key mappings on windows, then plug it into my Linux machine and it works fine.</li> <li>For Ergodox, It looks like you can install Wally from this repo. </li> </ul> <p>Here are links to each (not affiliated or sponsored): Kinesis Ergodox</p>"},{"location":"My%20Setup/Keyboards/My%20Current%20Setup/","title":"My Current Setup","text":""},{"location":"My%20Setup/Keyboards/My%20Current%20Setup/#overview","title":"Overview","text":"<p>Over the past few months using the Kinesis Advantage 2, I have tinkered with it enough to understand what works best for me, so hopefully you can take something away from this as well.</p> <p>This is an overview of my setup. As you can tell I have my text editor, and web browser to the left moniter, and obsidian for note taking and a calendar to the right (it's an X230 thinkpad with Linux, no I don't use Arch). Ideally the moniters would be parallel, but it doesn't bother me enough to care. ![[full_view.JPG]]</p>"},{"location":"My%20Setup/Keyboards/My%20Current%20Setup/#regular-mouse-setup","title":"Regular Mouse Setup","text":"<p>Since I spend upwards of 8 hours a day on my computer (probably more) I want to have the most comfortable setup where my wrist moves as little as possible when accessing the mouse. That means whether I have to keyboard on the desk or my lap, the mouse must be as close as possible, luckily, this desk has an L shape shaped perfectly. </p> <p></p>"},{"location":"My%20Setup/Keyboards/My%20Current%20Setup/#trackpad-setup","title":"Trackpad Setup","text":"<p>I love trackpads, like those on Thinkpads, ideally where the buttons are on top since I would have closer access to them here. Surprisingly, there are a huge lack of external trackpads for sale, excluding Apple's Magic Proprietary garbage trackpad, this Cirque works well enough (I could no longer find it on Amazon). Although for me, this setup is only useful for when I want it on my lap, if on the desk, a real mouse is easier to reach and feels much better. And if you're wondering, I just attached it with velcro.</p> <p></p>"},{"location":"Neovim/Usage/","title":"Usage","text":""},{"location":"Neovim/Usage/#stack-overflow","title":"Stack Overflow","text":"<p>IDEs/text editors were first added the the annual Stack Overflow Survey in 2021. That means we have 3 years to measure the growth of Neovim, as we can tell, it has nearly doubled since last year.</p> <p>Neovim Popularity by Year (all respondents) - 2021     - 4.99% - 2022     - 6.75% - 2023     - 11.88%</p>"},{"location":"Neovim/Usage/#google-trends","title":"Google Trends","text":"<p>This screenshot of google trends for also supports the case that Neovim is exploding in popularity.</p> <p></p>"},{"location":"Python/Advanced%20Webscraping/","title":"Advanced Webscraping","text":"<p>When using the requests library alone, it will often trigger a security feature to prevent online attacks. Here I'm wanting to do some scraping on Indeed for job listings and I suspect I will need a user agent to get past this. <pre><code>import requests\nr = requests.get(\"https://www.indeed.com/jobs\")\nprint(r.text)\n</code></pre> Here is the HTML page response. Cloudflare has blocked me.</p> <p></p> <p>Though I could try to use a fake user agent to fool Cloudflare, their security features are too advanced. Instead Undetectable Chromedriver should do the trick.</p>"},{"location":"Python/Miscellaneous%20Notes/","title":"Miscellaneous Notes","text":""},{"location":"Python/Miscellaneous%20Notes/#__repr__-and-__str__-methods","title":"__repr__ and __str__ methods","text":"<p>Two dunder (double underscore) methods that are very similar and often confused are repr and str. They both return a string representation of an object.  In this example, they are functionally very similar, both returning a string about the person object. Both attributes name and age are included.</p> <pre><code>class Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def __str__(self):\n        return f\"I'm {self.name}, and I'm {self.age} years old.\"\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(name='{self.name}', age={self.age})\"\n</code></pre> <p>Though functionally similar, the difference is who they are intended for. repr is meant to be an unambiguous string representation of an object for developers to debug and/or recreate the object if necessary. str  is used for a user friendly printed representation of an object for end-users. The focus here is on readability.</p> <p>Just remember repr is for developers and str is for customers.</p> <p>By default, printing an object will use the str method, unless there is none, in which case it will fall back to the repr method. if there is neither a repr nor a str method, it will print the memory address.</p>"},{"location":"Python/Miscellaneous%20Notes/#lamda","title":"lamda","text":"<p><pre><code>class Book:\n    def __init__(self, title, author, pages):\n        self.title = title\n        self.author = author  \nbook_details = [\n    (\"It\", \"Stephen King\", 1138),\n    (\"A Tale of Two Cities\", \"Charles Dickens\", 304),\n    (\"Going Postal\", \"Terry Pratchett\", 484)\n]\n\nbooks = []\n\nfor details in book_details:\n    books.append(Book(*details))\n\nsorted_books = sorted(books, key=lambda book: book.title)\n</code></pre> In this example, there is a list of book objects called <code>books</code> created from information in a list of tuples <code>book_details</code>. An additional list is created called <code>sorted_books</code> that makes reference to the same objects in the previous list, but orders them based on the book title of each object. lambda is used as a concise, inline key for sorting. It provides a anonymous function without the need for a <code>def</code> keyword, such as the following.</p> <pre><code>def get_title(book):\n    return book.title\n\nsorted_books = sorted(books, key=get_title)\n</code></pre>"},{"location":"Python/Package%20Management%20and%20Virtual%20Environments/","title":"Package Management and Virtual Environments","text":""},{"location":"Python/Package%20Management%20and%20Virtual%20Environments/#overview","title":"Overview","text":"<p>There are many different ways to manage Python package versions and dependencies. Despite the Zen of Python saying \"There should be only one obvious way to do it\", Python itself is wide spread and used in many different domains, preferences and requirements start to develop across these many fields. System administrators often use the default package manager on their system (apt, yum, etc.) while those in the scientific community and data science use Anaconda, which bundles Python and R in it's own environments. The most common is to just use <code>pip</code>, the default package manager that grabs packages from PyPi (Python Package Index), in combination with Virtual Environment (<code>venv</code>), which is part of the standard library. <code>virtualenv</code> is a very similar tool which includes a few more features.</p>"},{"location":"Python/Package%20Management%20and%20Virtual%20Environments/#various-tools","title":"Various Tools","text":"<p>Here are some various notes I've jotted down and other tools to be aware of.</p> <p>Pipfile: the replacement for  pip's requirements.txt <code>pipenv</code> combines pipfile, pip and <code>virtualenv</code>.  <code>pyenv</code> isolates Python versions <code>venv</code> is a built-in module in Python 3 while <code>virtualenv</code> is a third-party tool</p> <p>This Stackoverflow post sums everything up and is a great resource.</p>"},{"location":"Python/Package%20Management%20and%20Virtual%20Environments/#venv-and-virtualenv","title":"venv and virtualenv","text":"<p>venv is a subset of virtualenv integrated into the standard library which lacks the following features - <code>app-data</code> seed method, which caches and speeds up the creation of virtual environments - automatically discover python version to base virtual environment - Support of Python 2 Despite being included in standard libraries of most distributions, on Debian/Ubuntu, it may not be and you'll have to <code>apt add-repository 'ppa:deadsnakes/ppa</code>, next <code>apt update</code>, then <code>sudo apt install python3.12-venv</code> This video is a good comparison between the two</p> <p>For spinning up small projects, it seems <code>venv</code> is the best option as it is the standard library default and has less features (that's a good thing) than <code>virtualenv</code>.</p> <p>To spin up a small project on Linux, run the following. The <code>-m</code> is for running library module as a script <pre><code>python3 -m venv [project directory]\n</code></pre> This will create a <code>bin</code> folder for executable files (possibly symlinks to host machine) including the Python interpreter, the very large <code>lib</code> folder which contains an entire copy of the Python standard library as well as any other package you install, the <code>include</code> directory for C headers to compile Python packages, and finally the <code>pyvenv.cfg</code> config file. </p> <p>To activate the environment, you can run the Bash script <code>bin/activate</code>.  <pre><code>source `[project dir]/bin/activate`\n</code></pre></p> <p>Now anything you install with <code>pip3</code> will be confined to that environment, regardless of what directory you are in. Simply run <code>deactivate</code> when you are done.</p>"},{"location":"Python/Package%20Management%20and%20Virtual%20Environments/#pipenv","title":"pipenv","text":"<p>Python is over a 30 year old language and some libraries have limitations and incompatibilities, pipenv is a modern tool released in 2017 to bridge the gap between Pip, Python and virtualenv by having only a <code>pipfile</code> for package dependencies and <code>pipfile.lock</code> for version releases/builds and hashes as opposed to the antiquated <code>requirements.txt</code>. pipenv creates ands and manages virtualenv for you.</p>"},{"location":"Python/Pygame/","title":"Pygame","text":""},{"location":"Python/Pygame/#how-i-started","title":"How I Started","text":"<p>After learning basic programming logic (for loops, functions etc.) and Python syntax, I wanted to build something big, not just a small terminal program that asks for your name, or age, and does some simple function. Pygame was the perfect library. This would allow me build a larger program, learn debugging, and implement an object oriented design with the players and enemies to to learn OOP (Object Oriented Programming).</p>"},{"location":"Python/Pygame/#overview","title":"Overview","text":"<p>Whether you are on Windows or Linux, you should be able to install Pygame with Python's package manager <code>pip</code> , and use your editor of choice. I will be using Vim from my Linux Terminal. Source code for the entire game will be in this repo.</p> <p>This will not be a conventional programming tutorial where we go line by line. Instead it will be about adding and removing chunks of code at a time, so you can get a bigger picture of what is going on. This way you will not simply copy the code from the tutorial (like most are), but understand what a fully functioning program looks like that you can play around with and break. </p>"},{"location":"Python/Pygame/#boilerplate","title":"Boilerplate","text":"<p>Here is a basic boilerplate of any Pygame program. To avoid future confusion as you read other tutorials, I have given a few different options, including the main game loop being a never ending loop, only stopped by the <code>exit</code> method, as well as creating a <code>running</code> variable set to True, and set that to False when we want to quit. The other option is the <code>display</code> or <code>update</code> method, which I argue my case for the <code>display</code> method below.</p> <p>Pygame's <code>init</code> and <code>quit</code> methods are opposites, initializing and deinitializing the Pygame library. <code>display.set_mode</code> creates the window, taking the width and height as arguments. <code>for event in pygame.event.get():</code> will loop through each Pygame event, such as click events, keyboard input, or quitting the game. </p> <p>As for the end, the <code>update</code> method is really only for updating certain rectangular part of the display surface for optimizing performance, since it will default to updating the entire screen, it is functionally the same as the <code>flip</code> method. You will often see the update method in tutorials, I think this is wrong and the <code>flip</code> method is preferable until you actually need to optimize your game, in which case you should use <code>update</code></p> <pre><code>import pygame \nfrom sys import exit\n\npygame.init() # turn on pygame\n\nwin = pygame.display.set_mode((800,500)) # 800 pixels wide, 500 pixels tall\n\n#running = True\n\n#while running\n#OR\nwhile True:\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT: \n            pygame.quit() # turn off pygame\n            exit() # using the sys library, exit the entire python program\n        #OR\n        # running = False\n\n    pygame.display.flip()\n    #OR\n    #pygame.display.update()\n</code></pre>"},{"location":"Python/Pygame/#pictures-of-final-game","title":"Pictures of Final Game","text":"<p>You can dig through the repository for the finished project here. (Several months later) I don't imagine I will ever get around to finishing a tutorial for this. It's only 340 lines of Python that is well commnented so you can probably make things out on your own.</p> <p> </p>"},{"location":"Python/Python%20Under%20the%20Hood/","title":"Python Under the Hood","text":"<p>It is known that Python's implementation, CPython, which act's as both an interpreter and a compiler, is written in C. Similarly, the Python Standard Library, including all of the most popular modules such as time, random, and os are also written in C. To have a better appreciation of Python and how it works under the hood, it is worth digging through some of the Python's source code to gain a greater understanding.</p> <p>First let's consider a simple program in Python that imports the time module and prints out the time. </p> <pre><code>import time\n\nseconds = time.time() # seconds since Epoch/Unix time (Jan 1st 1970)\n\ntime_h = time.ctime(seconds) # human readable time\nprint(time_h) # Thu Sep 28 18:54:21 2023\n</code></pre> <p>This seems to work just like magic, and it is! But really it is an abstraction, a wrapper around C's <code>&lt;time.h&gt;</code> library. Let's consider the same program in C. First we include two header files,  one for standard input output (stdio), and the other for time, then we do the same thing as the python file. </p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;time.h&gt;\n\nint main() {\n    time_t current_time;\n\n    seconds = time(NULL); // seconds since epoch\n\n    printf(\"%s\", ctime(&amp;current_time)); // human readable time\n    // # Thu Sep 28 18:54:21 2023\n\n    return 0;\n}\n</code></pre> <p>That works just as easily, but there are times when writing C code can get very verbose and tedious, as it is a lower level language that provides more granular control over hardware and memory management, which often requires more explicit code. Python make's things easier and provides abstractions for C's standard libraries such as <code>&lt;time.h&gt;</code>, and <code>&lt;math.h&gt;</code>.</p> <p>The time module in python and it's methods are defined in timemodule.c under Modules in the CPython repository. Here is the <code>ctime</code> function we used earlier in the Python file that prints out the human readable date. </p> <pre><code>static PyObject *\n_asctime(struct tm *timeptr)\n{\n    static const char wday_name[7][4] = {\n        \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"\n    };\n    static const char mon_name[12][4] = {\n        \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n        \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n    };\n    return PyUnicode_FromFormat(\n        \"%s %s%3d %.2d:%.2d:%.2d %d\",\n        wday_name[timeptr-&gt;tm_wday],\n        mon_name[timeptr-&gt;tm_mon],\n        timeptr-&gt;tm_mday, timeptr-&gt;tm_hour,\n        timeptr-&gt;tm_min, timeptr-&gt;tm_sec,\n        1900 + timeptr-&gt;tm_year);\n}\n</code></pre> <p>If you you look at the beginning of this same file, you will see the <code>&lt;time.h&gt;</code> library.</p> <p></p> <p>As you can see, the Python Time module, and many others, are in fact built directly in C. For example, when you write a Python program managing sockets that enable communication over a network, the sockets module is in fact build directly from C's <code>&lt;sockets.h&gt;</code> library. The same goes for every other module in the standard library, such as random, abc, math, and so on. </p>"},{"location":"Python/Webscraping/","title":"Webscraping","text":"<p>The first package necessary for webscraping in Python is requests. At 331 million downloads per month, it is one of the most popular downloads on the Python Package Index (PyPI). It is a simple and easy HTTP library, which in our case, will be used to <code>get</code> a url and store that in a response object, to then print out the text (HTML) content. Pretty print gives us a formatted and readable output.</p> <pre><code>import requests\nfrom pprint import pprint\n\nr = requests.get('https://github.com/sveltejs/svelte')\nprint(type(r))\npprint(r.text)\n</code></pre> <p>As you can tell by the url I am requesting, I am interested in Svelte.js, a very popular, lightweight Javascript framework for building web apps. I want to track the growth of this project, and compare it against other frameworks like React and Angular. The easiest metric would be the stars on their github repositories. I recommend you take a look at the repo, and open up dev tools with  with <code>ctrl</code> + <code>alt</code> + <code>c</code> and inspect this element so you understand what we are about to do.</p> <p></p> <p>If you look at the HTML of that particular <code>div</code> element that contains the stars, there is no <code>id</code> to easily grab. Most element either have an <code>id</code> attribute, which is unique, or a <code>class</code> attribute, which is shared across elements. Since there is only a class of <code>mt-2</code> as of making this video, it will be a bit tricky as it is hard to distinguish that specific element while scraping. If there were an id of say, <code>star-content</code>, it would look something like this:</p> <pre><code>import requests, re\nfrom bs4 import BeautifulSoup as bs4\n\nr = requests.get('https://github.com/sveltejs/svelte')\nsoup = bs4(r.text,'html.parser')\n\nstars = soup.find('div',id=\"star-content\")\nprint(stars.text)\n</code></pre> <p>As you can tell from above, we will need the bs4 package. We would request the url the same as before, then wrap it in a <code>Beautiful Soup</code> object using either <code>html.parser</code> or <code>lxml</code>. Next, find a div element with an id of \"star content\", then print out just the text inside the html tags. Since we don't actually have an id in the element, we will instead have to identify it with the class of <code>mt-2</code>. Since their are many elements with this class, we will have to use the <code>find_all</code> method and find out which on it is.</p> <p></p> <p>Let's find all div elements with class of <code>mt-2</code>. This will store them in an array, each element being indexed. <pre><code>mt_el = soup.find_all('div',class_=\"mt-2\")\nprint(mt_el)\n</code></pre></p> <p>But how do we tell which one is the element with the star data? After all, that's all we want. The best way is to print out each element, with it's corresponding index. There is a lot of white space, so we will use python's build in <code>strip</code> method.</p> <pre><code>for i,el in enumerate(mt_el):\n    print(el.text.strip(),i)\n</code></pre> <p>You should get an output like this:</p> <p></p> <p>If you look at the screenshot from earlier, you can tell this output is exactly the text content of every element in the about section. Since we know the stars are on the 5th index, we can go back and grab just that content.</p> <pre><code>mt_el = soup.find_all('div',class_=\"mt-2\")\nstars = mt_el[5].text.strip()\nprint(stars)\n</code></pre> <p>This will output something like this:</p> <p></p> <p>To grab just the <code>73</code>, we will use a regex (regular expression) with the re module. </p> <pre><code>mt_el = soup.find_all('div',class_=\"mt-2\")\nstar_el = mt_el[5].text.strip()\nmatch = re.search(r'(\\d{1,4}\\.\\d+|\\d{1,4})', star_el)\nstars = match.group()\nprint(stars())\n</code></pre> <p>This should print just <code>73</code>. </p> <p>So we can find the stars on the Svelte repo, great. But if you test this on other repos, the index of the star element will differ slightly, making it difficult to wrap in a function, for example, if I wanted to find the stars for the React repo, it is not on the 5th index, but rather the 6th.</p> <pre><code>mt_el = soup.find_all('div',class_=\"mt-2\")\nstar_el = mt_el[6].text.strip()\nmatch = re.search(r'(\\d{1,4}\\.\\d+|\\d{1,4})', star_el)\nstars = match.group()\nprint(stars)\n</code></pre> <p>To have a function that takes a url as an argument and automatically finds the index with the stars, we will have to create a temporary list that contains the stripped out text content of each div, to then be be searched for \"stars\" and the regex applied. Here is the program so far.</p> <pre><code>import requests, re\nfrom bs4 import BeautifulSoup as bs\n\ndef get_stars(url):\n    r = requests.get(url)\n    soup = bs(r.text,'html.parser')\n\n    mt_el = soup.find_all('div',class_=\"mt-2\")\n    tmp_list = []\n    for i, e in enumerate(mt_el):\n        tmp_list.append(mt_el[i].text.strip())\n\n    for e in tmp_list:\n        if \"stars\" in e:\n           match = re.search(r'(\\d{1,4}\\.\\d+|\\d{1,4})',e)\n           star = match.group() \n\n    return star\n\nsvelte = get_stars(\"https://github.com/sveltejs/svelte\")\nreact = get_stars(\"https://github.com/facebook/react\")\nangular = get_stars(\"https://github.com/angular/angular\")\n\nprint(\"Project and Stars\\n\")\nprint(\"React: \",react)\nprint(\"Svelte: \",svelte)\nprint(\"Angular: \",angular)\n</code></pre> <p>Expected output:</p> <p></p> <p>That's pretty cool. But is it really that useful? I mean I can just go and open up the url and look myself, after all. What would really be useful is to track the growth, maybe run the script everyday at a scheduled time, then append that to a json file. So we will need the date, then key value pairs that correspond to the project and stars. I am imagining a nested dictionary like this:</p> <pre><code>data = {\n    \"10-04-2023\": {\n        \"React\": 214,\n        \"Angular\": 90.6,\n        \"Svelte\": 73,\n    }\n    \"10-05-2023\": {\n        \"React\": 218,\n        \"Angular\": 90.9,\n        \"Svelte\": 74,\n    }\n    \"10-06-2023\": {\n        \"React\": 221,\n        \"Angular\": 91.2,\n        \"Svelte\": 86,\n    }\n}\n</code></pre> <p>This will be super useful, because dictionaries in python are equivelent to json (javascript object notion) which are just key value pairs as well. This way, we could use a javascript charting library to visually represent the data using chart.js.</p> <p>Alright, so to do that, we first need to get the date using the datetime module. Then to make things easier, I've made a <code>urls</code> dictionary with the name as a key, and url as a value. This way, we can easily add as many projects we want to track by simply adding them to this dictionary. The <code>data</code> dictionary is where the valuable data is stored, we will then open a json file to append and <code>dump</code> that data using the json library.</p> <pre><code>from datetime import datetime\nimport requests, re, json\nfrom bs4 import BeautifulSoup as bs\n\ntoday = datetime.now().strftime('%m-%d-%Y')\nurls = {\n    'Svelte': 'https://github.com/sveltejs/svelte',\n    'React': 'https://github.com/facebook/react',\n    'Angular': 'https://github.com/angular/angular'\n}\n\n\ndef get_stars(url):\n    r = requests.get(url)\n    soup = bs(r.text,'html.parser')\n\n    mt_el = soup.find_all('div',class_=\"mt-2\")\n    #star_el = mt_el[6].text.strip()\n    tmp_list = []\n    for i, e in enumerate(mt_el):\n        tmp_list.append(mt_el[i].text.strip())\n\n    for e in tmp_list:\n        if \"stars\" in e:\n           match = re.search(r'(\\d{1,4}\\.\\d+|\\d{1,4})',e)\n           star = match.group() \n\n    return star\n\ndata = {}\ndata[today] = {}\nfor name, url in urls.items():\n    stars = get_stars(url)\n    data[today][name] = float(stars) if '.' in stars else int(stars)\n\nwith open(\"data.json\", \"a\") as f:\n    json.dump(data, f, indent=4)\n</code></pre> <p>Now if you want to schedule this to automatically run everyday, you could use windows scheduler or cron if you're on Linux or Mac.</p> <p>If you'd like, I can make a tutorial on displaying this data using chart.js. Otherwise, if you're really curious of this kind of data, you should know a tool like this already exists. To track the star history of a github repository, you can use Star Historywhich is an open source project that uses Github's official API to find metadata of each project, including stars. This is a much more reliable method than ours, as our script would break the day Github updates their user interface, which is a common obstacle in webscraping.</p> <p>To compare Svelte, React, and Angular, you can view using their website here</p> <p>Regardless, web scraping is a valuable skill to have, and Python libraries makes it very easy and intuitive. </p>"},{"location":"Stats%20For%20Devs/About/","title":"About","text":"<p>Right now I am trying to set up a self hosted web server to serve up some HTML files on my own domain statsfordevs.com. The idea is to provide useful insight such as usage statistics, such as those found in Stack Overflow Survey, and employment statistics found from Indeed or Glassdoor. So given a programming language, framework, or any technology, you will know how many people use it, if that number is increasing or decreasing, and compare it with other comparable technologies.  For employment, take Python for example, which can easily be searched on Indeed, and a number 37,000 is returned. I am still considering how to get historical data on that, which I may have to pay money to use their API, in that case, I probably wouldn't, and would just write a scraper to periodically get data.</p> <p>Before the usage and employment graphs are shown, an overview an history will be given. An overview will be 1 to 2 paragraphs explaining why the technology was created, some historical context, and how it's used today. A timeline will also be present. Several examples are in this directory.</p>"},{"location":"Stats%20For%20Devs/Python/","title":"Python","text":""},{"location":"Stats%20For%20Devs/Python/#overview-and-history","title":"Overview and History","text":"<p>Python is a programming language first developed by Guido Van Rossum as a hobby project over Christmas break in 1989. The CWI (Centrum Wiskunde &amp; Informatica) research institute he worked at later played a key part in the early development and first release of Python in 1991.</p> <p>1989 Python development starts at CWI 1994 Python 1.0 released 2000 Python 2.0 released 2001 Numeric (early version of NumPy) available 2003 Matplotlib release 2005 Guido van Rossum joins Google 2005 Django released 2008 Python 3.0 released 2010 pandas released 2011 scikit-learn release</p>"},{"location":"Stats%20For%20Devs/Python/#usage-statistics","title":"Usage Statistics","text":"<p>Python usage by professionals from 2017 to 2023 according to the annual Stack Overflow Survey.  %% TODO: Add Ruby %% ![[python-usage.png]]</p>"},{"location":"Stats%20For%20Devs/Python/#employment-statistics","title":"Employment Statistics","text":"<p>![[python-employment.png]]</p>"},{"location":"Stats%20For%20Devs/Untitled/","title":"Untitled","text":"<p>%% regex, click over to see characters&lt;[^&gt;]+&gt; %% %% see ~/projects/statsfordevs %%</p>"}]}