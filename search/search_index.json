{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to my Website! This will serve as the place for my more formatted and organized writing.</p> <p>I am an undergraduate computer science student who has been programming and taking apart computers since I was was young. Linux has been a major interest as a hobby as well as in professional pursuit of certifications and employment. Various other interests are as follows:</p> <ul> <li> <p>Obsidian, a note taking app which I created my own plugin for, which measures aggregated file sizes (over 2k downloads).</p> </li> <li> <p>Godot, an open source game engine. I am specifically interested in creating 2d gravity simulations like this.</p> </li> <li> <p>Building websites with zero to minimal Javascript, such as simple staticly generated websites that you are likely reading this from.</p> </li> <li> <p>Ruby and Go. Both cool programming languages.</p> </li> </ul> <p>Many articles, notes, pages, etc. on here will have an accompanying youtube video, and vice versa. You can see my youtube channel here</p>"},{"location":"Contact%20Me/","title":"Contact Me","text":"<p>  jbulfer13@gmail.com </p> <p>  LinkedIn </p> <p>  Github </p> <p>  Linux Study Website </p> <p>  Join the Linux Discord </p>"},{"location":"Criticism%20of%20Rust/","title":"Criticism of Rust","text":"<p>\u26a0\ufe0f: Trigger Warning: I know Rust is a well loved language by those that use it and clearly it is useful in some areas considering its wide adoption. I haven't used Rust extensively and keep in mind these are just my opinions.</p> <p>There are two kinds of languages, those that solve a problem and those that prove a point, the latter goes to die. ~ Bjarne Stroustrup, I think</p> <p>Rust proposes a new system of memory management, one based on ownership and borrowing. Kitty rails set in place as if I don't know how to properly allocate my own bytes. Many would consider it a skill issue of not knowing C. </p> <p>If writing software for embedded systems, such as a microcontroller/IOT device, TinyGo has the same performance as Rust as found in this research paper. If you can write Go instead of Rust and get the same performance, the answer is clearly the simpler language.</p> <p>65% of Go Developers use the language for work, vs 20% of Rust developers. This means the majority of those learning and writing software in Rust are doing so as a hobby for personal/side projects. Go on the other hand is used by professionals in the industry. This is because Go solves several real world problems, one being code maintainability. Becuase the language is intentianally minimal with features and emphasizes readability, a code base could be untouched for 10 years after the original author left the company and still be maintained or refactored with minimal hassle.  </p> <p>todo: it's overly complex.  no video made yet</p> <p>TLDR: Just write C, if not then Go or TinyGo for Microcontrollers where low-power and efficiency is needed. </p>"},{"location":"Golang/","title":"Golang","text":""},{"location":"Golang/#history-and-overview","title":"History and Overview","text":"<p>In 1970 Dennis Ritchie created a high level language called C to build the Unix operating system, previously written in assembly. Unfortunately passing away before the creation of Go, his colleagues Ken Thompson and Rob Pike developed Go, a modern C of the 21st century, publically released as open source in 2012. Modern object oriented languages continue to add eachothers features in an attempt to keep market share. This means they are growing in complexity while simultaneously becoming more similar to one another. This is bloat without distinction.  </p> <p>Go is an unapologetic boring language with an intended lack of features. This means code is written procedurally in a simple fashion. A single programmer could write an entire codebase, leave for 5 years, and his coworkers will easily pick up where he left off. Writing in Go means large code bases are readable. Files are read top to bottom without hunting down where a method is imported from, or traversing several files to find the original implementation in a nest of inheritance.</p>"},{"location":"Golang/#problems-of-c-addressed-with-high-level-features","title":"Problems of C Addressed With High Level Features","text":"<p>Though required to manage if absolute performance is necessary, the following are the main problems with writing code in C:</p> <ol> <li>Uninitialized variables</li> <li>Out-of-bounds</li> <li>Use-after-free</li> <li>Read or write to unintended object (memory safety bug)</li> </ol> <p>Go solves 1 by zeroing out newly created variables, so instead of unexpected behavior in C, Go will return a value of zero. Problem 2 is solved with bounds checking of arrays/slices to ensure index is not out of range. Next, without needing to malloc and free memory by hand, Go's garbage collector takes care of number 3. Finally, number 4 is prevented without the use of pointer arithmetic, though pointers are allowed and encouraged in Go, we cannot manipulate them.</p>"},{"location":"Golang/#learning-resource","title":"Learning Resource","text":"<p>Go is intentionally a conservative and minimal language that doesn\u2019t change quickly, however, here are two things to be aware of when reading The Go Programming Language book.</p> <ul> <li>Go Modules Replaced GOPATH</li> <li>Generics are not included</li> </ul> <p>Besides this there may be occasional deprecated functions from standard packages such as rand or crypto. Overall it is a highly recommended book as Brian Kerningham is renowned computer scientist, OG Unix developer and also author of The C Programming Language. Alan Donovan, co-author, is an engineer in the Go Team at Google.    Book PDF</p>"},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/","title":"How to Learn Assembly Language (And Why)","text":"<p>\ud83d\udcf9 Related Video: Watch the related video to this article here.</p>"},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/#tldr","title":"TLDR","text":""},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/#how","title":"HOW","text":"<ul> <li>lc3tutor.org for web based simulator and code snippets<ul> <li>or LC3Tools for a desktop application simulator</li> </ul> </li> <li>Introduction to Computing Systems Book, written by LC-3 creator</li> </ul>"},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/#why","title":"WHY","text":"<ul> <li>Gain appreciation of how computers work and shoulders of giants we stand on.</li> <li>Optimization scenarios such as digging into x86 of high level language to prevent bounds checking.</li> <li>There are few jobs where people write in assembly relative to high level languages, but they are compilers/toolchain developers, malware analysts, embedded development, and computer science teachers.</li> </ul>"},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/#how-i-learned-low-level-stuff","title":"How I learned Low Level Stuff","text":"<p>Machine Organization is an upper level Computer Science course at my university. Dreaded by many, understanding the computer down to the transistor is necessary as well as designing logic gates and writing assembly code. Programming in high level languages (which is all most know how to do) does not help prepare. The course starts with character and number representation in binary, as well as arithmetic operations (add, subtract) and logical operations (AND, OR). After we understand base 2 and can perform basic operations, Circuit Verse was introduced to build logic gates. Our final culmination was an ALU (Arithmetic Logic Unit), which is the heart of the CPU. After CircuitVerse, LC-3 was introduced.</p>"},{"location":"How%20to%20Learn%20Assembly%20Language%20%28And%20Why%29/#lc-3","title":"LC-3","text":"<p>LC-3 is a simulated instruction set architecture (ISA) and assembly language designed for educational purposes. It is hypothetical does not run on actual hardware as most ISAs do. Instead there are several simulators available. One is LC3Tools, a desktop application which requires an install. On Linux I downloaded a single AppImage and it worked flawlessly. Alternatively, there is a web based simulator on lc3tutor.org, as well as many great example code snippets demonstrated in the video.</p> <p>If you would like to gain an understanding of an even lower level than assembly to start from a truly bottom up approach, you can learn binary and build logic gates on CircuitVerse. This will help in understanding the inner working of LC-3, but is not necessary.</p> <p>Though very dense at nearly 800 pages, the textbook written by the original LC-3 creator is available. This was the reading material in my Machine Organization course, though most students only opened it up when they needed to reference an opcode.</p>"},{"location":"Language%20Benchmarking/","title":"Language Benchmarking","text":"<p>\ud83c\udfc3 TLDR: Based on these benchmarks and my code to aggregate the CPU seconds column, Go and Java perform the same, C# performs twice as fast as both of them, Rust outperforms C# by a small margin. Python is 10x slower than Go or Java.</p> <p>My favorite bench marking is the Computer Language Benchmarks Game created by Debian developers. They test physics and astronomy simulations (N-body), various matrix algorithms, binary trees, regex, and more. Though you can do your best to interpret their mean results. I have written some awk and sed to provide another simpler metric comparing one language to another. </p> <p>Take this this data comparing Go and Python which I've copied directly from the website. </p> <pre><code>fannkuch-redux\nsource  secs    mem     gz  cpu secs\nGo #3\n    8.25    10,936  969     32.92\nGo\n    11.83   11,128  900     47.26\nGo #2\n    11.89   12,708  896     47.47\nPython 3 #6\n    913.87  11,080  385     913.83\nPython 3 #4\n    285.20  14,264  950     1,123.47\nn-body\nsource  secs    mem     gz  cpu secs\nGo #3\n    6.36    11,244  1200    6.37\nGo\n    6.55    11,244  1310    6.56\nGo #2\n    6.94    11,244  1215    6.95\nPython 3\n    383.12  11,096  1196    383.11\nPython 3 #2\n    402.59  11,096  1242    402.57 \n</code></pre>"},{"location":"Language%20Benchmarking/#processing-the-data","title":"Processing the Data","text":"<p>I only care about the <code>secs</code> to measure CPU time. So that means I can remove the names of the algorithms as well as the headers that include <code>source</code>, <code>secs</code>, etc. A single mention of <code>source</code> will remove the each entire record. Though there are none above, there are some tests that fail, we'll remove those and the line above them that contain <code>Python</code> or <code>Go</code>.</p> <pre><code>/source\\|binary\\|reverse\\|nucleotide\\|fasta\\|regex\\|pidigits\\|mandelbrot\\|spectral\\|n-body\\|fannkuch/d\nN; /Timed\\|Bad\\|Failed/{d;}; P; D\n</code></pre> <p>Now before I run my awk script to average out the first numbered field, I want to make sure <code>Python</code> and <code>Go</code> are on the same record as the numbers. In vim, <code>J</code> will remove a newline, basically moving current selected line up one. Let's run that on every line. Also the test numbers <code>#3</code> I want to remove as well as the <code>3</code> after Python. That will make it easier for awk to read just the data we want.</p> <pre><code>:g/^/normal J\n:%s/#.//g\n</code></pre> <p>After all that our data should look much cleaner.</p> <pre><code>Go 33.37    247,824     525     60.85\nGo 34.62    236,728     482     61.41\nPython 104.73   271,980     338     104.72\nPython 35.33    274,884     660     125.79\n</code></pre> <p>Now using awk I simply take the second field and average them for Go and Python separately.</p> <pre><code>awk '/Python/{python_sum+=$2; python_count++} /Go/{go_sum+=$2; go_count++} END{print \"Python Average: \" (python_sum/python_count); print \"Go Average: \" (go_sum/go_count)}' go-python.txt\n</code></pre>"},{"location":"Language%20Benchmarking/#final-results","title":"Final Results","text":"<p>Here is the result. I've also included some other languages as well. First I compare Go to three other languages, then C# against Java and Go in the last two. Not surprising to see Go 10x faster than Python, though I was shocked to see C# performed 2-3x faster than Go and Java. </p> <pre><code>Python Average: 106.756\nGo Average: 8.98625\n\nJava Average: 9.0565\nGo Average: 8.98625\n\nRust Average: 3.06823\nGo Average: 8.98625\n\nC# Average: 3.74485\nJava Average: 9.0565\n\nC# Average: 3.74485\nGo Average: 8.98625\n</code></pre>"},{"location":"Stop%20Leetcode/","title":"Stop Leetcode","text":"<p>Here is a collection of my notes for an article and video which is are both in development. It is essentially against the notion that algorithmic problem solving/competitive programming skills correlate with real world development skills. </p> <p>Many companies, notably FANG and large tech companies, must filter through thousands of applicants, so they must cut through the herd in one way or another. It is similar to filtering only those who have a computer science degree. Even though having a degree may or may not translate to being a more competent developer, it is still practical to apply some potentially arbitrary constraint to bring the number of applicants to a managable number.</p> <p>In this short 2 minute video by The Primeagen, a famous youtuber who also is a software developer and hiring manager at Netflix, discusses how he has seen his company move away from focusing on hard technical, Leetcode-like problems and towards basic data strucures (linked list, heaps,graphs etc.). Secondly he goes on to explain the importance of having a visible project on Github that actually solves a real problem.</p> <p>Article about Google conference mentioning why competitive programmers make worse developers as they are used to cranking solutions out fast instead of reflective thinking over problems.</p> <p>Topic starts at 1:11:00 in the video</p> <p>\"being a winner at programming contests was a negative factor for performing well on the job\"</p> <p>Overall, assuming competetive programming/leetcode skills correlate with real talent as a developer is the same as assuming spelling bee champions make the best writers and journalists.</p> <p>The entire fact that there are courses and services around passing interview problems is indicitive of the fact they are heavily flawed. If I spent all my time working on personal projects, or contributing to an open source project writing real software, I may still perform poorly on a question about inverting a binary tree or some other obscure problem. This is because these problems are very rare outside of whiteboarding style interviews or data structure classes in university. Real world development involves navigating around large codebases, writing understandable, well commented code and documentation, as well as communication. Above all, software development is about solving real world problems and helping people. Solving obscure puzzles to impress an interviewer accomplishes neither of these things.  </p>"},{"location":"The%20Notion%20of%20Abstraction/","title":"The Notion of Abstraction","text":"<p>Abstraction is hiding information to simplify a set of instructions. For example, you are not aware of the inner workings of the internal combusion engine or electrical system in your car. Your instructions are to turn on the ignition, switch the gear and press the pedal to go. No information or instructions are necessary pertaining to the inner workings of the car. Even further abstraction would be someone telling you to come to their house. Getting in your car, turning on the ignition and all the rest is implied with this single instruction. </p> <p>With computers, there are many layers of abstraction one must understand. From the CPython bytecode that is excecuted by C, to the assembly code that is further generated from the C compiler, layers, or steps (whichever you like to visualize), are present. To be most productive, one must hide this information as much as possible and only worry about theory when the machine stops doing what its supposed to. This is top-down design. However, for learning sake and gaining an intuitive understanding of software, bottom-up learning is essential. </p> <p>Legendary computer scientist Yale N. Pratt and creator of LC-3,discusses the notion of abstraction in his book, which is the reading material in many universities.</p>"},{"location":"Web%20Servers/","title":"Web Servers","text":"<p>\ud83d\udcdd\ufe0f Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also see the video Web Servers and Rise of Javascript and NodeJS.</p>"},{"location":"Web%20Servers/#overview","title":"Overview","text":"<p>To understand modern web servers and rise of dynamic applications with asynchronous Javascript, we have to understand the web before any such thing existed and to find a point in history to begin.   </p> <p>Since a comprehensive history of the web would require going back 30 plus years to the original invention of the world wide web, early browsers such as Mosaic, Netscape, and even more ancient web servers, we will begin in 1995 with the release of Apache HTTPd, which after nearly 3 decades, still powers over 23% of the top million busiest websites.</p>"},{"location":"Web%20Servers/#apache","title":"Apache","text":"<p>The original Apache developers, started as a group of webmasters, now an outdated term, at the National Center for Super computing Applications or NSCA, where much of the pioneering of the early internet began. The previous web server NCSA HTTPd, needed several patches and improvements, and eventually, Apache was built as a general overhaul and redesign. Still, 30 years later, the main function of serving static HTML files to clients hasn't changed. Over HTTP, or hyper-text-transfer-protocol, a client, aka web browser, issues a GET-request and a server responds with HTML files and other resources that make up the web page presented to the user.</p> <p>For this simple purpose, Apache did great, however, with the growing popularity of the internet, and more and more people watching porn, looking at cat pictures, and ordering dog food, the issue of scalability arose. A single server running can only handle so many requests per second as well as concurrent connections. This become known as the C10K problem, the major challenge for computing in the late 90s and early 2000s where handling over 10 thousand concurrent connections was impossible. Only one man was capable of solving this problem, a Russian system administrator by the name of Igor Syostev. </p>"},{"location":"Web%20Servers/#nginx","title":"Nginx","text":"<p>Responsible for running servers for the top websites in Russia at the time, as well as Rambler, a Russian search engine and web portal, Igor experienced the scalability issues of Apache first hand. In his spare time, he began writing drafts for NGINX in 2002, focusing on handling many simultaneous connections with an asynchronous event driven approach. After 2 years, it was publically released in 2004. Fast forward to 2019 and 66% of top 10k sites use NGINX according to W3 Techs. There are of course several other roles it plays besides serving static content such as HTML and other resources to clients. One, being a reverse proxy, which acts as a gateway between the mass of clients and web servers. This acts as an intermediary load balancer that also can enforce security by directing traffic through firewalls.</p> <p>With Apache and Nginx being used primarily as web servers at the time, Javascript and event driven programming was growing in popularity. Despite this, sequential programming and blocking I/O was still a problem with web servers. For example if a server is waiting for one function of code to finish, this may block user requests or database queries, slowing an application down. Inspired by Nginx, American software engineer Ryan Dahl had a solution: a vision of a purely event driven, non-blocking web server. </p>"},{"location":"Web%20Servers/#nodejs","title":"NodeJS","text":"<p>Ryan was inspired by Nginx and suggests scenarios where they could be used together, Apache on the other hand he criticized for it's limited design where an entire thread is created per connection.</p> <p>Ryan explains pitfalls of blocking, sequential programming and cultural bias where functions with callbacks are seen as too complicated and as nested \"spaghetti code\" in this excerpt.</p> <p>Though Node JS has proven to be highly performant and reliable, it may come at a cost of complexity and code readability, with many preferring threaded servers and sequential code where you can easily read a file top to bottom without jumping between Javascript's callback functions.</p> <p>After these early criticisms of Javascript, there were improvements such as Promises in 2015 with the release of ES6. Promises are a abstraction build on top of callbacks to better handle async code and prevent callback hell. Finally the latest evolution is async/await which is essentially syntactic sugar for promises, completely optional and fully compatible with promises.</p> <p>So clearly, the history of asynchronous JavaScript is a bit messy, but that's the story of any technology that's been around for 25 plus years in an ever changing landscape such as the web.</p> <p>Besides the async/await pattern found in almost every major language today, another model of concurrency that seems promising is in the Go programming language with it's goroutines and channels which has proven useful in modern web servers in recent years. But that's a topic another video.</p> <p>Thank you for watching and don't forget to like, comment and subscribe to support more content like this.</p>"},{"location":"Web%20Servers/#additional-unorganized-notes-or-not-included-in-video","title":"Additional Unorganized Notes Or Not Included In Video","text":"<p>So to wrap everything up, the earliest web server still in wide use today is Apache, which saw limitations in scalability, known as the C10K problem, NGNIX came as a solution with it's asynchronous, even driven approach. As the web evolved, the demand for... interactive web pages, non-blocking, </p> <p>Explain promises and then async/await a bit more. Then summarize everything, maybe somehow slip in Go's concurrency compared to async await. Perhaps hype up some a concurrency model different than async/await created from the greatest engineers in the world who created C and Unix, just mention it and then leave them on a cliffhanger and say that will be a future video. </p>"},{"location":"Web%20Servers/#additional-notes","title":"Additional Notes","text":"<p>2012: Apache 2.4 released with larger selection of MPMs, reverse proxy impovements, and additional load balancing mechanism according to Timeline</p> <p>Apache has Prefork and Worker MPM (Multi Processing Module)</p> <p>Though primarily a Javascript runtime, like the JVM is for Java, or CPython is to Python, NodeJS can be used to create a web server, and very often is. And in that sense, comparable to Apache and Nginx.</p> <p>For serving static content, Apache or NGINX do fine on their own, for dynamic server-side functionality like interacting with databases, or a real time chat application, Node shines best. </p> <p>Sometimes Node and Nginx can be used together, which node containing the code base of an application, while Nginx sits infront handling load balancing and caching.</p> <p>Ryan Dahl in 2010: \"Apache creates a thread per connection\" link</p> <p>What is the difference between nodejs and nginx? See Reddit post. NodeJS vs Nginx Reddit</p>"},{"location":"Why%20Learn%20Vim/","title":"Why Learn Vim","text":"<p>\ud83d\udcdd\ufe0f Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also see the video  Why and How to Learn Vim/Neovim.</p>"},{"location":"Why%20Learn%20Vim/#history","title":"History","text":"<p>Vim is a terminal-based text editor that dates back to it' predecessor, Vi, first released in 1976. At this time, during Unix and early days of MSDos, text based interfaces were predominant, So Vi's development was influenced by the environment of that era, where interacting with computers was done mostly with keyboards rather than mice. It wasn't until the 1980s with the rise of Graphical User Interfaces where Mice become popularized. Despite this, Vim, which stands for Vi improved, and's it's successor Neovim,  remain popular, with people choosing to ditch mice  for full control from their keyboard. </p> <p> Vi dates back nearly 50 years, with Neovim (2015) as it's most recent successor.</p>"},{"location":"Why%20Learn%20Vim/#why-vim-is-so-popular","title":"Why Vim is So Popular","text":"<p>It's popularity is likely due to two reasons. The first being a practical reason, that, when surpassing the inevitable learning curve, you eventually become more productive at navigating through files and directories, editing text files, creating macros to automate tasks, and so on. Besides the utility, the configuration of Vim and Neovim appeals to a particular audience, those who enjoy modifying and sharing their Vim and Bash RCs, or configuration files with others. There's a certain itch for people that's scratched when they have full granular control over applications, this is what some people call a PDE, or Personalized Development Environment, as opposed to a regular IDE, or Integrated Developer Environment, such as PyCharm and VScode. </p> <p>I happen to fall in into the former utility based group, so although I can't show you fancy ways to customize or install any additional plugins, what I can demonstrate is straightforward examples of Vim being more efficient than the typical clicking and copy/paste with your mouse.</p>"},{"location":"Why%20Learn%20Vim/#netrw","title":"Netrw","text":"<p>Firstly we have that weird screen that opens up when you accidentally run vim on a directory instead of a file. That is netrw, or network read and write. As the name suggests, it was originally intended for reading and writing files over a network, but most use it for local browsing to have have your own file explorer all within Vim. Lately I much prefer it over the traditional GNU CoreUtils, instead of moving or renaming a file with mv,  sorting with various ls flags, or typing a command longer than I'd like, in netrw, I'm one key away. Toggling with with i until we reach the tree view, allows me to toggle what's inside each directory. If I find a file I want to inspect, I'll hit v to open a visual split, when done, I could either ZZ, or :wq to save and quit or CTRL + w + q for window quit.</p> GNU CoreUtils Netrw Description mkdir d Make directory touch % Create empty file rm or rmdir D Remove file or directory mv [file or dir] R Move or rename file or directory ls -S, -t s Sort by size, time, etc. cd [dir] [Enter] Move into directory cd .. - move up a directory <p> Inside Netrw</p> <p>See full video for more Vim hotkeys and shortcuts.</p> <p>So although all of these are a lot to memorize, with actual practice, muscle memory quickly takes over, and quickly managing files and directories, and editing files, becomes effortless from the keyboard. Your hands will remain more stationary and your eyes fixed on the screen, moving them less often to find your mouse, in a brief, periodic, and distracting instant.</p>"},{"location":"Why%20Learn%20Vim/#vim-in-vscode","title":"Vim in VSCode","text":"<p>Unfortunately, when working collaboratively at your job, you will likely be the odd one out using Vim instead of VSCode. Many jobs even require you to use a certain IDE with their list of extensions. Luckily in my case, VScode has a Vim emulator that behaves just as you'd expect. The only downside is the occasional conflict of hotkeys. </p> <p> Vim Emulator Extension in VScode</p> <p>So overall, given Vim's long history as an effective editor, it will likely stick around for many more years in the future, and therefore worth investing your time in learning. For nearly 50 years, the same insert, normal, command mode and basic motions have stuck around, and are now portable across IDEs in the form of extensions and plugins. Even my note taking app, Obsidian, has a optional Vim mode built in. </p> <p>So even if you don't want to install and configure vim or neovim, chances are, an application you're using now, supports basic motions and keybinding, which is a great way to practice, </p> <p>If you liked this video, consider subscribing, liking or commenting to support the channel and more content like this. Thanks for watching.</p>"},{"location":"Bash/Backticks/","title":"Backticks","text":"<p>Back ticks in bash are for command substitution, which is when you want to capture the output of a command, and store it in a variable.</p> <p>For example, running the <code>date</code> command in any Linux terminal will output the date in a long format. <pre><code>date\nWed Sep 13 06:41:48 AM CDT 2023\n</code></pre></p> <p>However, if we create a file <code>backticks.sh</code> and wanted to store that output in a special format inside our own variable, we can do that within back ticks. <pre><code>#!/bin/bash\nmy_date=`date +%m-%d-%Y`\necho \"You accessed this date on $my_date\"\n</code></pre> Output <pre><code>You accessed this date on 09-13-2023\n</code></pre></p> <p>Now let's get a little more advanced. First lets create three files, then write the text \"secret\" to the second file. We can search each file with <code>grep [options] [file]</code> and use the wildcard/star <code>*</code> to find all 3 files. We can see the output of <code>l</code> omits the regular output and just prints the file which contains the text, that's what we want. <pre><code>touch file1 file2 file3; echo secret &gt; file2\ngrep -l secret file*\n</code></pre></p> <p></p> <p>Let's write a script that uses the output of the second grep to automatically open our text editor with the file that contains the secret. Using any text editor,  wrap that command in the back tick.</p> <pre><code>#!/bin/bash\nvim `grep -l secret file*`\n</code></pre> <p>Now running the script with <code>bash backticks.sh</code> is equivalent to <code>vim file2</code> or opening whichever file has the secret. </p> <p>For a third example, I want to count how many markdown files are in my Obsidian vault, which contains all of my notes. Before we capture any new command in back ticks, let's first understand what we'll be using. <code>find</code> is a very useful tool in Linux an does just what it says, goes and finds files and directories. It has loads of options and it's man page is over 1,000 lines long, but we'll just stick with <code>-type f</code> to specify we are looking for files and <code>name \"*.md\"</code> to only return markdown files.</p> <p>If you would like to follow along, you can either find your own directory with all of the same file endings, or just <code>touch</code> a bunch of text files and replace the target directory and <code>name</code> with <code>\".txt\"</code>.</p> <p>After running the <code>find</code> command with the correct<code>type</code> and <code>file</code> options, I get very verbose output. </p> <p></p> <p>Next, since I know each instance of a markdown file is separated by a newline which I can use word count <code>wc</code> with the <code>-l</code> option to print the number of newline counts.</p> <p></p> <p>OK, we finally have what we want, now let's put this in a bash script. First, what we had before was recursive by default, meaning it continuously searched directories withing directories, but we can give the user the option by setting the <code>-maxdepth</code> to 1. This entire command is then wrapped in back ticks to be stored in the <code>count</code> variable.</p> <pre><code>read -p \"Recursive? y/n &gt; \" option \nread -p \"Enter directory path &gt; \" directory  \nif [ \"$option\" == \"y\" ]; then     \n    count=`find \"$directory\" -type f -name \"*.md\" | wc -l` \nelif [ \"$option\" == \"n\" ]; then     \n    count=`find \"$directory\" -maxdepth 1 -type f -name \"*.md\" | wc -l` \nelse     \n    echo \"Invalid option.\" \n    fi  \necho \"Count of .md files: $count\"``\n</code></pre>"},{"location":"Bash/More%20Bash%20Examples/","title":"More Bash Examples","text":""},{"location":"Bash/More%20Bash%20Examples/#add-text-to-all-files","title":"Add text to all files","text":"<p><pre><code>#option 1\nfor filename in *.md; do\n    echo \"for loop test\" &gt;&gt; \"$filename\";\n done\n\n# option 2\necho \"my text\" | tee -a *.md\n</code></pre> Named after the T-splitter in plumbing, <code>tee</code> \"splits\" to the standard output and the file to write to. The <code>-a</code> is for append, rather than overwrite. So the for loop and this command do the same thing, <code>tee</code> just prints out \"my text\" on top of appending the files. </p>"},{"location":"Bash/More%20Bash%20Examples/#for-loop","title":"For Loop","text":"<pre><code>for i in {1..5}\ndo\n\u00a0\u00a0\u00a0echo \"Welcome $i times\"\ndone\n</code></pre>"},{"location":"Bash/More%20Bash%20Examples/#make-three-files-and-search-all-with-grep-then-remove","title":"Make three files and search all with grep , then remove","text":"<pre><code>touch testfile1; echo I am file 1 &gt; testfile3\ntouch testfile2; echo I am file 2 &gt; testfile2\ntouch testfile3; echo I am file 3 &gt; testfile3\ntouch testfile4; echo I am file 4 &gt; testfile4\n# OR\nfor i in 1 2 3 4\ndo\n  touch \"testfile${i}\"\n  echo \"I am file ${i}\" &gt; \"testfile${i}\"\ndone\n\ngrep file testfile1 testfile2 testfile3\n\nrm -v testfile*\n#OR\nrm -v te?tf?l?*\n# ? is a single character wildcard\n</code></pre>"},{"location":"Bash/More%20Bash%20Examples/#cat-concatonate","title":"Cat (concatonate)","text":"<p><pre><code>#concatonate files\necho \"This is file1.\" &gt; file1.txt\necho \"This is file2.\" &gt; file2.txt\ncat file1.txt file2.txt &gt; combined.txt\n\n#print to stdout\ncat combined.txt\n</code></pre> <code>cat</code> can both be used to concatenate files or print to standard out (stdout), which is where you see it more commonly used.</p>"},{"location":"Bash/More%20Bash%20Examples/#while-loop","title":"While loop","text":"<p>Simple CPU monitor that prints out the load average every 3 seconds continuously.</p> <pre><code>while true; do\nclear\ncat /proc/loadavg\nsleep 3 \n</code></pre>"},{"location":"Bash/More%20Bash%20Examples/#vs","title":"&amp;&amp; vs ;","text":"<ul> <li>semicolon runs sequentially</li> <li>&amp;&amp; runs only if the previous on succeeds <pre><code>asdlkfjaaasldkfqwkler ; pwd\n# will run\n\nalasdfjanmsdqwuer &amp;&amp; pwd \n# will NOT run\n</code></pre></li> </ul>"},{"location":"Bash/More%20Bash%20Examples/#pretty-colors","title":"Pretty Colors","text":"<p>Green <pre><code>echo -e \"\\033[32;1mHello\"\n</code></pre> Red <pre><code>echo -e \"\\033[31;1mHello\"\n</code></pre></p>"},{"location":"Bash/More%20Bash%20Examples/#downloading","title":"Downloading","text":"<ul> <li><code>tar</code>: only supports .tar files<ul> <li>historically from tape archive, the name somehow stuck</li> </ul> </li> <li><code>wget</code>: World Wide Web get<ul> <li>primarily for downloading</li> </ul> </li> <li><code>curl</code>: Client URL<ul> <li>supports more protocols than wget (SCP, SFTP, SMB)</li> <li>often considered better for scripting due to its return codes and more output options.</li> <li>can download and upload</li> </ul> </li> <li><code>dpkg</code>: Debian packages<ul> <li>for .deb packages on Debian systems (Ubuntu, Linux Mint) </li> </ul> </li> <li><code>apt</code>: advanced package tool<ul> <li>will install all dependencies, not just single package</li> </ul> </li> </ul>"},{"location":"Bash/Python%20vs%20Bash/","title":"Python vs Bash","text":""},{"location":"Bash/Python%20vs%20Bash/#overview","title":"Overview","text":"<p>Python and Bash are both powerful languages and well worth learning. Before learning either, you should understand where they are and are not comparable. Unlike Bash, Python is used for data science, machine learning, and back end web development. Bash, being the default shell on Linux, can be ran directly from the terminal. If you've ever used Linux, the common \"Linux\" commands (historically UNIX programs)  such as <code>cd</code> or <code>ls</code> are really just writing bash directly in the terminal. This cannot be done with Python, which requires a <code>.py</code> file which must interpreted by a runtime. </p> <p>Basically, this means Bash scripts can often be more concise for simple tasks, but Python offers greater flexibility and is easier to integrate into larger projects. The two common libraries to do this are the os and pathlib library. Both serve to interact directly with you operating system, whether on Windows, Macos, or Linux. The pathlib is a more modern, object oriented library with more concise syntax. The os library is lower level with broader capabilities, such as process management and environment variable access. pathlib is exclusively designed for file and directory management, making it the better option.</p> <p><code>os.listdir</code> scans through the directory, and a list comprehension filters out files ending with <code>.md</code>. The <code>os.walk</code> function uses a recursive search, navigating through all sub directories. Notice how verbose and nested the function bodies of these functions are compared to pathlib.</p>"},{"location":"Bash/Python%20vs%20Bash/#python","title":"Python","text":""},{"location":"Bash/Python%20vs%20Bash/#os-library","title":"OS Library","text":"<pre><code>import os\n\ndef count_md_files(directory):\n    return len([f for f in os.listdir(directory) if f.endswith('.md')])\n\ndef count_md_files_rec(root_dir):\n    count = 0\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n        for filename in filenames:\n            if filename.endswith('.md'):\n                count += 1\n    return count\n\noption = input(\"Recursive? y/n &gt;\")\ndirectory = input(\"Enter directory path &gt; \")\n\nif option == 'y':\n    print(\"Number of .md files:\", count_md_files_rec(directory))\nif option == 'n':\n    print(\"Number of .md files:\", count_md_files(directory))\n</code></pre> <p>pathlib uses <code>Path().glob</code> for a non-recursive search and <code>Path().rglob</code> for a recursive search. Notice from the <code>option</code> assignment to the end of both files they are exactly the same. The function body is all that is different. With the pathlib functions, it is much more concise, with one line of code each. </p>"},{"location":"Bash/Python%20vs%20Bash/#pathlib-library","title":"Pathlib Library","text":"<pre><code>from pathlib import Path\n\ndef count_md_files(directory):\n    return sum(1 for _ in Path(directory).glob('*.md'))\n\ndef count_md_files_rec(directory):\n    return sum(1 for _ in Path(directory).rglob('*.md'))\n\noption = input(\"Recursive? y/n &gt; \")\ndirectory = input(\"Enter directory path &gt; \")\n\nif option == 'y':\n    print(\"Number of .md files:\", count_md_files_rec(directory))\nelif option == 'n':\n    print(\"Number of .md files:\", count_md_files(directory))\n</code></pre>"},{"location":"Bash/Python%20vs%20Bash/#bash","title":"Bash","text":"<p>Now we have seen the python files, let's try the exact same task in Bash. As mentioned in the beginning, the fundamental difference is that Bash is not only a language, but also the shell, which is just the interface between the user and the kernel and executes programs called commands. This shell, bash being the most common, is accessed through the terminal. And through the terminal, any commands, even a series of commands at once (a script) , can be run directly in the terminal without needing to create a separate file. This means you have more direct and immediate access for troubleshooting, administration, or short tasks such as managing files, in this case, counting <code>.md</code> files.</p> <p>Each of these, one counting recursive by defualt, the other with a <code>maxdepth</code> of 1, can be ran directly in the terminal, getting immediate results. We specify the <code>-type f</code> for files, as opposed to directories, or anything else. We use <code>-name \"*.md\"</code> to acces only files with a name that ends in <code>.md</code>, the asterisks is just a wildcard, for any character or series of characters. We then pipe <code>|</code> that output to the word count<code>wc</code> program, which despite the name, is a more general program that can count many things, including in this case, lines with the <code>-l</code> flag. </p> <pre><code># the find command in recursive by default\nfind ~/documents/sync-main/todo/ -type f -name \"*.md\" | wc -l\n# set maxdepth 1 to only search the given directory.\nfind ~/documents/sync-main/todo/ -maxdepth 1 -type f -name \"*.md\" | wc -l\n</code></pre> <p>If we want a more interactive program that prompts a user, like the python scripts, we can <code>read</code> a variable called <code>option</code> and <code>directory</code> with a prompt specifies with the <code>-p</code> flag. Then run some basic if-else statements. If we really wanted to make it exactly like the python files, we could use functions, which Bash supports, but I found this to be more concise for this tutorial.</p> <pre><code>#!/bin/bash\n\nread -p \"Recursive? y/n &gt; \" option\nread -p \"Enter directory path &gt; \" directory\n\nif [ \"$option\" == \"y\" ]; then\n    find \"$directory\" -type f -name \"*.md\" | wc -l\nelif [ \"$option\" == \"n\" ]; then\n    find \"$directory\" -maxdepth 1 -type f -name \"*.md\" | wc -l\nelse\n    echo \"Invalid option.\"\nfi\n</code></pre>"},{"location":"Bash/Python%20vs%20Bash/#key-takeaways","title":"Key Takeaways","text":""},{"location":"Bash/Python%20vs%20Bash/#python_1","title":"Python","text":"<ul> <li>Python is used for Data Science, Machine Learning, and Backed web apps, and is only comparable to Bash for scripting purposes.</li> <li>Python offers two main libraries for file management: <code>os</code> and <code>pathlib</code>. <code>os</code> is older and has broader capabilities, while <code>pathlib</code> is modern and focuses solely on file and directory manipulation.</li> </ul>"},{"location":"Bash/Python%20vs%20Bash/#bash_1","title":"Bash","text":"<ul> <li>Bash serves two roles\u2014it's both a scripting language and the default shell for Linux. This gives it more direct access to system functionalities, allowing you to run scripts and commands directly from the terminal.</li> </ul>"},{"location":"Bash/Python%20vs%20Bash/#conclusion","title":"Conclusion","text":"<ul> <li>if you can solve the problem at hand using only calls to command-line tools, you might as well use Bash. But for times when your script is part of a larger Python project, you might as well use Python.</li> </ul>"},{"location":"Bash/Scripts%20From%20A%20Real%20Project/","title":"Scripts From A Real Project","text":""},{"location":"Bash/Scripts%20From%20A%20Real%20Project/#overview","title":"Overview","text":"<p>In this article you will find several bash scripts that I will walk through. These scripts were part of a real project and hopefully you will learn more with a real life example than a simple tutorial, but first we need context. </p> <p>Obsidian is a markdown based note taking app with a large ecosystem of over 1,000 3rd party extensions written by open source developers. Having my own idea of tracking disk usage by folders, filetypes and plugins, I decided to create my own. On Linux, I had Bash ready at my command to easily write these scripts, since Obsidian is all markdown based, all I would need to do is write all of this data to a <code>.md</code> file, such as the following:</p> <p></p> <p>This would then render a chart using Mermaid inside of Obsidian.</p> <p></p>"},{"location":"Bash/Scripts%20From%20A%20Real%20Project/#disk-usage-by-folder","title":"Disk Usage by Folder","text":"<p>To start the bash script, we must first learn about the <code>du</code> (disk usage) command. <code>du</code> followed by a directory will output the directory and size in bytes. I only want to display the first level of directories, so I will use the depth <code>-d</code> flag telling it to only go 1 level deep.</p> <pre><code># du -d 1 sync-main/\n\n552     sync-main/Academic\n968     sync-main/Notes\n5672    sync-main/.obsidian\n232     sync-main/Personal\n72      sync-main/Military\n12      sync-main/Daily\n188     sync-main/Professional\n364     sync-main/todo\n8072    sync-main/\n</code></pre> <p>Although you can use the human readable <code>-h</code> flag to read kilobytes or megabytes, we will need the raw bytes as a universal measure which will calculate the percents on the chart you see above.</p> <p>The problem is, the output will look a bit different. You see, this script sits in a plugin folder which is exactly 3 layers below where we need to measure <code>du</code> from, which is the root of the Obsidian vault.</p> <p><code>[ROOT-OF-VAULT]/.obsidian/plugins/disk-usage/script.sh</code></p> <p>This measures from the root of the vault. <pre><code># du -d 1 ../../../\n\n552     ../../../Academic\n968     ../../../Notes\n5672    ../../../.obsidian\n232     ../../../Personal\n72      ../../../Military\n12      ../../../Daily\n188     ../../../Professional\n364     ../../../todo\n8072    ../../../\n</code></pre></p> <p>Now let's pipe that output into <code>sed</code> (Stream EDitor) and clear up the slashes first.  <pre><code># du -d 1 ../../../ | sed 's|[/]||g'\n\n552     ......Academic\n968     ......Notes\n5672    .......obsidian\n232     ......Personal\n72      ......Military\n12      ......Daily\n188     ......Professional\n364     ......todo\n8072    ......\n</code></pre></p> <p>Next let's remove the dots. <pre><code># du -d 1 ../../../ | sed 's|[/]||g' | sed 's/\\.\\{2,\\}//g'\n\n552     Academic\n968     Notes\n5672    obsidian\n232     Personal\n72      Military\n12      Daily\n188     Professional\n364     todo\n8072    \n</code></pre></p> <p>I don't care to include the <code>.obsidian</code> hidden folder, so I will remove it with <code>grep -v</code>.  Also I don't need the total at the bottom so I can remove it wil <code>head -n -1</code>. These commands are also getting long so I will create a script for it. I'll give excecutable permission with <code>chmod</code> and excecute. <pre><code>#!/bin/bash\ndu -d 1 ../../../ | sed 's|[/]||g' | sed 's/\\.\\{2,\\}//g' | grep -v 'obsidian' | head -n -1\n</code></pre></p> <pre><code># chmod +x script.sh\n# bash script.sh\n\n552 Academic\n968 Notes\n232 Personal\n72  Military\n12  Daily\n188 Professional\n364 todo\n</code></pre> <p>Now we can <code>read</code> each line into an associative array. This is just key value pairs, similar to a dictionary in Python. The <code>while read</code> will read the input line by line storing each value in <code>numbers</code> and <code>name</code> respectivily, storing the name as key and numbers as values. The input source for the while loop is generated from <code>&lt; &lt;</code> followed by the commands from earlier.</p> <pre><code>declare -A myArray\nwhile read number name; do\n  myArray[\"$name\"]=$number\ndone &lt; &lt;(du -d 1 ../../../ | sed 's|[/]||g' | sed 's/\\.\\{2,\\}//g' | grep -v 'obsidian' | head -n -1)\n</code></pre> <p>Now remember the chart from before? Let's begin by writing to a file with <code>&gt;</code> and then appending with <code>&gt;&gt;</code>. We then loop through the keys and values of the associative array, appending the folders and their respective size. </p> <pre><code>echo '```mermaid' &gt; folders.md\necho 'pie title Disk Usage by Folder' &gt;&gt; folders.md\n\n# Add pie chart data to Markdown file\nfor key in \"${!myArray[@]}\"; do\n  echo \"    \\\"$key\\\" : ${myArray[$key]}\" &gt;&gt; folders.md\ndone\n\n# Close mermaid code block\necho '```' &gt;&gt; folders.md\n</code></pre> <p>Now we should have the same pie chart from before! </p>"},{"location":"Bash/Scripts%20From%20A%20Real%20Project/#disk-usage-by-file-type","title":"Disk Usage by File type","text":"<p>Now what if we wanted to find data by file types? For example, my vault could be slow while loading, and this could be due to limited space, so I then want to find the culprit. Could it be images, pdfs? A chart like this would help.</p> <p></p> <p>What we want to do is <code>find</code> all files of a specific type and then pipe that into <code>du</code> with the total <code>-c</code> flag. By default, the find command seperates the output with newlines, the du command will only work if they are null terminated (\\0) with <code>-print0</code>. The next odd looking flag, <code>--files0-from=-</code> basically says read a list of null-terminated file paths from standard input. <code>-</code> is stdin, <code>F</code> would reading from a file.</p> <pre><code># find ../../../ -name '*.md' -print0 | du -c --files0-from=-\n\n#[ommitted files for brevity]\n4   ../../../todo/Calendar/Drill/2023-09-09 Muta 6.md\n4   ../../../todo/Calendar/CSC155L/(Every R) CSC155L.md\n4   ../../../todo/Calendar/MUS100/2023-10-22 asdf.md\n4   ../../../todo/Calendar/MUS100/2023-10-03 Music Midterm.md\n4   ../../../todo/Calendar/MUS100/(Every R,T) MUS100.md\n4   ../../../todo/Calendar/CSC155/(Every R,T) CSC155.md\n4   ../../../todo/Calendar/THC Cutoff/2023-07-27 Go clean.md\n4   ../../../todo/st.md\n4   ../../../todo/scratch.md\n1800    total\n</code></pre> <p>We will get a very verbose output. The very last line is all we will need. <code>tail -1</code> will print just the last line and <code>awk '{print $1}'</code> will just print the first field of the line, which is 1800.</p> <pre><code>#find ../../../ -name \"*.md\" -print0 | du -c --files0-from=- | tail -1 | awk '{print $1}'\n\n1800\n</code></pre> <p>So now we have some commands strung together that can find the disk usage that any particular file type is taking up in a given directory. Let's now create a function with a local variable <code>file_type</code> which will be the first parameter. Then <code>disk_usage</code> will use command substitution to wrap the previous commands inside the variable. We can then call the function with the file types as arguments. </p> <p><pre><code>get_size() {\n  local file_type=\"$1\"\n  local disk_usage=$(find ../../../ -iname \"*.$file_type\" -print0 | du -c --   files0-from=- | tail -1 | awk '{print $1}')\n  echo \"    \\\"$file_type\\\" : $disk_usage\"\n  }\n\nget_size png\nget_size md\nget_size pdf\n</code></pre> Output <pre><code>\"png\" : 68\n\"md\" : 1800\n\"pdf\" : 0\n</code></pre></p> <p>Next, to generate the chart, we will create an array with every file extension we want, then loop through it while appending to the <code>.md</code> file which will contain the chart.</p> <pre><code>file_types=(\"webm\" \"gif\" \"pdf\" \"json\" \"md\" \"jpg\" \"png\" \"jpeg\" \"excalidraw\")\n\necho '```mermaid' &gt; filetypes.md\necho 'pie title Disk Usage by Filetype' &gt;&gt; filetypes.md\n\nfor file_type in ${file_types[@]}; do\n  get_size $file_type &gt;&gt; filetypes.md\ndone\n\necho '```' &gt;&gt; filetypes.md\n</code></pre> <p>The chart should look something like this which can be rendered inside Obsidian, and other markdown editors that support Mermaid.</p> <p></p>"},{"location":"Bash/Scripts%20From%20A%20Real%20Project/#full-bash-code-and-rewritten-in-javascript-for-obsidian","title":"Full Bash Code and Rewritten in Javascript for Obsidian","text":"<p>After I wrote all this I realized since it required sudo (elevated priviledge) and only would run on Linux, it would be a poor implementation. Instead writing it in Javascript would make it cross-platform as the code would excecute within the Electron cross-platform instance, whether on MacOS, or Android.</p> <p>Since over a year ago since publishing this plugin, it has 2,400 downloads as of writing this and can be seen on the Obsidian Website here. </p> <p>Here are links to the Bash implementation and the Disk Usage plugin repository (Javascript implementation).</p>"},{"location":"Bash/Shell%20Navigation/","title":"Shell Navigation","text":"<p>These are the default Bash terminal commands based on Emacs commands, however, you can set <code>set -o vi</code> to your .bashrc to set Vi mode</p> <p><code>alt + b</code> backward a word <code>alt + f</code> forward a word OR <code>ctrl +</code> left/right arrow</p> <p><code>ctrl + e</code> end of line <code>ctrl + a</code> beginning of line</p> <p><code>ctrl + w</code> kill/delete word until white space <code>alt + backspace</code> kill/delete word until special character - more useful for file trees, to stop at a slash</p> <p><code>ctrl + k</code> kill/cut ahead of cursor <code>ctrl + u</code> kill/cut before of cursor</p> <p><code>ctrl + y</code> paste </p> <p><code>ctrl + r</code> search old search <code>ctrl + R</code> search new commands</p> <p>More tricks on youtube</p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/","title":"Inkscape","text":"<p>\ud83d\udcdd\ufe0f Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also find the video here.</p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#inkscape","title":"Inkscape","text":"<p>I've used many image editing software and there's one in particular that stands out the most. Inkscape has grown on me more than anything else because it's easy to use, yet also has a large number of features where you're always learning something new and constantly improving your workflow. I have used Gimp in the past, but eventually gave up with how difficult it was to learn, not to mention how dated the interface felt. Inkscape seems to have more frequent updates and support and overall better outlook for the future. People often compare Inkscape to Adobe illustrator as they are both SVG based image editors. I have no comment on this as I am proud to say I have never paid for an Adobe product and do not support the company, so I unfortunately cannot give a good comparison.</p> <p>Here is my brain anatomy map in Inkscape I plan on making interactable in a webpage by clicking a region and a side bar appears with information. Also very useful for taking notes for Neurobiology.</p> <p></p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#photopea","title":"Photopea","text":"<p>The next image editor worth mentioning is Photopea, this is a free web based editor I have used in the past to make thumbnails. It is very easy to use and impressive as it was built entirely by one person. The difference between Photopea and Inkscape of course is one is a normal image editor and the other deals with SVG graphics. These are 2d vector graphics based on geometry rather than a fixed grid of pixels. What I like most about SVGs is that they are just a file with XML tags that I have more control over. When I draw a path or add a shape, a path or rect tag is added which I can inspect and have granular control in the XML editor.</p> <p>Though Photopea does not have the XML editing features, it is very handy for creating thumbnails or offensive memes.</p> <p></p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#svgs-for-web-design","title":"SVGs for Web Design","text":"<p>My favorite part of SVGs, and probably their biggest use case, is embedding in websites. Though there are other ways to make diagrams and cool visual graphics like the HTML canvas element, this requires more Javascript than I prefer. Instead, having the entire graphic be composed of elements in XML to be represented like a document makes more sense to me. Using Inkscape's XML editor, I've added these HTMX attributes to swap the side panel of the selected brain region with the associated HTML file. This is a pretty cool way of building interactive diagrams, maps, timelines, really anything you want. If you can draw it in Inkscape and write some basic Javascript and CSS to add functionality, you can build really anything you want.</p> <p></p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#history-of-inkscape","title":"History of Inkscape","text":"<p>When considering what kind of software to invest your time in learning, weather it be a framework for building websites, a programming language, or a graphics/image editor, one of the things to consider is age. Basically, the longer that something has been around, the longer it is likely to stick around in the future. Inkscape dates back to 2003 as a fork of Sodipodi (see below), a vector graphics tool intended for artists. This makes Inkscape over 20 years old, all while still maintaining popularity.</p> <p></p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#future-of-inkscape","title":"Future of Inkscape","text":"<p>Besides its historic track record, looking towards the future, Inkscape has a positive outlook. I say this because it's active group of developers just recently switched to GTK4 for their development version, or branch. GTK stands for Gnome Tool Kit, the fourth version being the most modern UI framework for Linux desktop apps. This is a major architectural improvement which enables graphics acceleration, better overall performance, and a more modern user interface. This will make Inkscape one of the first relatively high-profile desktop-agnostic apps to use GTK4 in the near future. Though we may not see this until after 1.4 as some issues were found on Windows and MacOS, as you can see in this \"toot\" on Mastodon from inkscape themselves. But, these occurrences are expected and sometimes more time is needed to ensure a stable release. Regardless, I'm impressed by the work so far optimistic for the future.</p> <p></p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/Inkscape/#additional","title":"Additional","text":"<p>https://www.reddit.com/r/linux/comments/1bodw7i/inkscapes_development_version_switches_to_gtk4/</p> <p>https://inkscape.org/news/2023/04/17/inkscape-hiring-accelerating-gtk4-migration/</p> <p>https://www.phoronix.com/news/Inkscape-Switches-To-GTK4</p> <p>https://mastodon.art/@inkscape/112151266538190571</p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/SVG%20Timeline/","title":"SVG Timeline","text":"<p>I am very interested in history of programming languages, Linux distros, and computing in general. I want some way to visually graph all my notes seen in my timeline. At first I read through the SVG docs on Mozilla, and soon realized hand coding SVGs is very tedious and inefficient. I later found Inkscape, a click-and-drag open source SVG editor. Here is what I have so far and progress can be seen on this repo.  </p> <p>So far there is a main timeline of the major events, such as early computing, languages, operating systems, and recent container technology. We will have separate timelines such as modern web quality assurance (QA)/testing frameworks. There will be a internet/web history either starting with ARPANET in 1969 or Tim Berners Lee and the world wide web of the 90s.</p> <p></p> <p>The plan is to click on a single event and a side panel will open to show an article. Similar functionality to this website. There are multiple ways to do this, one, you could store each article as an HTML file on a server and every click of the SVG element would trigger an HTTP GET request and return that article, injecting/swapping whatever was in the side panel previously. Since by default only link and form tags can submit HTTP requests, you would use HTMX to give that ability to SVG rect elements. Also HTMX would gives hx-swap attribute to rects to swap the current content of the side panel with that which was just clicked. </p> <p>This hypermedia, client-to-server approach would be most scalable. However, until the project grows and it is necessary, I will be pushing a single SVG file to the limit by storing the articles as the rect's description attribute added with inkscape. A javascript event listener will be placed on every rect that will swap what is in the side panel all within the browser. As the single SVG grows, the loading speed may be effected and we can switch to a client-server architecture in the future.</p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/SVG%20Timeline/#svg-libraries","title":"SVG Libraries","text":"<p>Using Inkscape makes creating SVGs much easier. What might be even easier is using an SVG framework like Rapheal or Snap Repo. I am reading that Snap Repo is a newer version targeting modern browsers and was written by the same author as Rapheal. Here is a star comparison of the two repos. Also an example of what is possible with these frameworks. It is a map. We want to build a timeline, but it doesn't matter, they are clickable, intractable SVG elements with tooltips.  </p> <p>SVG libraries include Rapheal and Snap.svg. Snap was built by the same author of SVG but meant for modern web browsers. These are possibile frameworks though the current Inkscape approach works fine.</p> <p>Example of rapheal https://www.energy.gov/national-laboratories</p> <p>Snap repo https://github.com/adobe-webplatform/Snap.svg</p>"},{"location":"Javascript%20%26%20SVG%20Web%20Design/SVG%20Timeline/#personal-updates","title":"Personal Updates","text":"<p>UPDATE March 28th 2024: Ok looking back on this project, my Youtube channel, and website, I'm considering starting back up. I just got burned out cause I'm taking 18 credits in college plus an internship.</p> <p>UPDATE Oct 19th 2024: No progress since last update. I don't know if this will go anywhere unless I start making youtube videos about history of technology Asionometry style in which case I would make articles and videos with years associated with them, making a SVG timeline to embed into my website more useful and relavent.</p>"},{"location":"Linux/How%20Linux%20Boots/","title":"How Linux Boots","text":"<p>\ud83d\udcdd\ufe0f Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also find the video here.</p> <p>Understanding the Linux boot process is a complex task, but knowing a few major steps and concepts is possible for anyone. If you inspect your file system at the root level, you'll see you have a boot directory, upon inspecting your block devices, you'll find that it may be a separate partition altogether. Looking at the contents, you'll see each type of file have different versions according to each kernel release. Currently I'm on 6.8.9, my Fedora machine stores two previous versions in case I ever need to roll back a version due to a system breaking kernel update.</p> <p>After the firmware has check and initialized hardware components with a Power on Self Check, or POST test, it looks for a boot device on a hard drive or SSD where the bootloader, GRUB is stored, which the firmware hands over control to. The next step involves Vmlinuz, which stands for Virtual Memory LINUx gZip, meaning it's a gzip-compressed kernel image. </p> <p>If you run <code>journalctl</code> with the -boot flag, you can see uncompressing this image into memory is the first step. The next step is initramfs, or initial ram filesystem. This loads a temporary filesystem into memory to load just enough kernel modules to recognize the full kernel in order to mount the real filesystem, begin systemd as the init service, and continue into userspace.</p> <p>These steps can be viewed in a summarized way with <code>systemd-analyze</code> starting with exactly how long the firmware took to initialize the hardware, around 10 seconds, 2 and a half seconds for grub, the bootloader, 1 second for uncompressing the kernel, 5 for loading the temporary filesystem, and 12 seconds for systemd to start up every process necessary for userspace. </p> <p>An even better way to graphically visualize each step is the run <code>systemd-analyze plot</code> and direct the output towards a created SVG file. Here I open it up with Inkscape and have a comprehensive chart of every process. It looks complicated but as I mentioned, it just starts with firmware, the bootloader, uncompressing the kernel, our temporary filesystem which loads all these necessary modules, and next our very first process, or PID 1, SystemD, which spawns all other processes.</p> <p>This can be visualized with process tree, where you can see every ongoing process on your machine and it's parents, all eventually spawning from systemd. Also as seen in the pseudo or virtual filesystem proc, which displays each ongoing process as a directory in real time, here we can peak at what's inside.</p> <p>Keep in mind this video is not comprehensive and I'm leaving out many details of about early hardware stages like BIOS and EUFI, instead I hope to show some useful terminal commands and tips to explore yourself. Probably the most useful thing you can take from this video is the SVG chart created from <code>systemd-analyze</code>, which you should spent some time with exploring yourself.</p> <p>If you like these short videos where I explain a Linux subject leave a comment, like, or subscribe to support more content like it. Thanks for watching.</p>"},{"location":"Linux/Linux%2B%20vs%20LPIC/","title":"Linux+ vs LPIC","text":"<p>The first Linux certification to discuss is Linux+ offered by CompIA (Computing Technology Industry Associate), a more general IT certification association that offers everything from help desk support do data analyst certifications. The second certification worth comparing is the LPIC-1, offered by the LPI (Linux Professional Institute), an organization which focuses on Linux in and of itself. Although after passing the LPIC-1, there are additional certifications that go far more in depth, being the LPIC-2 and LPIC-3, the only certification that matches the depth of knowledge as Linux+ is LPIC-1.</p> <p>First off, let's compare what is similar. Both certifications are vender and distro nuetral, meaning they are not specifically about one Linux distribution offered by one company, such as Redhat offering certifications covering their REHL distibution (RHCSA or RHCE). Instead, both the Linux+ and LPIC-1 focus on concepts that apply across all distros, and include tools used by the most common distros, such as RPM (Red Hat Package Manager) as well as APT (Advanced Package Manager), used by Debian-based systems.</p> <p>While reading over the exam objectives for each certification, I noticed the LPIC-1 putting much more weight on basic file management, the shell environment and Bash scripting, while the Linux+, as seen below, puts less emphasis on Bash scripting. What the Linux+ has that the LPIC-1 doesn't however is Git version control, containerization (Docker/Podman) and orchestration (Kubernetes). These are more recent technology trends only found in the Linux+ and often used in an enterprise environment. So we could say the LPIC-1 is a more pure Linux certification and Linux+ focuses on Linux, in addition to the related tools and technology used in enterprise environments.  </p> <p>From what I have read, CompTIA is more widely recognized as a industry standard in the United States, while the LPI is more recognized in Europe. </p> <p>After passing the Linux+ or LPIC-1, the next certifications would either be the RHCSA (Red Hat Certified Systems Administrator) or the LPIC-2.</p> <p>For price, the two LPIC-1 exams each cost $200 in the United States as of writing this. The single Linux+ exam costs $358, although they have a tempting bundle for $462 which included a exam retake and official study guide.</p> <p>The old Linux+ XK0-004 exam is currently expired. The newer XK0-005 version has condensed 5 domains into only 4, and has included Docker/container management and orchestration.</p> XK0-005 System Management 32% Security 21% Scripting, containers, and automation 19% Troubleshooting 28% <p>Key Takeaways</p> <ul> <li>Both are vendor and distro neutral.</li> <li>CompTIA certifications will be far more recognized than LPI. </li> <li>LPI is a specialized Linux organization that has additional certs (LIPC-2 and 3), while CompTIA is a general IT certification company with only one Linux+ cert. </li> <li>Linux+ focuses on more enterprise technology like containers and additional tools like version control (Git) while LPIC-1 is purely Linux focused.</li> <li>Linux+ is a single exam while LPIC-1 is broken into two.</li> <li>CompTIA has a student discount for those in a four year university.</li> </ul> <p>Two Latest Exams Released</p> <p>2018 Oct LIPC-1 version 5</p> <p>2022 July Linux+ XK0-005</p> <p>Exam Objectives</p> <p>Linux+</p> <p>LPIC-1 Exam 101 &amp; 102</p>"},{"location":"Linux/Systemd/","title":"Systemd","text":"<p>Just some notes of mine so far.</p> <p>Here is a blog post titled Rethinking PID 1 in 2010 of experiments with a new init system.</p> <p>Unix System V is as old as 1983.</p> <p>Systemd is a relatively new system manager introduced in 2010 as a replacement for SysV init manager. Despite criticism of feature creep and bloat, Systemd has gained massive popularity in the last decade and is here to stay. Most distros such as Ubuntu and Red Hat ship it by default. </p> <p>Here are various changes introduced in Systemd to be aware of. Though these new features do not necessarily make the traditional tools obsolete, adoption may be the best long term solution.</p>"},{"location":"Linux/Systemd/#cron-jobs-systemd-timers","title":"Cron Jobs &gt; Systemd Timers","text":""},{"location":"Linux/Systemd/#etcfstab-unit-files","title":"/etc/fstab &gt; Unit Files","text":""},{"location":"Linux/Systemd/#run-levels-targets","title":"Run Levels &gt; Targets","text":"<p>Instead of traditional Sysv runlevels, such as switching to Single-User Mode with <code>telinit 1</code>, you would now use <code>systemctl isolate rescue.target</code>.</p> <p>Check out this Sysv to Systemd cheatsheet.</p> Run Level Description Systemd Target 0 Poweroff poweroff.target 1 Single user mode (recovery mode) rescue.target 2 Multi user mode with no networking multi-user.target 3 Multi user mode with networking multi-user.target 4 User Definable multi-user.target 5 \u00a0Multi user mode under GUI (standard in most systemds) graphical.target 6 Reboot reboot.target"},{"location":"Linux/Systemd/#community-feedback-and-opinion","title":"Community Feedback and Opinion","text":"<p>Notes of The Linux Experiment's video:</p> <ul> <li>All Linux based systems use an init system, the first process that starts after you boot your OS and continues to run essential programs and services.</li> <li>Systemd was spearheaded by Red Hat to replace existing Sysv (1983) and Ubuntu's now discontinued Upstart (2006-2015).</li> <li>Critics claim Systemd violates the Unix Philosophy of modularity, separate programs that communicate with each other. Instead, Systemd has become a monolithic, umbrella project that has more functions than traditional init systems were meant for.</li> <li>There is really not much competition to Systemd.</li> </ul>"},{"location":"Linux/Systemd/#unit-timers","title":"Unit Timers","text":"<p>Cron to Systemd Time Format Tutorial</p> <pre><code>\u276f cat foo.timer \n[Unit]\nDescription=Append to file in /tmp every minute\n\n[Timer]\nUnit=foo.service\nOnCalendar=*:*:0\n\n[Install]\nWantedBy=timers.target\n</code></pre> <pre><code>\u276f cat foo.service \n[Unit]\nDescription=Append text to /tmp file\n\n[Service]\nExecStart=/usr/local/bin/systemd-timer.sh\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <pre><code>\u276f cat /usr/local/bin/systemd-timer.sh \n#!/bin/bash\necho \"Systemd at `date`\" &gt;&gt; /tmp/from-systemd.txt\n</code></pre> <pre><code>\u276f cat /tmp/from-systemd.txt \nSystemd at Mon Dec 25 10:40:01 AM CST 2023\nSystemd at Mon Dec 25 10:41:01 AM CST 2023\nSystemd at Mon Dec 25 10:42:01 AM CST 2023\n...\n</code></pre> <p>For more information:<code>man systemd.unit</code>. WantedBy and Required by just create symlinks in .wants/ and .requires/ directories when <code>systemctl enable</code> is called. Wants and Requires are for actual dependencies.</p>"},{"location":"Linux/The%20Rise%20of%20Linux/","title":"The Rise of Linux","text":"<p>\ud83d\udcdd\ufe0f Video Transcript: This is the transcript for a video on my youtube channel. It should still be just as readable as an article, but you can also find the video here.</p> <p>Covering the rise in market share of Linux may seem simple, but it's gets more complicated as you break it down. Since Android is based on the Linux kernel, and global operating system market share has Android at 43%, the number one place, wouldn't that mean Linux already won and we can end the discussion here? Well, when people say Linux, they usually mean common distributions like Ubuntu, Fedora, Arch and so on,  so we'll stick first to the desktop market share rather than mobile, then later dive into Server market share. </p> <p>But before discussing number of Linux users, it's worth mentioning something that could skew these statistics, that is the privacy measures many Linux users take to ensure their personal data is not collected, this very data that SEO and analytics companies use to provide these reports. Ad blockers such as U-block origin will block or filter out tracking scripts that websites use to gather data about users operating systems and other details. Other than custom scripts, this data can be found in your user agent, something that is included in the header of every HTTP Get request. Which, if you're curious, can be found by typing in <code>window.navigator.userAgent</code> in the dev tools, and that is your personal identifier which much of this data is collected from. </p>"},{"location":"Linux/The%20Rise%20of%20Linux/#liberal-estimate","title":"Liberal Estimate","text":"<p>Back to the market share, Stat Counter which is the most liberal estimate, shows Linux desktop worldwide reached just under 4% this year up from 3% in 2023. The increase is more pronounced in the United States at 3.7% up from 2% last year, nearly doubling. Those in India use Linux the most, at 14%, which brings the global average up significantly. </p>"},{"location":"Linux/The%20Rise%20of%20Linux/#india","title":"India","text":"<p>This is due to several reasons, one, since India is a developing country, not everyone can afford a computer with a i9 processor and 30 gigs of ram. Sometimes all they have is an old laptop where installing a Linux distro brings it back to life. Schools and governments adopt Linux to cut costs, as Windows can be expensive due to their licenses, not to mention increased risk of malware and cyber security threats. As a response, the Indian Defense Research and Development Organization  forked their own distribution of Ubuntu called Maya OS. So Linux in India is less expensive, more secure, and widely adopted. This also could be due to many in India choosing fields like engineering, including computer science.  </p>"},{"location":"Linux/The%20Rise%20of%20Linux/#use-friendly","title":"Use Friendly","text":"<p>Another reason for the adoption of Linux worldwide is it's user friendliness compared to 20 year ago. In the late 90s and early 2000s, installing Linux meant purchasing a floppy drive, partitioning hard drive space, and manually configuring and installing X Windows programs from the terminal. Nowadays its much more approachable with a streamlined installation process. In fact, during my last few installs of Fedora, I don't remember even needing to open the terminal once.</p>"},{"location":"Linux/The%20Rise%20of%20Linux/#microsoft","title":"Microsoft","text":"<p>Besides India driving the global market share and Linux being more approachable and user friendly nowadays, the third possible reason is Microsoft just not caring about Windows as much as the 90s and 2000s. Currently, Windows represents only 12% of total revenue, far surpassed by Azure cloud products and services as well as Office subscriptions. </p>"},{"location":"Linux/The%20Rise%20of%20Linux/#conservative-estimate","title":"Conservative Estimate","text":"<p>Anyways, aside from the 4% figure from Stat Counter, we have a more conservative estimate from Steam hovering just below 2%. Now of course these are Steam users where gaming isn't as supported on Linux, as opposed to Windows. Either way, this number doubled since 2020 where where market share was reported at just 0.9%. So whether you are looking at Stat Counter, or Steam's report, Linux desktop usage has been increasing considerably over the past several years.</p>"},{"location":"Linux/The%20Rise%20of%20Linux/#additional","title":"Additional","text":"<p>W3 Techs report shows Unix (which includes Linux) had an increase of operating system for websites from 65% to 85% from 2013 to 2024 while Windows server declined from 35% to 16%.</p> <p>5 Reasons Why Desktop Linux Is Finally Growing In Popularity: zdnet article</p> <p>India</p> <p>Good News! Indian State Aims to Save Over $400 Million by Choosing Linux Why is India\u2019s Defence Ministry ditching Microsoft Windows for Ubuntu-based Maya OS?</p> <p>User Friendliness of Linux in 2004</p> <p>very little has been done to increase its user-friendliness and extend its reach outside highly technical and knowledgeable individuals and those in academic computing environments. Without a reasonable level of user-friendliness, Linux cannot attract a critical mass of users required for its success.</p> <p>PG 19 of This Research Paper https://www.researchgate.net/publication/3248102_Economics_of_Linux_Adoption_in_Developing_Countries</p>"},{"location":"Linux/Top/","title":"Top","text":""},{"location":"Linux/Top/#overview","title":"Overview","text":"<p>Let's say your new to Linux, transitioning from the Windows Task Manager to Ubuntu's System Moniter. This is a relatively easy transition, however, further down your Linux journey you will delve further into the terminal and find alternatives to GUI applications. This alternative is a program called <code>top</code>. Written in 1984, it is tried and tested against time, and will continue to be the default resource monitor in the near future. Despite other programs, such as <code>htop</code>, <code>vtop</code>, <code>powertop</code>, etc., doing essentially the same thing, I would recommend not to over analyze and simply learn the default. <code>top</code> works perfectly fine as an alternative to the System Monitor for an everyday Linux user and can even help down the line with Linux Administration if that's what you are interested in.</p>"},{"location":"Linux/Top/#statistics-and-process-table","title":"Statistics and Process Table","text":"<p>After running the <code>top</code> command, you will get something like this. In two main parts, the statistics are at the top, and the process table at the bottom. At the very top, starting from the left, you can see the up time, which is how long the system has been running. Moving right, the number of users is displayed, then we have load average, which is average CPU usage in the past 1 minute, 5 minutes, and 15 minutes. It is  based on your number of cores. For example if you have 4 cores, then a 4.0 would mean 100% CPU usage, in this screenshot, I have 2 cores so in the past 1 minute I had 63% usage of my 2 cores.</p> <p></p> <p>Below the number of tasks running, you will see the percent of CPU utilization from user processes, the kernel, and processes with a positive niceness value (often manually configured). Next the <code>id</code> is the percent of time idle (if high, CPU may be overworked). Lastly <code>wa</code> is the percent of wait time (if high, CPU is waiting for I/O access).</p> <pre><code>%Cpu(s):  1.7 us,  0.6 sy,  0.1 ni\n</code></pre> <p><code>id</code>, <code>wa</code>, and <code>st</code> help identify whether the system is overworked. Here is a total  Now, let's go over each column in the process table.</p> <ul> <li>PID: Process ID.</li> <li>USER: The user running the process.</li> <li>PR: Priority of the task computed by the kernel on a scale of 20 to -20.</li> <li>NI: \"Niceness\" value, which involves the priority of user processes. 0 is the default and highest priority.</li> <li>VIRT: Virtual and physical memory, representing the total memory space allocated to a process, including RAM and swap. It's like a hypothetical maximum needed by the program. RES + Swap space = VIRT</li> <li>RES: Resident (Physical) memory used by the process. VIRT - Swap space = RES</li> <li>SHR: Shared memory.</li> <li>S: State of the process, where \"R\" means running, \"S\" means sleeping, and \"I\" is idle.</li> </ul>"},{"location":"Linux/Top/#htop","title":"htop","text":"<p>The most popular alternative, <code>htop</code>, provides more customization, scrolling and mouse support, color, and an overall cleaner interface. Unlike <code>top</code>, it does not come preinstalled but is worth checking out with a quick download.</p> <pre><code>sudo apt install htop\n</code></pre> <p>I recommend to learn <code>top</code> first and then try out <code>htop</code>, comparing the two. </p> <p>Upon running <code>htop</code>, you will first realize the displayed columns discussed above are exactly the same, as well as the tasks, load average, and up time at the top right. The main difference is the colorful, more readable TUI (text user interface) that supports mouse events and scrolling. An example of this is the CPU column, colored blue after clicking, ordering the processes by CPU consumption. </p> <p></p>"},{"location":"Linux/Top/#additional","title":"Additional","text":"<ul> <li>Mental Outlaw, a popular Linux youtuber, has reviewed <code>htop</code> and like me, recommends still learning <code>top</code> as it comes preinstalled on many Linux distros by default. At 1:08-1:55 of this video he compares the two.</li> <li>Great video by Learn Linux TV going more into load average.</li> </ul>"},{"location":"Linux/Docker/Tutorial/","title":"Tutorial","text":""},{"location":"Linux/Docker/Tutorial/#installation","title":"Installation","text":"<p>The fastest and easiest way to get docker up and running on Linux is the  Docker Install Script.This will install the latest stable version on supported Linux Distros. This includes the following:</p> <ul> <li>docker-ce - Community Edition Docker Engine</li> <li>docker-ce-cli - Docker Client/Command line Interface</li> <li>docker-buildx-plugin: Extends the Docker build with new features with a CLI plugin.</li> <li>containerd.io: A container runtime that manages the container\u2019s lifecycle.</li> <li>docker-compose-plugin - orchestrates and manages Docker containers with compose files</li> </ul> <p>Let's first fetch the script from the repo. </p> <pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\n</code></pre> <p>Now let's verify the script by looking through the code. You can cat into less or open it with your editor of choice</p> <pre><code>cat install-docker.sh | less\n#OR\n[YOUR-EDITOR] install-docker.sh`\n</code></pre> <p>For additional verification, we can dry run to display steps before actually running them</p> <pre><code>sh install-docker.sh --dry-run`\n</code></pre> <p>Now if we are confident it it secure and understand everything it is installing, let's run the script for real.</p> <pre><code>sudo sh install-docker.sh`\n</code></pre> <p>Now add yourself to the docker group. This will allow you to run docker commands with without prefixing everything with <code>sudo</code>. Reboot your system for this to take effect.</p> <pre><code>sudo usermod -aG docker ${USER}\n</code></pre> <p>That's it! Now you should have Docker installed and ready to go.</p>"},{"location":"Linux/Docker/Tutorial/#pull-run-and-basic-usage","title":"Pull, Run and Basic Usage","text":"<p>There are two common ways to run containers. First, we can <code>pull</code> pre-built images from public registries like Docker Hub. Second, we can create our own dockerfile, <code>build</code> an image from it, and <code>run</code> it to create our own container.</p> <p>First let's go with the first method. Before pulling this image, you can check it out on  Docker Hub to get information about it. Optionally you can run  <code>docker search hello-world</code> from the command line.</p> <p>You can either <code>pull</code> and then <code>run</code>, or you can just run. Docker will first check for the image locally, and if it does not find it (in this case, if you didn't pull it), it will automatically pull it for you off of Dockerhub.</p> <pre><code>docker pull hello-world\n#OR\ndocker run hello-world\n</code></pre> <p>This hello-world image is really just to make sure your installation works correctly, and in this case, to familiarize yourself with the basic commands.</p> <p>To run a real container, you can pull the official Ubuntu image and run it, or again, just run it and have docker find and pull it for you.</p> <pre><code>docker run -it ubuntu bash\n</code></pre> <p>The <code>-i</code> flag is for interactive, which keeps the standard input open, allowing you to interact with the container like you would expect, and <code>-t</code> is for tty, which allocates a psuedo tty/terminal inside the container. This simulates a real terminal like when you SSH into a remote server. After we specify our image, in this case Ubuntu, we run a bash shell. Although for the official Ubuntu image, this is probably unnecessary and already configured as a command in the cmd attribute.</p> <p>After running the above command you should now be inside a real docker container. Play around with it, install whatever you want. If you have some balls, run <code>sudo rm -rf /</code> and see what happens. When you're done dinking around, you can <code>exit</code> the container. </p> <p>Changes to the container will not be saved until you <code>commit</code> them. Let's say you make a test directory with <code>mkdir test</code>, then <code>exit</code> the container.  When commiting, you enter the container_id followed by the name of the image. The container_id can be found after the user \"root\", I've copied and pasted it here. Every commit creates a new image as if it were a snapshot in time. You can see my test_image worked after listing the images with <code>docker images</code>.</p> <p></p> <p>Ok cool, so we can play around with a prebuilt image. My problem is, this Ubuntu image always logs me in as root. I don't like that. I want to be logged in as a regular user but have the option to act as root with <code>sudo</code>, like the normal Linux experience. This is ideal for a development environment, which is what I am interested in, and maybe you are too. Either way it will be useful for you to know how to create your own images from docker files.</p>"},{"location":"Linux/Docker/Tutorial/#making-containers-with-dockerfiles","title":"Making Containers with Dockerfiles","text":"<p>Making docker files is simple, just create one with your text editor (I use neovim btw). Just make sure to name it exactly \"Dockerfile\" with a capital D. This is what docker expects when you <code>build</code> the image from it.</p> <p><code>nvim Dockerfile</code></p> <p>Add this to your file. Replace USERNAME with whatever name you want and 1234 to your own password.</p> <pre><code>FROM ubuntu\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y sudo &amp;&amp; \\\n    useradd -m USERNAME &amp;&amp; \\\n    echo \"USERNAME:1234\" | chpasswd &amp;&amp; \\\n    adduser USERNAME sudo\n\nUSER USERNAME\n</code></pre> <p>Basically all we are doing, is creating an Ubuntu image with your own username and password. This way it will be similar to a regular Linux experience, with regular root privileges using <code>sudo</code> which prompts for your password.   </p> <p>Now build an image from the file. You do not have to specify the file, Docker fill find it. the <code>-t</code> stands for tag, this allows you to name your container with an optional tag name prefixed with a semicolon. Make sure you include the dot at the end for location.</p> <pre><code>docker build -t container_name:tag_name . \n</code></pre> <p>Now make sure it was created by listing images with <code>docker images</code>. </p> <p></p> <p>Ok, now you should be good to good to go. Spin up the c</p> <p>NOTES ARE STILL IN DEVELOPMENT</p>"},{"location":"Linux/Docker/Tutorial/#manage-docker-engine","title":"Manage Docker Engine","text":"<p>Let's learn how the docker engine works. It is managed as a <code>systemd</code> service and is thus managed with the  <code>systemctl</code> (system control) command. Let's verify it's active. <pre><code>sudo systemctl is-active docker\n</code></pre></p>"},{"location":"Linux/Docker/Tutorial/#removing-images-and-containers","title":"Removing Images and Containers","text":"<p>When trying to remove a image, you will often get an error because an active container is running off the image. To fix this, simply <code>stop</code> and <code>rm</code> that container referencing it's id. This will permanently delete the container and image, so make sure you know what you are doing.</p> <p></p>"},{"location":"Linux/Docker/Tutorial/#additional","title":"Additional","text":"<p>To check the command attribute of an image, enter the following. This is useful to know to check if you need to specify the shell when spinning up a container.</p> <p><code>docker image inspect [image] --format '{{.Config.Cmd}}'</code></p> <p>This should usually return <code>[/bin/bash]</code>. If it doesn't, you may have to add <code>bash</code> to this command:  <code>docker run -it [image] bash</code></p>"},{"location":"Python/Advanced%20Webscraping/","title":"Advanced Webscraping","text":"<p>When using the requests library alone, it will often trigger a security feature to prevent online attacks. Here I'm wanting to do some scraping on Indeed for job listings and I suspect I will need a user agent to get past this. <pre><code>import requests\nr = requests.get(\"https://www.indeed.com/jobs\")\nprint(r.text)\n</code></pre> Here is the HTML page response. Cloudflare has blocked me.</p> <p></p> <p>Though I could try to use a fake user agent to fool Cloudflare, their security features are too advanced. Instead Undetectable Chromedriver should do the trick.</p>"},{"location":"Python/Miscellaneous%20Notes/","title":"Miscellaneous Notes","text":""},{"location":"Python/Miscellaneous%20Notes/#__repr__-and-__str__-methods","title":"__repr__ and __str__ methods","text":"<p>Two dunder (double underscore) methods that are very similar and often confused are repr and str. They both return a string representation of an object.  In this example, they are functionally very similar, both returning a string about the person object. Both attributes name and age are included.</p> <pre><code>class Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def __str__(self):\n        return f\"I'm {self.name}, and I'm {self.age} years old.\"\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(name='{self.name}', age={self.age})\"\n</code></pre> <p>Though functionally similar, the difference is who they are intended for. repr is meant to be an unambiguous string representation of an object for developers to debug and/or recreate the object if necessary. str  is used for a user friendly printed representation of an object for end-users. The focus here is on readability.</p> <p>Just remember repr is for developers and str is for customers.</p> <p>By default, printing an object will use the str method, unless there is none, in which case it will fall back to the repr method. if there is neither a repr nor a str method, it will print the memory address.</p>"},{"location":"Python/Miscellaneous%20Notes/#lamda","title":"lamda","text":"<p><pre><code>class Book:\n    def __init__(self, title, author, pages):\n        self.title = title\n        self.author = author  \nbook_details = [\n    (\"It\", \"Stephen King\", 1138),\n    (\"A Tale of Two Cities\", \"Charles Dickens\", 304),\n    (\"Going Postal\", \"Terry Pratchett\", 484)\n]\n\nbooks = []\n\nfor details in book_details:\n    books.append(Book(*details))\n\nsorted_books = sorted(books, key=lambda book: book.title)\n</code></pre> In this example, there is a list of book objects called <code>books</code> created from information in a list of tuples <code>book_details</code>. An additional list is created called <code>sorted_books</code> that makes reference to the same objects in the previous list, but orders them based on the book title of each object. lambda is used as a concise, inline key for sorting. It provides a anonymous function without the need for a <code>def</code> keyword, such as the following.</p> <pre><code>def get_title(book):\n    return book.title\n\nsorted_books = sorted(books, key=get_title)\n</code></pre>"},{"location":"Python/Package%20Management%20and%20Virtual%20Environments/","title":"Package Management and Virtual Environments","text":""},{"location":"Python/Package%20Management%20and%20Virtual%20Environments/#overview","title":"Overview","text":"<p>There are many different ways to manage Python package versions and dependencies. Despite the Zen of Python saying \"There should be only one obvious way to do it\", Python itself is wide spread and used in many different domains, preferences and requirements start to develop across these many fields. System administrators often use the default package manager on their system (apt, yum, etc.) while those in the scientific community and data science use Anaconda, which bundles Python and R in it's own environments. The most common is to just use <code>pip</code>, the default package manager that grabs packages from PyPi (Python Package Index), in combination with Virtual Environment (<code>venv</code>), which is part of the standard library. <code>virtualenv</code> is a very similar tool which includes a few more features.</p>"},{"location":"Python/Package%20Management%20and%20Virtual%20Environments/#various-tools","title":"Various Tools","text":"<p>Here are some various notes I've jotted down and other tools to be aware of.</p> <p>Pipfile: the replacement for  pip's requirements.txt <code>pipenv</code> combines pipfile, pip and <code>virtualenv</code>.  <code>pyenv</code> isolates Python versions <code>venv</code> is a built-in module in Python 3 while <code>virtualenv</code> is a third-party tool</p> <p>This Stackoverflow post sums everything up and is a great resource.</p>"},{"location":"Python/Package%20Management%20and%20Virtual%20Environments/#venv-and-virtualenv","title":"venv and virtualenv","text":"<p>venv is a subset of virtualenv integrated into the standard library which lacks the following features - <code>app-data</code> seed method, which caches and speeds up the creation of virtual environments - automatically discover python version to base virtual environment - Support of Python 2 Despite being included in standard libraries of most distributions, on Debian/Ubuntu, it may not be and you'll have to <code>apt add-repository 'ppa:deadsnakes/ppa</code>, next <code>apt update</code>, then <code>sudo apt install python3.12-venv</code> This video is a good comparison between the two</p> <p>For spinning up small projects, it seems <code>venv</code> is the best option as it is the standard library default and has less features (that's a good thing) than <code>virtualenv</code>.</p> <p>To spin up a small project on Linux, run the following. The <code>-m</code> is for running library module as a script <pre><code>python3 -m venv [project directory]\n</code></pre> This will create a <code>bin</code> folder for executable files (possibly symlinks to host machine) including the Python interpreter, the very large <code>lib</code> folder which contains an entire copy of the Python standard library as well as any other package you install, the <code>include</code> directory for C headers to compile Python packages, and finally the <code>pyvenv.cfg</code> config file. </p> <p>To activate the environment, you can run the Bash script <code>bin/activate</code>.  <pre><code>source `[project dir]/bin/activate`\n</code></pre></p> <p>Now anything you install with <code>pip3</code> will be confined to that environment, regardless of what directory you are in. Simply run <code>deactivate</code> when you are done.</p>"},{"location":"Python/Package%20Management%20and%20Virtual%20Environments/#pipenv","title":"pipenv","text":"<p>Python is over a 30 year old language and some libraries have limitations and incompatibilities, pipenv is a modern tool released in 2017 to bridge the gap between Pip, Python and virtualenv by having only a <code>pipfile</code> for package dependencies and <code>pipfile.lock</code> for version releases/builds and hashes as opposed to the antiquated <code>requirements.txt</code>. pipenv creates ands and manages virtualenv for you.</p>"},{"location":"Python/Pygame/","title":"Pygame","text":""},{"location":"Python/Pygame/#how-i-started","title":"How I Started","text":"<p>After learning basic programming logic (for loops, functions etc.) and Python syntax, I wanted to build something big, not just a small terminal program that asks for your name, or age, and does some simple function. Pygame was the perfect library. This would allow me build a larger program, learn debugging, and implement an object oriented design with the players and enemies to to learn OOP (Object Oriented Programming).</p>"},{"location":"Python/Pygame/#overview","title":"Overview","text":"<p>Whether you are on Windows or Linux, you should be able to install Pygame with Python's package manager <code>pip</code> , and use your editor of choice. I will be using Vim from my Linux Terminal. Source code for the entire game will be in this repo.</p> <p>This will not be a conventional programming tutorial where we go line by line. Instead it will be about adding and removing chunks of code at a time, so you can get a bigger picture of what is going on. This way you will not simply copy the code from the tutorial (like most are), but understand what a fully functioning program looks like that you can play around with and break. </p>"},{"location":"Python/Pygame/#boilerplate","title":"Boilerplate","text":"<p>Here is a basic boilerplate of any Pygame program. To avoid future confusion as you read other tutorials, I have given a few different options, including the main game loop being a never ending loop, only stopped by the <code>exit</code> method, as well as creating a <code>running</code> variable set to True, and set that to False when we want to quit. The other option is the <code>display</code> or <code>update</code> method, which I argue my case for the <code>display</code> method below.</p> <p>Pygame's <code>init</code> and <code>quit</code> methods are opposites, initializing and deinitializing the Pygame library. <code>display.set_mode</code> creates the window, taking the width and height as arguments. <code>for event in pygame.event.get():</code> will loop through each Pygame event, such as click events, keyboard input, or quitting the game. </p> <p>As for the end, the <code>update</code> method is really only for updating certain rectangular part of the display surface for optimizing performance, since it will default to updating the entire screen, it is functionally the same as the <code>flip</code> method. You will often see the update method in tutorials, I think this is wrong and the <code>flip</code> method is preferable until you actually need to optimize your game, in which case you should use <code>update</code></p> <pre><code>import pygame \nfrom sys import exit\n\npygame.init() # turn on pygame\n\nwin = pygame.display.set_mode((800,500)) # 800 pixels wide, 500 pixels tall\n\n#running = True\n\n#while running\n#OR\nwhile True:\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT: \n            pygame.quit() # turn off pygame\n            exit() # using the sys library, exit the entire python program\n        #OR\n        # running = False\n\n    pygame.display.flip()\n    #OR\n    #pygame.display.update()\n</code></pre>"},{"location":"Python/Pygame/#pictures-of-final-game","title":"Pictures of Final Game","text":"<p>You can dig through the repository for the finished project here. (Several months later) I don't imagine I will ever get around to finishing a tutorial for this. It's only 340 lines of Python that is well commnented so you can probably make things out on your own.</p> <p> </p>"},{"location":"Python/Python%20Under%20the%20Hood/","title":"Python Under the Hood","text":"<p>It is known that Python's implementation, CPython, which act's as both an interpreter and a compiler, is written in C. Similarly, the Python Standard Library, including all of the most popular modules such as time, random, and os are also written in C. To have a better appreciation of Python and how it works under the hood, it is worth digging through some of the Python's source code to gain a greater understanding.</p> <p>First let's consider a simple program in Python that imports the time module and prints out the time. </p> <pre><code>import time\n\nseconds = time.time() # seconds since Epoch/Unix time (Jan 1st 1970)\n\ntime_h = time.ctime(seconds) # human readable time\nprint(time_h) # Thu Sep 28 18:54:21 2023\n</code></pre> <p>This seems to work just like magic, and it is! But really it is an abstraction, a wrapper around C's <code>&lt;time.h&gt;</code> library. Let's consider the same program in C. First we include two header files,  one for standard input output (stdio), and the other for time, then we do the same thing as the python file. </p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;time.h&gt;\n\nint main() {\n    time_t current_time;\n\n    seconds = time(NULL); // seconds since epoch\n\n    printf(\"%s\", ctime(&amp;current_time)); // human readable time\n    // # Thu Sep 28 18:54:21 2023\n\n    return 0;\n}\n</code></pre> <p>That works just as easily, but there are times when writing C code can get very verbose and tedious, as it is a lower level language that provides more granular control over hardware and memory management, which often requires more explicit code. Python make's things easier and provides abstractions for C's standard libraries such as <code>&lt;time.h&gt;</code>, and <code>&lt;math.h&gt;</code>.</p> <p>The time module in python and it's methods are defined in timemodule.c under Modules in the CPython repository. Here is the <code>ctime</code> function we used earlier in the Python file that prints out the human readable date. </p> <pre><code>static PyObject *\n_asctime(struct tm *timeptr)\n{\n    static const char wday_name[7][4] = {\n        \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\"\n    };\n    static const char mon_name[12][4] = {\n        \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n        \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n    };\n    return PyUnicode_FromFormat(\n        \"%s %s%3d %.2d:%.2d:%.2d %d\",\n        wday_name[timeptr-&gt;tm_wday],\n        mon_name[timeptr-&gt;tm_mon],\n        timeptr-&gt;tm_mday, timeptr-&gt;tm_hour,\n        timeptr-&gt;tm_min, timeptr-&gt;tm_sec,\n        1900 + timeptr-&gt;tm_year);\n}\n</code></pre> <p>If you you look at the beginning of this same file, you will see the <code>&lt;time.h&gt;</code> library.</p> <p></p> <p>As you can see, the Python Time module, and many others, are in fact built directly in C. For example, when you write a Python program managing sockets that enable communication over a network, the sockets module is in fact build directly from C's <code>&lt;sockets.h&gt;</code> library. The same goes for every other module in the standard library, such as random, abc, math, and so on. </p>"},{"location":"Python/Webscraping/","title":"Webscraping","text":"<p>The first package necessary for webscraping in Python is requests. At 331 million downloads per month, it is one of the most popular downloads on the Python Package Index (PyPI). It is a simple and easy HTTP library, which in our case, will be used to <code>get</code> a url and store that in a response object, to then print out the text (HTML) content. Pretty print gives us a formatted and readable output.</p> <pre><code>import requests\nfrom pprint import pprint\n\nr = requests.get('https://github.com/sveltejs/svelte')\nprint(type(r))\npprint(r.text)\n</code></pre> <p>As you can tell by the url I am requesting, I am interested in Svelte.js, a very popular, lightweight Javascript framework for building web apps. I want to track the growth of this project, and compare it against other frameworks like React and Angular. The easiest metric would be the stars on their github repositories. I recommend you take a look at the repo, and open up dev tools with  with <code>ctrl</code> + <code>alt</code> + <code>c</code> and inspect this element so you understand what we are about to do.</p> <p></p> <p>If you look at the HTML of that particular <code>div</code> element that contains the stars, there is no <code>id</code> to easily grab. Most element either have an <code>id</code> attribute, which is unique, or a <code>class</code> attribute, which is shared across elements. Since there is only a class of <code>mt-2</code> as of making this video, it will be a bit tricky as it is hard to distinguish that specific element while scraping. If there were an id of say, <code>star-content</code>, it would look something like this:</p> <pre><code>import requests, re\nfrom bs4 import BeautifulSoup as bs4\n\nr = requests.get('https://github.com/sveltejs/svelte')\nsoup = bs4(r.text,'html.parser')\n\nstars = soup.find('div',id=\"star-content\")\nprint(stars.text)\n</code></pre> <p>As you can tell from above, we will need the bs4 package. We would request the url the same as before, then wrap it in a <code>Beautiful Soup</code> object using either <code>html.parser</code> or <code>lxml</code>. Next, find a div element with an id of \"star content\", then print out just the text inside the html tags. Since we don't actually have an id in the element, we will instead have to identify it with the class of <code>mt-2</code>. Since their are many elements with this class, we will have to use the <code>find_all</code> method and find out which on it is.</p> <p></p> <p>Let's find all div elements with class of <code>mt-2</code>. This will store them in an array, each element being indexed. <pre><code>mt_el = soup.find_all('div',class_=\"mt-2\")\nprint(mt_el)\n</code></pre></p> <p>But how do we tell which one is the element with the star data? After all, that's all we want. The best way is to print out each element, with it's corresponding index. There is a lot of white space, so we will use python's build in <code>strip</code> method.</p> <pre><code>for i,el in enumerate(mt_el):\n    print(el.text.strip(),i)\n</code></pre> <p>You should get an output like this:</p> <p></p> <p>If you look at the screenshot from earlier, you can tell this output is exactly the text content of every element in the about section. Since we know the stars are on the 5th index, we can go back and grab just that content.</p> <pre><code>mt_el = soup.find_all('div',class_=\"mt-2\")\nstars = mt_el[5].text.strip()\nprint(stars)\n</code></pre> <p>This will output something like this:</p> <p></p> <p>To grab just the <code>73</code>, we will use a regex (regular expression) with the re module. </p> <pre><code>mt_el = soup.find_all('div',class_=\"mt-2\")\nstar_el = mt_el[5].text.strip()\nmatch = re.search(r'(\\d{1,4}\\.\\d+|\\d{1,4})', star_el)\nstars = match.group()\nprint(stars())\n</code></pre> <p>This should print just <code>73</code>. </p> <p>So we can find the stars on the Svelte repo, great. But if you test this on other repos, the index of the star element will differ slightly, making it difficult to wrap in a function, for example, if I wanted to find the stars for the React repo, it is not on the 5th index, but rather the 6th.</p> <pre><code>mt_el = soup.find_all('div',class_=\"mt-2\")\nstar_el = mt_el[6].text.strip()\nmatch = re.search(r'(\\d{1,4}\\.\\d+|\\d{1,4})', star_el)\nstars = match.group()\nprint(stars)\n</code></pre> <p>To have a function that takes a url as an argument and automatically finds the index with the stars, we will have to create a temporary list that contains the stripped out text content of each div, to then be be searched for \"stars\" and the regex applied. Here is the program so far.</p> <pre><code>import requests, re\nfrom bs4 import BeautifulSoup as bs\n\ndef get_stars(url):\n    r = requests.get(url)\n    soup = bs(r.text,'html.parser')\n\n    mt_el = soup.find_all('div',class_=\"mt-2\")\n    tmp_list = []\n    for i, e in enumerate(mt_el):\n        tmp_list.append(mt_el[i].text.strip())\n\n    for e in tmp_list:\n        if \"stars\" in e:\n           match = re.search(r'(\\d{1,4}\\.\\d+|\\d{1,4})',e)\n           star = match.group() \n\n    return star\n\nsvelte = get_stars(\"https://github.com/sveltejs/svelte\")\nreact = get_stars(\"https://github.com/facebook/react\")\nangular = get_stars(\"https://github.com/angular/angular\")\n\nprint(\"Project and Stars\\n\")\nprint(\"React: \",react)\nprint(\"Svelte: \",svelte)\nprint(\"Angular: \",angular)\n</code></pre> <p>Expected output:</p> <p></p> <p>That's pretty cool. But is it really that useful? I mean I can just go and open up the url and look myself, after all. What would really be useful is to track the growth, maybe run the script everyday at a scheduled time, then append that to a json file. So we will need the date, then key value pairs that correspond to the project and stars. I am imagining a nested dictionary like this:</p> <pre><code>data = {\n    \"10-04-2023\": {\n        \"React\": 214,\n        \"Angular\": 90.6,\n        \"Svelte\": 73,\n    }\n    \"10-05-2023\": {\n        \"React\": 218,\n        \"Angular\": 90.9,\n        \"Svelte\": 74,\n    }\n    \"10-06-2023\": {\n        \"React\": 221,\n        \"Angular\": 91.2,\n        \"Svelte\": 86,\n    }\n}\n</code></pre> <p>This will be super useful, because dictionaries in python are equivelent to json (javascript object notion) which are just key value pairs as well. This way, we could use a javascript charting library to visually represent the data using chart.js.</p> <p>Alright, so to do that, we first need to get the date using the datetime module. Then to make things easier, I've made a <code>urls</code> dictionary with the name as a key, and url as a value. This way, we can easily add as many projects we want to track by simply adding them to this dictionary. The <code>data</code> dictionary is where the valuable data is stored, we will then open a json file to append and <code>dump</code> that data using the json library.</p> <pre><code>from datetime import datetime\nimport requests, re, json\nfrom bs4 import BeautifulSoup as bs\n\ntoday = datetime.now().strftime('%m-%d-%Y')\nurls = {\n    'Svelte': 'https://github.com/sveltejs/svelte',\n    'React': 'https://github.com/facebook/react',\n    'Angular': 'https://github.com/angular/angular'\n}\n\n\ndef get_stars(url):\n    r = requests.get(url)\n    soup = bs(r.text,'html.parser')\n\n    mt_el = soup.find_all('div',class_=\"mt-2\")\n    #star_el = mt_el[6].text.strip()\n    tmp_list = []\n    for i, e in enumerate(mt_el):\n        tmp_list.append(mt_el[i].text.strip())\n\n    for e in tmp_list:\n        if \"stars\" in e:\n           match = re.search(r'(\\d{1,4}\\.\\d+|\\d{1,4})',e)\n           star = match.group() \n\n    return star\n\ndata = {}\ndata[today] = {}\nfor name, url in urls.items():\n    stars = get_stars(url)\n    data[today][name] = float(stars) if '.' in stars else int(stars)\n\nwith open(\"data.json\", \"a\") as f:\n    json.dump(data, f, indent=4)\n</code></pre> <p>Now if you want to schedule this to automatically run everyday, you could use windows scheduler or cron if you're on Linux or Mac.</p> <p>If you'd like, I can make a tutorial on displaying this data using chart.js. Otherwise, if you're really curious of this kind of data, you should know a tool like this already exists. To track the star history of a github repository, you can use Star Historywhich is an open source project that uses Github's official API to find metadata of each project, including stars. This is a much more reliable method than ours, as our script would break the day Github updates their user interface, which is a common obstacle in webscraping.</p> <p>To compare Svelte, React, and Angular, you can view using their website here</p> <p>Regardless, web scraping is a valuable skill to have, and Python libraries makes it very easy and intuitive. </p>"}]}